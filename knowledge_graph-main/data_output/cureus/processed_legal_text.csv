chapter_number,chapter_title,article_number,article_id,article_title,paragraph_number,paragraph_id,point_number,text
1,General provisions,1,1.1,Subject matter,1,1.1.1,1,"Article 1
Subject matter"
1,General provisions,1,1.1,Subject matter,2,1.1.2,1,"1. The purpose of this Regulation is to improve the functioning of the internal market and 
promote the uptake of human-centric and trustworthy artificial intelligence (AI), while 
ensuring a high level of protection of health, safety, fundamental rights enshrined in the 
Charter of Fundamental Rights, including democracy, the rule of law and 
environmental protection, against the harmful effects of artificial intelligence systems 
(AI systems) in the Union, and to support innovation."
1,General provisions,1,1.1,Subject matter,3,1.1.3,1,2. This Regulation lays down:
1,General provisions,1,1.1,Subject matter,3,1.1.3,2,"(a) harmonised rules for the placing on the market, the putting into service, and the use 
of AI systems in the Union;"
1,General provisions,1,1.1,Subject matter,3,1.1.3,3,(b) prohibitions of certain AI practices;
1,General provisions,1,1.1,Subject matter,3,1.1.3,4,"(c) specific requirements for high-risk AI systems and obligations for operators of such 
systems;"
1,General provisions,1,1.1,Subject matter,3,1.1.3,5,(d) harmonised transparency rules for certain AI systems;
1,General provisions,1,1.1,Subject matter,3,1.1.3,6,(e) harmonised rules for the placing on the market of general-purpose AI models;
1,General provisions,1,1.1,Subject matter,3,1.1.3,7,"(f) rules on market monitoring, market surveillance governance and enforcement;"
1,General provisions,1,1.1,Subject matter,3,1.1.3,8,"(g) measures to support innovation, with a particular focus on SMEs, including start-
ups."
1,General provisions,2,1.2,Scope,1,1.2.1,1,"Article 2
Scope"
1,General provisions,2,1.2,Scope,2,1.2.2,1,1. This Regulation applies to:
1,General provisions,2,1.2,Scope,2,1.2.2,2,"(a) providers placing on the market or putting into service AI systems or placing on the 
market general-purpose AI models in the Union, irrespective of whether those 
providers are established or located within the Union or in a third country;"
1,General provisions,2,1.2,Scope,2,1.2.2,3,"(b) deployers of AI systems that have their place of establishment or are located within 
the Union;"
1,General provisions,2,1.2,Scope,2,1.2.2,4,"(c) providers and deployers of AI systems that have their place of establishment or are 
located in a third country, where the output produced by the AI system is used in the 
Union;"
1,General provisions,2,1.2,Scope,2,1.2.2,5,(d) importers and distributors of AI systems;
1,General provisions,2,1.2,Scope,2,1.2.2,6,"(e) product manufacturers placing on the market or putting into service an AI system 
together with their product and under their own name or trademark;"
1,General provisions,2,1.2,Scope,2,1.2.2,7,"(f) authorised representatives of providers, which are not established in the Union;"
1,General provisions,2,1.2,Scope,2,1.2.2,8,(g) affected persons that are located in the Union.
1,General provisions,2,1.2,Scope,3,1.2.3,1,2. For  AI systems classified as high-risk AI systems in accordance with Article 6(1) and
1,General provisions,2,1.2,Scope,4,1.2.4,1,"(2) related to products covered by the Union harmonisation legislation listed in section B 
of Annex I, only Article 112 applies. Article 57 applies only in so far as the requirements 
for high-risk AI systems under this Regulation have been integrated in that Union 
harmonisation legislation."
1,General provisions,2,1.2,Scope,5,1.2.5,1,"3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in 
any event, affect the competences of the Member States concerning national security, 
regardless of the type of entity entrusted by the Member States with carrying out tasks in 
relation to those competences.
This Regulation does not apply to AI systems where and in so far they are placed on the 
market, put into service, or used with or without modification exclusively for military, 
defence or national security purposes, regardless of the type of entity carrying out those 
activities.
This Regulation does not apply to AI systems which are not placed on the market or put 
into service in the Union, where the output is used in the Union exclusively for military, 
defence or national security purposes, regardless of the type of entity carrying out those 
activities."
1,General provisions,2,1.2,Scope,6,1.2.6,1,"4. This Regulation applies neither to public authorities in a third country nor to international 
organisations falling within the scope of this Regulation pursuant to paragraph 1, where 
those authorities or organisations use AI systems in the framework of international 
cooperation or agreements for law enforcement and judicial cooperation with the Union or 
with one or more Member States, provided that such a third country or international 
organisation provides adequate safeguards with respect to the protection of fundamental 
rights and freedoms of individuals."
1,General provisions,2,1.2,Scope,7,1.2.7,1,"5. This Regulation shall not affect the application of the provisions on the liability of 
providers of intermediary services as set out in Chapter II of Regulation (EU) 2022/2065."
1,General provisions,2,1.2,Scope,8,1.2.8,1,"6. This Regulation does not apply to AI systems or AI models, including their output, 
specifically developed and put into service for the sole purpose of scientific research and 
development."
1,General provisions,2,1.2,Scope,9,1.2.9,1,"7. Union law on the protection of personal data, privacy and the confidentiality of 
communications applies to personal data processed in connection with the rights and 
obligations laid down in this Regulation. This Regulation shall not affect Regulation 
(EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without 
prejudice to the arrangements provided for in Article 10(5) and Article 59 of this 
Regulation."
1,General provisions,2,1.2,Scope,10,1.2.10,1,"8. This Regulation does not apply to any research, testing or development activity 
regarding AI systems or models prior to their being placed on the market or put into 
service. Such activities shall be conducted in accordance with applicable Union law. 
Testing in real world conditions shall not be covered by that exclusion."
1,General provisions,2,1.2,Scope,11,1.2.11,1,"9. This Regulation is without prejudice to the rules laid down by other Union legal acts 
related to consumer protection and product safety."
1,General provisions,2,1.2,Scope,12,1.2.12,1,"10. This Regulation does not apply to obligations of deployers who are natural persons 
using AI systems in the course of a purely personal non-professional activity."
1,General provisions,2,1.2,Scope,13,1.2.13,1,"11. This Regulation does not preclude the Union or Member States from maintaining or 
introducing laws, regulations or administrative provisions which are more favourable to 
workers in terms of protecting their rights in respect of the use of AI systems by 
employers, or from encouraging or allowing the application of collective agreements 
which are more favourable to workers."
1,General provisions,2,1.2,Scope,14,1.2.14,1,"12. This Regulation applies to AI systems released under free and open source licences, 
unless they are placed on the market or put into service as high-risk AI systems or as an 
AI system that falls under Article 5 or 50."
1,General provisions,3,1.3,Definitions,1,1.3.1,1,"Article 3
Definitions
For the purposes of this Regulation, the following definitions apply:"
1,General provisions,3,1.3,Definitions,2,1.3.2,1,"(1) ‘AI system’ means a machine-based system designed to operate with varying levels of 
autonomy, that may exhibit adaptiveness after deployment and that, for explicit or 
implicit objectives, infers, from the input it receives, how to generate outputs such as 
predictions, content, recommendations, or decisions that can influence physical or virtual 
environments;"
1,General provisions,3,1.3,Definitions,3,1.3.3,1,"(2) ‘risk’ means the combination of the probability of an occurrence of harm and the 
severity of that harm;"
1,General provisions,3,1.3,Definitions,4,1.3.4,1,"(3) ‘provider’ means a natural or legal person, public authority, agency or other body that 
develops an AI system or a general-purpose AI model or that has an AI system or a 
general-purpose AI model developed and places it on the market or puts the AI system 
into service under its own name or trademark, whether for payment or free of charge;"
1,General provisions,3,1.3,Definitions,5,1.3.5,1,"(4) ‘deployer’ means a natural or legal person, public authority, agency or other body using an 
AI system under its authority  except where the AI system is used in the course of a 
personal non-professional activity;"
1,General provisions,3,1.3,Definitions,6,1.3.6,1,"(5) ‘authorised representative’ means a natural or legal person located or established in the 
Union who has received and accepted a written mandate from a provider of an AI system 
or a general-purpose AI model to, respectively, perform and carry out on its behalf the 
obligations and procedures established by this Regulation;"
1,General provisions,3,1.3,Definitions,7,1.3.7,1,"(6) ‘importer’ means a natural or legal person located or established in the Union that places 
on the market  an AI system that bears the name or trademark of a natural or legal person 
established in a third country;"
1,General provisions,3,1.3,Definitions,8,1.3.8,1,"(7) ‘distributor’ means a natural or legal person in the supply chain, other than the provider or 
the importer, that makes an AI system available on the Union market ;"
1,General provisions,3,1.3,Definitions,9,1.3.9,1,"(8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
importer or distributor;"
1,General provisions,3,1.3,Definitions,10,1.3.10,1,"(9) ‘placing on the market’ means the first making available of an AI system or a general-
purpose AI model on the Union market;"
1,General provisions,3,1.3,Definitions,11,1.3.11,1,"(10) ‘making available on the market’ means the supply of an AI system or a general-purpose 
AI model for distribution or use on the Union market in the course of a commercial 
activity, whether in return for payment or free of charge;"
1,General provisions,3,1.3,Definitions,12,1.3.12,1,"(11) ‘putting into service’ means the supply of an AI system by the provider for first use 
directly to the deployer or for own use in the Union  for its intended purpose;"
1,General provisions,3,1.3,Definitions,13,1.3.13,1,"(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, 
including the specific context and conditions of use, as specified in the information 
supplied by the provider in the instructions for use, promotional or sales materials and 
statements, as well as in the technical documentation;"
1,General provisions,3,1.3,Definitions,14,1.3.14,1,"(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in 
accordance with its intended purpose, but which may result from reasonably foreseeable 
human behaviour or interaction with other systems, including other AI systems;"
1,General provisions,3,1.3,Definitions,15,1.3.15,1,"(14) ‘safety component’ means a component of a product or of a system which fulfils a safety 
function for that product or system, or the failure or malfunctioning of which endangers the 
health and safety of persons or property;"
1,General provisions,3,1.3,Definitions,16,1.3.16,1,"(15) ‘instructions for use’ means the information provided by the provider to inform the 
deployer of in particular an AI system’s intended purpose and proper use ;"
1,General provisions,3,1.3,Definitions,17,1.3.17,1,"(16) ‘recall of an AI system’ means any measure aiming to achieve the return to the provider or 
taking out of service or disabling the use of an AI system made available to deployers;"
1,General provisions,3,1.3,Definitions,18,1.3.18,1,"(17) ‘withdrawal of an AI system’ means any measure aiming to prevent an AI system in the 
supply chain being made available on the market;"
1,General provisions,3,1.3,Definitions,19,1.3.19,1,"(18) ‘performance of an AI system’ means the ability of an AI system to achieve its intended 
purpose;"
1,General provisions,3,1.3,Definitions,20,1.3.20,1,"(19) ‘notifying authority’ means the national authority responsible for setting up and carrying 
out the necessary procedures for the assessment, designation and notification of conformity 
assessment bodies and for their monitoring;"
1,General provisions,3,1.3,Definitions,21,1.3.21,1,"(20) ‘conformity assessment’ means the process of demonstrating whether the requirements set 
out in Chapter II, Section 2 relating to a high-risk AI system have been fulfilled;"
1,General provisions,3,1.3,Definitions,22,1.3.22,1,"(21) ‘conformity assessment body’ means a body that performs third-party conformity 
assessment activities, including testing, certification and inspection;"
1,General provisions,3,1.3,Definitions,23,1.3.23,1,"(22) ‘notified body’ means a conformity assessment body notified in accordance with this 
Regulation and other relevant Union harmonisation legislation as listed in Section B of 
Annex I;"
1,General provisions,3,1.3,Definitions,24,1.3.24,1,"(23) ‘substantial modification’ means a change to an AI system after its placing on the market 
or putting into service which is not foreseen or planned in the initial conformity 
assessment carried out by the provider and as a result of which the compliance of the AI 
system with the requirements set out in Chapter II, Section 2 is affected or results in a 
modification to the intended purpose for which the AI system has been assessed;"
1,General provisions,3,1.3,Definitions,25,1.3.25,1,"(24) ‘CE marking’ means a marking by which a provider indicates that an AI system is in 
conformity with the requirements set out in Chapter II, Section 2 and other applicable 
Union harmonisation legislation listed in Annex I, providing for its affixing;"
1,General provisions,3,1.3,Definitions,26,1.3.26,1,"(25) ‘post-market monitoring system’ means all activities carried out by providers of AI 
systems to  collect and review experience gained from the use of AI systems they place 
on the market or put into service for the purpose of identifying any need to immediately 
apply any necessary corrective or preventive actions;"
1,General provisions,3,1.3,Definitions,27,1.3.27,1,"(26) ‘market surveillance authority’ means the national authority carrying out the activities and 
taking the measures pursuant to Regulation (EU) 2019/1020;"
1,General provisions,3,1.3,Definitions,28,1.3.28,1,"(27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of 
Regulation (EU) No 1025/2012;"
1,General provisions,3,1.3,Definitions,29,1.3.29,1,"(28) ‘common specification’ means a set of technical specifications as defined in Article 2, 
point (4) of Regulation (EU) No 1025/2012, providing means to  comply with certain 
requirements  established under this Regulation;"
1,General provisions,3,1.3,Definitions,30,1.3.30,1,"(29) ‘training data’ means data used for training an AI system through fitting its learnable 
parameters ;"
1,General provisions,3,1.3,Definitions,31,1.3.31,1,"(30) ‘validation data’ means data used for providing an evaluation of the trained AI system and 
for tuning its non-learnable parameters and its learning process in order, inter alia, to 
prevent underfitting or overfitting;"
1,General provisions,3,1.3,Definitions,32,1.3.32,1,"(31) ‘validation data set’ means a separate data set or part of the training data set, either as a 
fixed or variable split;"
1,General provisions,3,1.3,Definitions,33,1.3.33,1,"(32) ‘testing data’ means data used for providing an independent evaluation of the  AI system 
in order to confirm the expected performance of that system before its placing on the 
market or putting into service;"
1,General provisions,3,1.3,Definitions,34,1.3.34,1,"(33) ‘input data’ means data provided to or directly acquired by an AI system on the basis of 
which the system produces an output;"
1,General provisions,3,1.3,Definitions,35,1.3.35,1,"(34) ‘biometric data’ means personal data resulting from specific technical processing relating 
to the physical, physiological or behavioural characteristics of a natural person,  such as 
facial images or dactyloscopic data;"
1,General provisions,3,1.3,Definitions,36,1.3.36,1,"(35) ‘biometric identification’ means the automated recognition of physical, physiological, 
behavioural, or psychological human features for the purpose of establishing the identity 
of a natural person by comparing biometric data of that individual to biometric data of 
individuals stored in a database;"
1,General provisions,3,1.3,Definitions,37,1.3.37,1,"(36) ‘biometric verification’ means the automated, one-to-one verification, including 
authentication, of the identity of natural persons by comparing their biometric data to 
previously provided biometric data;"
1,General provisions,3,1.3,Definitions,38,1.3.38,1,"(37) ‘special categories of personal data’ means the categories of personal data referred to in 
Article 9(1) of Regulation (EU) 2016/679, Article 10 of Directive (EU) 2016/680 and 
Article 10(1) of Regulation (EU) 2018/1725;"
1,General provisions,3,1.3,Definitions,39,1.3.39,1,"(38) ‘sensitive operational data’ means operational data related to activities of prevention, 
detection, investigation or prosecution of criminal offences, the disclosure of which 
could jeopardise the integrity of criminal proceedings;"
1,General provisions,3,1.3,Definitions,40,1.3.40,1,"(39) ‘emotion recognition system’ means an AI system for the purpose of identifying or 
inferring emotions or intentions of natural persons on the basis of their biometric data;"
1,General provisions,3,1.3,Definitions,41,1.3.41,1,"(40) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural 
persons to specific categories on the basis of their biometric data, unless it is ancillary to 
another commercial service and strictly necessary for objective technical reasons;"
1,General provisions,3,1.3,Definitions,42,1.3.42,1,"(41) ‘remote biometric identification system’ means an AI system for the purpose of identifying 
natural persons, without their active involvement, typically at a distance through the 
comparison of a person’s biometric data with the biometric data contained in a reference 
database ;"
1,General provisions,3,1.3,Definitions,43,1.3.43,1,"(42) ‘real-time remote biometric identification system’ means a remote biometric identification 
system whereby the capturing of biometric data, the comparison and the identification all 
occur without a significant delay and comprises not only instant identification, but also 
limited short delays in order to avoid circumvention;"
1,General provisions,3,1.3,Definitions,44,1.3.44,1,"(43) ‘post remote biometric identification system’ means a remote biometric identification 
system other than a real-time remote biometric identification system;"
1,General provisions,3,1.3,Definitions,45,1.3.45,1,"(44) ‘publicly accessible space’ means any publicly or privately owned physical place 
accessible to an undetermined number of natural persons, regardless of whether certain 
conditions for access may apply, and regardless of the potential capacity restrictions;"
1,General provisions,3,1.3,Definitions,46,1.3.46,1,(45) ‘law enforcement authority’ means:
1,General provisions,3,1.3,Definitions,46,1.3.46,2,"(a) any public authority competent for the prevention, investigation, detection or 
prosecution of criminal offences or the execution of criminal penalties, including the 
safeguarding against and the prevention of threats to public security; or"
1,General provisions,3,1.3,Definitions,46,1.3.46,3,"(b) any other body or entity entrusted by Member State law to exercise public authority 
and public powers for the purposes of the prevention, investigation, detection or 
prosecution of criminal offences or the execution of criminal penalties, including the 
safeguarding against and the prevention of threats to public security;"
1,General provisions,3,1.3,Definitions,47,1.3.47,1,"(46) ‘law enforcement’ means activities carried out by law enforcement authorities or on their 
behalf for the prevention, investigation, detection or prosecution of criminal offences or 
the execution of criminal penalties, including safeguarding against and preventing threats 
to public security;"
1,General provisions,3,1.3,Definitions,48,1.3.48,1,"(47) ‘AI Office’ means the Commission’s function of contributing to the implementation, 
monitoring and supervision of AI systems and AI governance carried out by the 
European Artificial Intelligence Office established by Commission Decision of 
24.1.2024; references in this Regulation to the AI Office shall be construed as references 
to the Commission;"
1,General provisions,3,1.3,Definitions,49,1.3.49,1,"(48) ‘national competent authority’ means a notifying authority or a market surveillance 
authority;;"
1,General provisions,3,1.3,Definitions,50,1.3.50,1,"(49) ‘serious incident’ means an incident or malfunctioning of an AI system that directly or 
indirectly leads to any of the following:"
1,General provisions,3,1.3,Definitions,50,1.3.50,2,"(a) the death of a person, or serious harm to a person’s health;"
1,General provisions,3,1.3,Definitions,50,1.3.50,3,"(b) a serious and irreversible disruption of the management or operation of critical 
infrastructure."
1,General provisions,3,1.3,Definitions,50,1.3.50,4,"(c) the infringement of obligations under Union law intended to protect fundamental 
rights;"
1,General provisions,3,1.3,Definitions,50,1.3.50,5,(d) serious harm to property or the environment;
1,General provisions,3,1.3,Definitions,51,1.3.51,1,"(50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation 
(EU) 2016/679;"
1,General provisions,3,1.3,Definitions,52,1.3.52,1,"(51) ‘non-personal data’ means data other than personal data as defined in Article 4, point 
(1), of Regulation (EU) 2016/679;"
1,General provisions,3,1.3,Definitions,53,1.3.53,1,"(52) ‘profiling’ means profiling as defined in Article 4, point (4), of Regulation (EU) 
2016/679 or, in the case of law enforcement authorities, as defined in Article 3, point (4) 
of Directive (EU) 2016/680 or, in the case of Union institutions, bodies, offices or 
agencies, as defined in Article 3, point (5) of Regulation (EU) 2018/1725;"
1,General provisions,3,1.3,Definitions,54,1.3.54,1,"(53) ‘real-world testing plan’ means a document that describes the objectives, methodology, 
geographical, population and temporal scope, monitoring, organisation and conduct of 
testing in real-world conditions;"
1,General provisions,3,1.3,Definitions,55,1.3.55,1,"(54) ‘sandbox plan’ means a document agreed between the participating provider and the 
competent authority describing the objectives, conditions, timeframe, methodology and 
requirements for the activities carried out within the sandbox;"
1,General provisions,3,1.3,Definitions,56,1.3.56,1,"(55) ‘AI regulatory sandbox’ means a controlled framework set up by a competent authority 
which offers providers or prospective providers of AI systems the possibility to develop, 
train, validate and test, where appropriate in real-world conditions, an innovative AI 
system, pursuant to a sandbox plan for a limited time under regulatory supervision;"
1,General provisions,3,1.3,Definitions,57,1.3.57,1,"(56) ‘AI literacy’ means skills, knowledge and understanding that allows providers, deployers 
and affected persons, taking into account their respective rights and obligations in the 
context of this Regulation, to make an informed deployment of AI systems, as well as to 
gain awareness about the opportunities and risks of AI and possible harm it can cause;"
1,General provisions,3,1.3,Definitions,58,1.3.58,1,"(57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its 
intended purpose in real-world conditions outside a laboratory or otherwise simulated 
environment, with a view to gathering reliable and robust data and to assessing and 
verifying the conformity of the AI system with the requirements of this Regulation and it 
is not considered to be placing the AI system on the market or putting it into service 
within the meaning of this Regulation, provided that all the conditions laid down in 
Article 57 or 60 are fulfilled;"
1,General provisions,3,1.3,Definitions,59,1.3.59,1,"(58) ‘subject’, for the purpose of real-world testing, means a natural person who participates 
in testing in real-world conditions;"
1,General provisions,3,1.3,Definitions,60,1.3.60,1,"(59) ‘informed consent’ means a subject's freely given, specific, unambiguous and voluntary 
expression of his or her willingness to participate in a particular testing in real-world 
conditions, after having been informed of all aspects of the testing that are relevant to 
the subject's decision to participate;"
1,General provisions,3,1.3,Definitions,61,1.3.61,1,"(60) ‘deep fake’ means AI-generated or manipulated image, audio or video content that 
resembles existing persons, objects, places or other entities or events and would falsely 
appear to a person to be authentic or truthful;"
1,General provisions,3,1.3,Definitions,62,1.3.62,1,"(61) ‘widespread infringement’ means any act or omission contrary to Union law protecting 
the interest of individuals, which:"
1,General provisions,3,1.3,Definitions,62,1.3.62,2,"(a) has harmed or is likely to harm the collective interests of individuals residing in at 
least two Member States other than the Member State in which:"
1,General provisions,3,1.3,Definitions,62,1.3.62,3,"(i) the act or omission originated or took place;
(ii) the provider concerned, or, where applicable, its authorised representative is 
located or established; or
(iii) the deployer is established, when the infringement is committed by the 
deployer;
(i) the act or omission originated or took place;
(ii) the provider concerned, or, where applicable, its authorised representative is 
located or established; or
(iii) the deployer is established, when the infringement is committed by the 
deployer;"
1,General provisions,3,1.3,Definitions,62,1.3.62,4,"(b) has caused, causes or is likely to cause harm to the collective interests of 
individuals and has common features, including the same unlawful practice or the 
same interest being infringed, and is occurring concurrently, committed by the 
same operator, in at least three Member States;"
1,General provisions,3,1.3,Definitions,63,1.3.63,1,"(62) ‘critical infrastructure’ means critical infrastructure as defined in Article 2, point (4), of 
Directive (EU) 2022/2557;"
1,General provisions,3,1.3,Definitions,64,1.3.64,1,"(63) ‘general-purpose AI model’ means an AI model, including where such an AI model is 
trained with a large amount of data using self-supervision at scale, that displays 
significant generality and is capable of competently performing a wide range of distinct 
tasks regardless of the way the model is placed on the market and that can be integrated 
into a variety of downstream systems or applications, except AI models that are used for 
research, development or prototyping activities before they are released on the market;"
1,General provisions,3,1.3,Definitions,65,1.3.65,1,"(64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities 
recorded in the most advanced general-purpose AI models;"
1,General provisions,3,1.3,Definitions,66,1.3.66,1,"(65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-
purpose AI models, having a significant impact on the Union market due to their reach, 
or due to actual or reasonably foreseeable negative effects on public health, safety, 
public security, fundamental rights, or the society as a whole, that can be propagated at 
scale across the value chain;"
1,General provisions,3,1.3,Definitions,67,1.3.67,1,"(66) ‘general-purpose AI system’ means an AI system which is based on a general-purpose 
AI model, that has the capability to serve a variety of purposes, both for direct use as 
well as for integration in other AI systems;"
1,General provisions,3,1.3,Definitions,68,1.3.68,1,"(67) ‘floating-point operation’ or ‘FLOP’ means any mathematical operation or assignment 
involving floating-point numbers, which are a subset of the real numbers typically 
represented on computers by an integer of fixed precision scaled by an integer exponent 
of a fixed base;"
1,General provisions,3,1.3,Definitions,69,1.3.69,1,"(68) ‘downstream provider’ means a provider of an AI system, including a general-purpose 
AI system, which integrates an AI model, regardless of whether the model is provided by 
themselves and vertically integrated or provided by another entity based on contractual 
relations."
1,General provisions,4,1.4,Ai literacy,1,1.4.1,1,"Article 4
AI literacy
Providers and deployers of AI systems shall take measures to ensure, to their best extent, a 
sufficient level of AI literacy of their staff and other persons dealing with the operation and use 
of AI systems on their behalf, taking into account their technical knowledge, experience, 
education and training and the context the AI systems are to be used in, and considering the 
persons or groups of persons on whom the AI systems are to be used."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,1,2.1.1,1,"Article 5
Prohibited AI Practices"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,1,1. The following AI practices shall be prohibited:
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,2,"(a) the placing on the market, the putting into service or the use of an AI system that 
deploys subliminal techniques beyond a person’s consciousness or purposefully 
manipulative or deceptive techniques, with the objective, or the effect of, materially 
distorting the behaviour of a person or a group of persons by appreciably impairing 
their ability to make an informed decision, thereby causing a person to take a 
decision that that person would not have otherwise taken in a manner that causes or 
is likely to cause that person, another person or group of persons significant harm;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,3,"(b) the placing on the market, the putting into service or the use of an AI system that 
exploits any of the vulnerabilities of a person or a specific group of persons due to 
their age, disability or a specific social or economic situation, with the objective, or 
the effect, of materially distorting the behaviour of that person or a person 
belonging to that group in a manner that causes or is reasonably likely to cause that 
person or another person significant harm;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,4,"(c) the placing on the market, the putting into service or the use of AI systems  for the 
purpose of the evaluation or classification of natural persons or groups of persons 
over a certain period of time based on their social behaviour or known, inferred or 
predicted personal or personality characteristics, with the social score leading to 
either or both of the following:"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,5,"(i) detrimental or unfavourable treatment of certain natural persons or whole 
groups of persons in social contexts that are unrelated to the contexts in which 
the data was originally generated or collected;
(ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
persons that is unjustified or disproportionate to their social behaviour or its 
gravity;
(i) detrimental or unfavourable treatment of certain natural persons or whole 
groups of persons in social contexts that are unrelated to the contexts in which 
the data was originally generated or collected;
(ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
persons that is unjustified or disproportionate to their social behaviour or its 
gravity;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,6,"(d) the placing on the market, the putting into service for this specific purpose, or the 
use of an AI system for making risk assessments of natural persons in order to 
assess or predict the likelihood of a natural person committing a criminal offence, 
based solely on the profiling of a natural person or on assessing their personality 
traits and characteristics; this prohibition shall not apply to AI systems used to 
support the human assessment of the involvement of a person in a criminal 
activity, which is already based on objective and verifiable facts directly linked to a 
criminal activity;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,7,"(e) the placing on the market, the putting into service for this specific purpose, or use 
of AI systems that create or expand facial recognition databases through the 
untargeted scraping of facial images from the internet or CCTV footage;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,8,"(f) the placing on the market, the putting into service for this specific purpose, or the 
use of AI systems to infer emotions of a natural person in the areas of workplace 
and education institutions, except where the use of the AI system is intended to be 
put in place or into the market for medical or safety reasons."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,9,"(g) the placing on the market, the putting into service for this specific purpose, or the 
use of biometric categorisation systems that categorise individually natural persons 
based on their biometric data to deduce or infer their race, political opinions, trade 
union membership, religious or philosophical beliefs, sex life or sexual 
orientation; this prohibition does not cover any labelling or filtering of lawfully 
acquired biometric datasets, such as images, based on biometric data or 
categorizing of biometric data in the area of law enforcement;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,10,"(h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
spaces for the purposes of law enforcement,  unless and in so far as such use is 
strictly necessary for one of the following objectives:"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,2,2.1.2,11,"(i) the targeted search for specific  victims of abduction, trafficking in human 
beings or sexual exploitation of human beings, as well as searching for 
missing persons;
(ii) the prevention of a specific, substantial and imminent threat to the life or 
physical safety of natural persons or a genuine and present or genuine and 
foreseeable threat of a terrorist attack;
(iii) the  localisation or identification of a person suspected of having committed 
a criminal offence, for the purpose of conducting a criminal investigation, 
prosecution or executing a criminal penalty for offences referred to in Annex 
II and punishable in the Member State concerned by a custodial sentence or a 
detention order for a maximum period of at least four years;
 
Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
2016/679 for the processing of biometric data for purposes other than law enforcement."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,3,2.1.3,1,"2. The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces 
for the purposes of law enforcement for any of the objectives referred to in paragraph 1, 
point (h), shall be deployed only for the purposes set out in paragraph 1, point (h), to 
confirm the identity of the specifically targeted individual, and it shall take into account 
the following elements:"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,3,2.1.3,2,"(a) the nature of the situation giving rise to the possible use, in particular the seriousness, 
probability and scale of the harm that would be caused if the system were not used;"
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,3,2.1.3,3,"(b) the consequences of the use of the system for the rights and freedoms of all persons 
concerned, in particular the seriousness, probability and scale of those consequences.
In addition, the use of ‘real-time’ remote biometric identification systems in publicly 
accessible spaces for the purposes of law enforcement for any of the objectives referred to 
in paragraph 1, point (h), of this Article shall comply with necessary and proportionate 
safeguards and conditions in relation to the use in accordance with national law 
authorising the use thereof, in particular as regards the temporal, geographic and personal 
limitations. The use of the ‘real-time’ remote biometric identification system in publicly 
accessible spaces shall be authorised only if the law enforcement authority has 
completed a fundamental rights impact assessment as provided for in Article 27 and has 
registered the system in the EU database according to Article 49. However, in duly 
justified cases of urgency, the use of such systems may be commenced without the 
registration in the EU database, provided that such registration is completed without 
undue delay."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,4,2.1.4,1,"3. For the purposes of paragraph 1, point (h) and paragraph 2, each  use for the purposes of 
law enforcement of a ‘real-time’ remote biometric identification system in publicly 
accessible spaces shall be subject to a prior authorisation granted by a judicial authority or 
 an independent administrative authority whose decision is binding of the Member State 
in which the use is to take place, issued upon a reasoned request and in accordance with the 
detailed rules of national law referred to in paragraph 5. However, in a duly justified 
situation of urgency, the use of such system may be commenced without an authorisation 
provided that such authorisation is requested without undue delay, at the latest within 24 
hours. If such authorisation is rejected, the use shall be stopped with immediate effect 
and all the data, as well as the results and outputs of that use shall be immediately 
discarded and deleted.
The competent judicial authority or an independent administrative authority whose 
decision is binding shall grant the authorisation only where it is satisfied, on the basis of 
objective evidence or clear indications presented to it, that the use of the ‘real-time’ remote 
biometric identification system concerned is necessary for, and proportionate to, achieving 
one of the objectives specified in paragraph 1, point (h), as identified in the request and, in 
particular, remains limited to what is strictly necessary concerning the period of time as 
well as the geographic and personal scope. In deciding on the request, that authority shall 
take into account the elements referred to in paragraph 2. No decision that produces an 
adverse legal effect on a person may be taken based solely on the output of the ‘real-
time’ remote biometric identification system."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,5,2.1.5,1,"4. Without prejudice to paragraph 3, each use of a ‘real-time’ remote biometric 
identification system in publicly accessible spaces for law enforcement purposes shall be 
notified to the relevant market surveillance authority and the national data protection 
authority in accordance with the national rules referred to in paragraph 5. The 
notification shall, as a minimum, contain the information specified under paragraph 6 
and shall not include sensitive operational data."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,6,2.1.6,1,"5. A Member State may decide to provide for the possibility to fully or partially authorise the 
use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for 
the purposes of law enforcement within the limits and under the conditions listed in 
paragraph 1, point (h), and paragraphs 2 and 3.  Member States concerned shall lay 
down in their national law the necessary detailed rules for the request, issuance and 
exercise of, as well as supervision and reporting relating to, the authorisations referred to 
in paragraph 3. Those rules shall also specify in respect of which of the objectives listed in 
paragraph 1, point (h), including which of the criminal offences referred to in point (h)(iii) 
thereof, the competent authorities may be authorised to use those systems for the purposes 
of law enforcement. Member States shall notify those rules to the Commission at the 
latest 30 days following the adoption thereof. Member States may introduce, in 
accordance with Union law, more restrictive laws on the use of remote biometric 
identification systems."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,7,2.1.7,1,"6. National market surveillance authorities and the national data protection authorities of 
Member States that have been notified of the use of ‘real-time’ remote biometric 
identification systems in publicly accessible spaces for law enforcement purposes 
pursuant to paragraph 4 shall submit to the Commission annual reports on such use. 
For that purpose, the Commission shall provide Member States and national market 
surveillance and data protection authorities with a template, including information on 
the number of the decisions taken by competent judicial authorities or an independent 
administrative authority whose decision is binding upon requests for authorisations in 
accordance with paragraph 3 and their result."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,8,2.1.8,1,"7. The Commission shall publish annual reports on the use of real-time remote biometric 
identification systems in publicly accessible spaces for law enforcement purposes, based 
on aggregated data in Member States on the basis of the annual reports referred to in 
paragraph 6. Those annual reports shall not include sensitive operational data of the 
related law enforcement activities."
2,Prohibited artificial intelligence practices,1,2.1,Prohibited ai practices,9,2.1.9,1,"8. This Article shall not affect the prohibitions that apply where an AI practice infringes 
other Union law."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,1,3.1.1,1,"Article 6
Classification rules for high-risk AI systems"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,2,3.1.2,1,"1. Irrespective of whether an AI system is placed on the market or put into service 
independently from the products referred to in points (a) and (b), that AI system shall be 
considered to be high-risk where both of the following conditions are fulfilled:"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,2,3.1.2,2,"(a) the AI system is intended to be used as a safety component of a product, or the AI 
system is itself a product, covered by the Union harmonisation legislation listed in 
Annex I;"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,2,3.1.2,3,"(b) the product whose safety component pursuant to point (a) is the AI system, or the 
AI system itself as a product, is required to undergo a third-party conformity 
assessment, with a view to the placing on the market or the putting into service of 
that product pursuant to the Union harmonisation legislation listed in Annex I."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,3,3.1.3,1,"2. In addition to the high-risk AI systems referred to in paragraph 1, AI systems referred to in 
Annex III shall be considered to be high-risk."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,4,3.1.4,1,"3. By derogation from paragraph 2, an AI system shall not be considered to be high-risk if 
it does not pose a significant risk of harm to the health, safety or fundamental rights of 
natural persons, including by not materially influencing the outcome of decision 
making. This shall be the case where one or more of the following conditions are 
fulfilled:"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,4,3.1.4,2,(a) the AI system is intended to perform a narrow procedural task;
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,4,3.1.4,3,"(b) the AI system is intended to improve the result of a previously completed human 
activity;"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,4,3.1.4,4,"(c) the AI system is intended to detect decision-making patterns or deviations from 
prior decision-making patterns and is not meant to replace or influence the 
previously completed human assessment, without proper human review; or"
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,4,3.1.4,5,"(d) the AI system is intended to perform a preparatory task to an assessment relevant 
for the purposes of the use cases listed in Annex III.
Notwithstanding the first subparagraph, an AI system referred to in Annex III shall 
always be considered to be high-risk where the AI system performs profiling of natural 
persons."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,5,3.1.5,1,"4. A provider who considers that an AI system referred to in Annex III is not high-risk 
shall document its assessment before that system is placed on the market or put into 
service. Such provider shall be subject to the registration obligation set out in 
Article 49(2). Upon request of national competent authorities, the provider shall provide 
the documentation of the assessment."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,6,3.1.6,1,"5. The Commission shall, after consulting the European Artificial Intelligence Board (the 
‘Board’), and no later than … [18 months from the date of entry into force of this 
Regulation], provide guidelines specifying the practical implementation of this Article in 
line with Article 96 together with a comprehensive list of practical examples of use cases 
of AI systems that are high-risk and not high-risk."
3,High-risk ai systems,1,3.1,Classification rules for high-risk ai systems,7,3.1.7,1,"6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
conditions laid down in paragraph 3, first subparagraph, of this Article.
The Commission may adopt delegated acts in accordance with Article 97 in order to add 
new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
only where there is concrete and reliable evidence of the existence of AI systems that fall 
under the scope of Annex III but do not pose a significant risk of harm to the health, 
safety or fundamental rights of natural persons.
The Commission shall adopt delegated acts in accordance with Article 97 in order to 
delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
there is concrete and reliable evidence that this is necessary for the purpose of 
maintaining the level of protection of health, safety and fundamental rights in the 
Union.
Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
not decrease the overall level of protection of health, safety and fundamental rights in 
the Union.
When adopting the delegated acts, the Commission shall ensure consistency with the 
delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
technological developments."
3,High-risk ai systems,2,3.2,Amendments to annex iii,1,3.2.1,1,"Article 7
Amendments to Annex III"
3,High-risk ai systems,2,3.2,Amendments to annex iii,2,3.2.2,1,"1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
III by adding or modifying use-cases of high-risk AI systems where both of the following 
conditions are fulfilled:"
3,High-risk ai systems,2,3.2,Amendments to annex iii,2,3.2.2,2,(a) the AI systems are intended to be used in any of the areas listed in Annex III;
3,High-risk ai systems,2,3.2,Amendments to annex iii,2,3.2.2,3,"(b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
of adverse impact posed by the high-risk AI systems already referred to in Annex III."
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,1,"2. When assessing the condition under paragraph 1, point (b),, the Commission shall take into 
account the following criteria:"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,2,(a) the intended purpose of the AI system;
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,3,(b) the extent to which an AI system has been used or is likely to be used;
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,4,"(c) the nature and amount of the data processed and used by the AI system, in 
particular whether special categories of personal data are processed;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,5,"(d) the extent to which the AI system acts autonomously and the possibility for a 
human to override a decision or recommendations that may lead to potential harm;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,6,"(e) the extent to which the use of an AI system has already caused harm to  health and 
safety, has had an adverse impact on  fundamental rights or has given rise to 
significant concerns in relation to the likelihood of such harm or adverse impact, as 
demonstrated, for example, by reports or documented allegations submitted to 
national competent authorities or by other reports, as appropriate;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,7,"(f) the potential extent of such harm or such adverse impact, in particular in terms of its 
intensity and its ability to affect multiple persons or to disproportionately affect a 
particular group of persons;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,8,"(g) the extent to which persons who are potentially harmed or suffer an adverse impact 
are dependent on the outcome produced with an AI system, in particular because for 
practical or legal reasons it is not reasonably possible to opt-out from that outcome;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,9,"(h) the extent to which there is an imbalance of power, or the persons who are 
potentially harmed or suffer an adverse impact are in a vulnerable position in relation 
to the deployer of an AI system, in particular due to status, authority, knowledge, 
economic or social circumstances, or age;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,10,"(i) the extent to which the outcome produced involving an AI system is easily corrigible 
or reversible, taking into account the technical solutions available to correct or 
reverse it, whereby outcomes having an adverse impact on  health, safety or 
fundamental rights, shall not be considered to be easily corrigible or reversible;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,11,"(j) the magnitude and likelihood of benefit of the deployment of the AI system for 
individuals, groups, or society at large, including possible improvements in product 
safety;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,12,(k) the extent to which existing Union law provides for:
3,High-risk ai systems,2,3.2,Amendments to annex iii,3,3.2.3,13,"(i) effective measures of redress in relation to the risks posed by an AI system, 
with the exclusion of claims for damages;
(ii) effective measures to prevent or substantially minimise those risks."
3,High-risk ai systems,2,3.2,Amendments to annex iii,4,3.2.4,1,"3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
list in Annex III by removing high-risk AI systems where both of the following 
conditions are fulfilled:"
3,High-risk ai systems,2,3.2,Amendments to annex iii,4,3.2.4,2,"(a) the high-risk AI system concerned no longer poses any significant risks to 
fundamental rights, health or safety, taking into account the criteria listed in 
paragraph 2;"
3,High-risk ai systems,2,3.2,Amendments to annex iii,4,3.2.4,3,"(b) the deletion does not decrease the overall level of protection of health, safety and 
fundamental rights under Union law.
Section 2
Requirements for high-risk AI systems"
3,High-risk ai systems,3,3.3,Compliance with the requirements,1,3.3.1,1,"Article 8
Compliance with the requirements"
3,High-risk ai systems,3,3.3,Compliance with the requirements,2,3.3.2,1,"1. High-risk AI systems shall comply with the requirements laid down in this Section, taking 
into account their intended purposes as well as the generally acknowledged state of the 
art on AI and AI-related technologies. The risk management system referred to in 
Article 9 shall be taken into account when ensuring compliance with those 
requirements."
3,High-risk ai systems,3,3.3,Compliance with the requirements,3,3.3.3,1,"2. Where a product contains an AI system, to which the requirements of this Regulation as 
well as requirements of the Union harmonisation legislation listed in Section A of Annex 
I apply, providers shall be responsible for ensuring that their product is fully compliant 
with all applicable requirements required under applicable Union harmonisation 
legislation. In ensuring the compliance of high-risk AI systems referred to in paragraph"
3,High-risk ai systems,3,3.3,Compliance with the requirements,4,3.3.4,1,"1 with the requirements set out in this Section, and in order to ensure consistency, avoid 
duplications and minimise additional burdens, providers shall have a choice of 
integrating, as appropriate, the necessary testing and reporting processes, information 
and documentation they provide with regard to their product into documentation and 
procedures that already exist and are required under the Union harmonisation 
legislation listed in Section A of Annex I,."
3,High-risk ai systems,4,3.4,Risk management system,1,3.4.1,1,"Article 9
Risk management system"
3,High-risk ai systems,4,3.4,Risk management system,2,3.4.2,1,"1. A risk management system shall be established, implemented, documented and maintained 
in relation to high-risk AI systems."
3,High-risk ai systems,4,3.4,Risk management system,3,3.4.3,1,"2. The risk management system shall be understood as a continuous iterative process 
planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular 
systematic review and updating. It shall comprise the following steps:"
3,High-risk ai systems,4,3.4,Risk management system,3,3.4.3,2,"(a) the identification and analysis of the known and the reasonably foreseeable risks 
that the high-risk AI system can pose to health, safety or fundamental rights when 
the high-risk AI system is used in accordance with its intended purpose;"
3,High-risk ai systems,4,3.4,Risk management system,3,3.4.3,3,"(b) the estimation and evaluation of the risks that may emerge when the high-risk AI 
system is used in accordance with its intended purpose, and under conditions of 
reasonably foreseeable misuse;"
3,High-risk ai systems,4,3.4,Risk management system,3,3.4.3,4,"(c) the evaluation of other risks possibly arising, based on the analysis of data gathered 
from the post-market monitoring system referred to in Article 72;"
3,High-risk ai systems,4,3.4,Risk management system,3,3.4.3,5,"(d) the adoption of appropriate and targeted risk management measures designed to 
address the risks identified pursuant to point (a) ."
3,High-risk ai systems,4,3.4,Risk management system,4,3.4.4,1,"3. The risks referred to in this Article shall concern only those which may be reasonably 
mitigated or eliminated through the development or design of the high-risk AI system, or 
the provision of adequate technical information."
3,High-risk ai systems,4,3.4,Risk management system,5,3.4.5,1,"4. The risk management measures referred to in paragraph 2, point (d), shall give due 
consideration to the effects and possible interaction resulting from the combined 
application of the requirements set out in this Section, with a view to minimising risks 
more effectively while achieving an appropriate balance in implementing the measures 
to fulfil those requirements."
3,High-risk ai systems,4,3.4,Risk management system,6,3.4.6,1,"5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
relevant residual risk associated with each hazard, as well as the overall residual risk of the 
high-risk AI systems is judged to be acceptable.
In identifying the most appropriate risk management measures, the following shall be 
ensured:"
3,High-risk ai systems,4,3.4,Risk management system,6,3.4.6,2,"(a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
as far as technically feasible through adequate design and development of the high-
risk AI system;"
3,High-risk ai systems,4,3.4,Risk management system,6,3.4.6,3,"(b) where appropriate, implementation of adequate mitigation and control measures 
addressing risks that cannot be eliminated;"
3,High-risk ai systems,4,3.4,Risk management system,6,3.4.6,4,"(c) provision of information required pursuant to Article 13 and, where appropriate, 
training to deployers. 
With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
due consideration shall be given to the technical knowledge, experience, education, the 
training to be expected by the deployer, and the presumable context in which the system is 
intended to be used."
3,High-risk ai systems,4,3.4,Risk management system,7,3.4.7,1,"6. High-risk AI systems shall be tested for the purpose of identifying the most appropriate 
and targeted risk management measures. Testing shall ensure that high-risk AI systems 
perform consistently for their intended purpose and that they are in compliance with the 
requirements set out in this Section."
3,High-risk ai systems,4,3.4,Risk management system,8,3.4.8,1,"7. Testing procedures may include testing in real-world conditions in accordance with 
Article 60."
3,High-risk ai systems,4,3.4,Risk management system,9,3.4.9,1,"8. The testing of high-risk AI systems shall be performed, as appropriate, at any time 
throughout the development process, and, in any event, prior to their being placed on the 
market or put into service. Testing shall be carried out against prior defined metrics and 
probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI 
system."
3,High-risk ai systems,4,3.4,Risk management system,10,3.4.10,1,"9. When implementing the risk management system as provided for in paragraphs 1 to 7, 
providers shall give consideration to whether in view of its intended purpose the high-risk 
AI system is likely to have an adverse impact on persons under the age of 18 and, as 
appropriate, other groups of vulnerable persons."
3,High-risk ai systems,4,3.4,Risk management system,11,3.4.11,1,"10. For providers of high-risk AI systems that are subject to requirements regarding internal 
risk management processes under other relevant provisions of Union law, the aspects 
provided in paragraphs 1 to 9 may be part of, or combined with, the risk management 
procedures established  pursuant to that law."
3,High-risk ai systems,5,3.5,Data and data governance,1,3.5.1,1,"Article 10
Data and data governance"
3,High-risk ai systems,5,3.5,Data and data governance,2,3.5.2,1,"1. High-risk AI systems which make use of techniques involving the training of AI models 
with data shall be developed on the basis of training, validation and testing data sets that 
meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used."
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,1,"2. Training, validation and testing data sets shall be subject to data governance and 
management practices appropriate for the intended purpose of the high-risk AI system. 
Those practices shall concern in particular:"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,2,(a) the relevant design choices;
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,3,"(b) data collection processes and the origin of data, and in the case of personal data, 
the original purpose of the data collection;"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,4,"(c) relevant data-preparation processing operations, such as annotation, labelling, 
cleaning, updating, enrichment and aggregation;"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,5,"(d) the formulation of  assumptions, in particular with respect to the information that 
the data are supposed to measure and represent;"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,6,"(e) an assessment of the availability, quantity and suitability of the data sets that are 
needed;"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,7,"(f) examination in view of possible biases that are likely to affect the health and safety 
of persons, have a negative impact on fundamental rights or lead to discrimination 
prohibited under Union law, especially where data outputs influence inputs for 
future operations;"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,8,"(g) appropriate measures to detect, prevent and mitigate possible biases identified 
according to point (f);"
3,High-risk ai systems,5,3.5,Data and data governance,3,3.5.3,9,"(h) the identification of relevant data gaps or shortcomings that prevent compliance 
with this Regulation, and how those gaps and shortcomings can be addressed."
3,High-risk ai systems,5,3.5,Data and data governance,4,3.5.4,1,"3. Training, validation and testing data sets shall be relevant, sufficiently representative, and 
to the best extent possible, free of errors and complete in view of the intended purpose. 
They shall have the appropriate statistical properties, including, where applicable, as 
regards the persons or groups of persons in relation to whom the high-risk AI system is 
intended to be used. Those characteristics of the data sets may be met at the level of 
individual data sets or at the level of a combination thereof."
3,High-risk ai systems,5,3.5,Data and data governance,5,3.5.5,1,"4. Data sets shall take into account, to the extent required by the intended purpose, the 
characteristics or elements that are particular to the specific geographical, contextual, 
behavioural or functional setting within which the high-risk AI system is intended to be 
used."
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,1,"5. To the extent that it is strictly necessary for the purpose of ensuring bias  detection and 
correction in relation to the high-risk AI systems in accordance with paragraph (2), points"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,2,"(f) and (g) of this Article, the providers of such systems may exceptionally process special 
categories of personal data, subject to appropriate safeguards for the fundamental rights 
and freedoms of natural persons. In addition to the provisions set out in Regulation (EU) 
2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following 
conditions shall apply in order for such processing to occur:"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,3,"(a) the bias detection and correction cannot be effectively fulfilled by processing other 
data, including synthetic or anonymised data;"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,4,"(b) the special categories of personal data are subject to technical limitations on the 
re-use of the personal data, and state of the art security and privacy-preserving 
measures, including pseudonymisation;"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,5,"(c) the special categories of personal data are subject to measures to ensure that the 
personal data processed are secured, protected, subject to suitable safeguards, 
including strict controls and documentation of the access, to avoid misuse and 
ensure that only authorised persons with appropriate confidentiality obligations 
have access to those personal data;"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,6,"(d) the personal data in the special categories of personal data are not to be 
transmitted, transferred or otherwise accessed by other parties;"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,7,"(e) the personal data in the special categories of personal data are deleted once the 
bias has been corrected or the personal data has reached the end of its retention 
period, whichever comes first;"
3,High-risk ai systems,5,3.5,Data and data governance,6,3.5.6,8,"(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and 
(EU) 2018/1725 and Directive (EU) 2016/680include the reasons why the 
processing of special categories of personal data was strictly necessary to detect 
and correct biases, and why that objective could not be achieved by processing 
other data."
3,High-risk ai systems,5,3.5,Data and data governance,7,3.5.7,1,"6. For the development of high-risk AI systems not using techniques involving the training 
of AI models, paragraphs 2 to 5 apply only to the testing data sets."
3,High-risk ai systems,6,3.6,Technical documentation,1,3.6.1,1,"Article 11
Technical documentation"
3,High-risk ai systems,6,3.6,Technical documentation,2,3.6.2,1,"1. The technical documentation of a high-risk AI system shall be drawn up before that system 
is placed on the market or put into service and shall be kept up-to date.
The technical documentation shall be drawn up in such a way as to demonstrate that the 
high-risk AI system complies with the requirements set out in this Section and to provide 
national competent authorities and notified bodies with the necessary information in a 
clear and comprehensive form to assess the compliance of the AI system with those 
requirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, 
including start-ups, may provide the elements of the technical documentation specified 
in Annex IV in a simplified manner. For this purpose, the Commission shall establish a 
simplified technical documentation form targeted at the needs of small and 
microenterprises. Where an SME, including a start-up, opts to provide the information 
required in Annex IV in a simplified manner, it shall use the form referred to in this 
paragraph. Notified bodies shall accept the form for the purposes of the conformity 
assessment."
3,High-risk ai systems,6,3.6,Technical documentation,3,3.6.3,1,"2. Where a high-risk AI system related to a product covered by the Union harmonisation 
legislation listed in Section A of Annex I is placed on the market or put into service, a 
single set of technical documentation shall be drawn up containing all the information set 
out in paragraph 1, as well as the information required under those legal acts."
3,High-risk ai systems,6,3.6,Technical documentation,4,3.6.4,1,"3. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
IV where necessary to ensure that, in the light of technical progress, the technical 
documentation provides all the information necessary to assess the compliance of the 
system with the requirements set out in this Section."
3,High-risk ai systems,7,3.7,Record-keeping,1,3.7.1,1,"Article 12
Record-keeping"
3,High-risk ai systems,7,3.7,Record-keeping,2,3.7.2,1,"1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) 
over their lifetime."
3,High-risk ai systems,7,3.7,Record-keeping,3,3.7.3,1,"2. In order to ensure a level of traceability of the functioning of a high-risk AI system  that 
is appropriate to the intended purpose of the system, logging capabilities shall enable the 
recording of events relevant for:"
3,High-risk ai systems,7,3.7,Record-keeping,3,3.7.3,2,"(a) identifying situations that may result in the high-risk AI system presenting a risk 
within the meaning of Article 79(1) or in a substantial modification;"
3,High-risk ai systems,7,3.7,Record-keeping,3,3.7.3,3,(b) facilitating the post-market monitoring referred to in Article 72; and
3,High-risk ai systems,7,3.7,Record-keeping,3,3.7.3,4,(c) monitoring the operation of high-risk AI systems referred to in Article 26(6).
3,High-risk ai systems,7,3.7,Record-keeping,4,3.7.4,1,"3. For high-risk AI systems referred to in point 1 (a) of Annex III, the logging capabilities 
shall provide, at a minimum:"
3,High-risk ai systems,7,3.7,Record-keeping,4,3.7.4,2,"(a) recording of the period of each use of the system (start date and time and end date 
and time of each use);"
3,High-risk ai systems,7,3.7,Record-keeping,4,3.7.4,3,(b) the reference database against which input data has been checked by the system;
3,High-risk ai systems,7,3.7,Record-keeping,4,3.7.4,4,(c) the input data for which the search has led to a match;
3,High-risk ai systems,7,3.7,Record-keeping,4,3.7.4,5,"(d) the identification of the natural persons involved in the verification of the results, as 
referred to in Article 14(5)."
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,1,3.8.1,1,"Article 13
Transparency and provision of information to deployers"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,2,3.8.2,1,"1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
operation is sufficiently transparent to enable deployers to interpret a system’s output and 
use it appropriately. An appropriate type and degree of transparency shall be ensured  
with a view to achieving compliance with the relevant obligations of the provider and 
deployer set out in Section 3."
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,3,3.8.3,1,"2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital 
format or otherwise that include concise, complete, correct and clear information that is 
relevant, accessible and comprehensible to deployers."
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,1,3. The instructions for use shall contain at least the following information:
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,2,"(a) the identity and the contact details of the provider and, where applicable, of its 
authorised representative;"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,3,"(b) the characteristics, capabilities and limitations of performance of the high-risk AI 
system, including:"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,4,"(i) its intended purpose;
(ii) the level of accuracy, including its metrics, robustness and cybersecurity 
referred to in Article 15 against which the high-risk AI system has been tested 
and validated and which can be expected, and any known and foreseeable 
circumstances that may have an impact on that expected level of accuracy, 
robustness and cybersecurity;
(iii) any known or foreseeable circumstance, related to the use of the high-risk AI 
system in accordance with its intended purpose or under conditions of 
reasonably foreseeable misuse, which may lead to risks to the health and safety 
or fundamental rights referred to in Article 9(2);
(iv) where applicable, the technical capabilities and characteristics of the high-
risk AI system to provide information that is relevant to explain its output;
(v) when appropriate, its performance regarding specific persons or groups of 
persons on which the system is intended to be used;
(vi) when appropriate, specifications for the input data, or any other relevant 
information in terms of the training, validation and testing data sets used, 
taking into account the intended purpose of the high-risk AI system;
(vii) where applicable, information to enable deployers to interpret the output of 
the high-risk AI system and use it appropriately;
(i) its intended purpose;
(ii) the level of accuracy, including its metrics, robustness and cybersecurity 
referred to in Article 15 against which the high-risk AI system has been tested 
and validated and which can be expected, and any known and foreseeable 
circumstances that may have an impact on that expected level of accuracy, 
robustness and cybersecurity;
(iii) any known or foreseeable circumstance, related to the use of the high-risk AI 
system in accordance with its intended purpose or under conditions of 
reasonably foreseeable misuse, which may lead to risks to the health and safety 
or fundamental rights referred to in Article 9(2);
(iv) where applicable, the technical capabilities and characteristics of the high-
risk AI system to provide information that is relevant to explain its output;
(v) when appropriate, its performance regarding specific persons or groups of 
persons on which the system is intended to be used;
(vi) when appropriate, specifications for the input data, or any other relevant 
information in terms of the training, validation and testing data sets used, 
taking into account the intended purpose of the high-risk AI system;
(vii) where applicable, information to enable deployers to interpret the output of 
the high-risk AI system and use it appropriately;"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,5,"(c) the changes to the high-risk AI system and its performance which have been pre-
determined by the provider at the moment of the initial conformity assessment, if 
any;"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,6,"(d) the human oversight measures referred to in Article 14, including the technical 
measures put in place to facilitate the interpretation of the outputs of the high-risk AI 
systems by the deployers;"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,7,"(e) the computational and hardware resources needed, the expected lifetime of the 
high-risk AI system and any necessary maintenance and care measures, including 
their frequency, to ensure the proper functioning of that AI system, including as 
regards software updates;"
3,High-risk ai systems,8,3.8,Transparency and provision of information to deployers,4,3.8.4,8,"(f) where relevant, a description of the mechanisms included within the high-risk AI 
system that allows deployers to properly collect, store and interpret the logs in 
accordance with Article 12."
3,High-risk ai systems,9,3.9,Human oversight,1,3.9.1,1,"Article 14
Human oversight"
3,High-risk ai systems,9,3.9,Human oversight,2,3.9.2,1,"1. High-risk AI systems shall be designed and developed in such a way, including with 
appropriate human-machine interface tools, that they can be effectively overseen by natural 
persons during the period in which they are in use."
3,High-risk ai systems,9,3.9,Human oversight,3,3.9.3,1,"2. Human oversight shall aim to prevent or minimise the risks to health, safety or 
fundamental rights that may emerge when a high-risk AI system is used in accordance with 
its intended purpose or under conditions of reasonably foreseeable misuse, in particular 
where such risks persist despite the application of other requirements set out in this 
Section."
3,High-risk ai systems,9,3.9,Human oversight,4,3.9.4,1,"3. The oversight measures shall be commensurate to the risks, level of autonomy and 
context of use of the high-risk AI system, and shall be ensured through either one or both 
of the following types of measures:"
3,High-risk ai systems,9,3.9,Human oversight,4,3.9.4,2,"(a) measures identified and built, when technically feasible, into the high-risk AI system 
by the provider before it is placed on the market or put into service;"
3,High-risk ai systems,9,3.9,Human oversight,4,3.9.4,3,"(b) measures identified by the provider before placing the high-risk AI system on the 
market or putting it into service and that are appropriate to be implemented by the 
deployer."
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,1,"4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
provided to the user in such a way that natural persons to whom human oversight is 
assigned are enabled, as appropriate and proportionate to the following circumstances:"
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,2,"(a) to properly understand the relevant capacities and limitations of the high-risk AI 
system and be able to duly monitor its operation, including in view of detecting and 
addressing anomalies, dysfunctions and unexpected performance ;"
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,3,"(b) to remain aware of the possible tendency of automatically relying or over-relying on 
the output produced by a high-risk AI system (‘automation bias’), in particular for 
high-risk AI systems used to provide information or recommendations for decisions 
to be taken by natural persons;"
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,4,"(c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
example, the interpretation tools and methods available;"
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,5,"(d)  to decide, in any particular situation, not to use the high-risk AI system or to 
otherwise disregard, override or reverse the output of the high-risk AI system;"
3,High-risk ai systems,9,3.9,Human oversight,5,3.9.5,6,"(e)  to intervene in the operation of the high-risk AI system or interrupt the system 
through a ‘stop’ button or a similar procedure that allows the system to come to a 
halt in a safe state."
3,High-risk ai systems,9,3.9,Human oversight,6,3.9.6,1,"5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in 
paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision 
is taken by the deployer on the basis of the identification resulting from the system unless 
that identification has been separately verified and confirmed by at least two natural 
persons with the necessary competence, training and authority.
The requirement for a separate verification by at least two natural persons shall not 
apply to high-risk AI systems used for the purposes of law enforcement, migration, 
border control or asylum, where Union or national law considers the application of this 
requirement to be disproportionate."
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",1,3.10.1,1,"Article 15
Accuracy, robustness and cybersecurity"
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",2,3.10.2,1,"1. High-risk AI systems shall be designed and developed in such a way that they achieve  
an appropriate level of accuracy, robustness, and cybersecurity, and that they perform 
consistently in those respects throughout their lifecycle."
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",3,3.10.3,1,"2. To address the technical aspects of how to measure the appropriate levels of accuracy 
and robustness set out in paragraph 1 and any other relevant performance metrics, the 
Commission shall, in cooperation with relevant stakeholder and organisations such as 
metrology and benchmarking authorities, encourage, as appropriate, the development of 
benchmarks and measurement methodologies."
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",4,3.10.4,1,"3. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be 
declared in the accompanying instructions of use."
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",5,3.10.5,1,"4. High-risk AI systems shall be as resilient as possible regarding errors, faults or 
inconsistencies that may occur within the system or the environment in which the system 
operates, in particular due to their interaction with natural persons or other systems. 
Technical and organisational measures shall be taken towards this regard.
The robustness of high-risk AI systems may be achieved through technical redundancy 
solutions, which may include backup or fail-safe plans.
High-risk AI systems that continue to learn after being placed on the market or put into 
service shall be developed in such a way as to eliminate or reduce as far as possible the 
risk of possibly biased outputs influencing input for future operations (‘feedback loops’), 
and as to ensure that any such feedback loops are duly addressed with appropriate 
mitigation measures."
3,High-risk ai systems,10,3.10,"Accuracy, robustness and cybersecurity",6,3.10.6,1,"5. High-risk AI systems shall be resilient against attempts by unauthorised third parties to 
alter their use, outputs or performance by exploiting system vulnerabilities.
The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be 
appropriate to the relevant circumstances and the risks.
The technical solutions to address AI specific vulnerabilities shall include, where 
appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying 
to manipulate the training data set (‘data poisoning’), or pre-trained components used in 
training (‘model poisoning’), inputs designed to cause the AI model to make a mistake 
(‘adversarial examples’ or ‘model evasion’), confidentiality attacks or model flaws.
Section 3
Obligations of providers and deployers of high-risk AI systems and 
other parties"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,1,"Article 16
Obligations of providers of high-risk AI systems 
Providers of high-risk AI systems shall:"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,2,"(a) ensure that their high-risk AI systems are compliant with the requirements set out in 
Section 2;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,3,"(b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
accompanying documentation, as applicable their name, registered trade name or 
registered trade mark, the address at which they can be contacted;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,4,(c) have a quality management system in place which complies with Article 17;
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,5,(d) keep the documentation referred to in Article 18;
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,6,"(e) when under their control, keep the logs automatically generated by their high-risk AI 
systems as referred to in Article 19;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,7,"(f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
procedure as referred to in Article 43, prior to its being placed on the market or put into 
service;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,8,(g) draw up an EU declaration of conformity in accordance with Article 47;
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,9,"(h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
packaging or its accompanying documentation, to indicate conformity with this 
Regulation, in accordance with Article 48;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,10,(i) comply with the registration obligations referred to in Article 49(1);
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,11,(j) take the necessary corrective actions and provide information as required in Article 20;
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,12,"(k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
the high-risk AI system with the requirements set out in Section 2;"
3,High-risk ai systems,11,3.11,Obligations of providers of high-risk ai systems,1,3.11.1,13,"(l) ensure that the high-risk AI system complies with accessibility requirements in 
accordance with Directives (EU) 2016/2102 and (EU) 2019/882."
3,High-risk ai systems,12,3.12,Quality management system,1,3.12.1,1,"Article 17
Quality management system"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,1,"1. Providers of high-risk AI systems shall put a quality management system in place that 
ensures compliance with this Regulation. That system shall be documented in a systematic 
and orderly manner in the form of written policies, procedures and instructions, and shall 
include at least the following aspects:"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,2,"(a) a strategy for regulatory compliance, including compliance with conformity 
assessment procedures and procedures for the management of modifications to the 
high-risk AI system;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,3,"(b) techniques, procedures and systematic actions to be used for the design, design 
control and design verification of the high-risk AI system;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,4,"(c) techniques, procedures and systematic actions to be used for the development, 
quality control and quality assurance of the high-risk AI system;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,5,"(d) examination, test and validation procedures to be carried out before, during and after 
the development of the high-risk AI system, and the frequency with which they have 
to be carried out;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,6,"(e) technical specifications, including standards, to be applied and, where the relevant 
harmonised standards are not applied in full or do not cover all of the relevant 
requirements set out in Section 2, the means to be used to ensure that the high-risk 
AI system complies with those requirements ;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,7,"(f) systems and procedures for data management, including data acquisition, data 
collection, data analysis, data labelling, data storage, data filtration, data mining, data 
aggregation, data retention and any other operation regarding the data that is 
performed before and for the purpose of the placing on the market or the putting into 
service of high-risk AI systems;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,8,(g) the risk management system referred to in Article 9;
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,9,"(h) the setting-up, implementation and maintenance of a post-market monitoring system, 
in accordance with Article 72;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,10,"(i) procedures related to the reporting of a serious incident in accordance with 
Article 73;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,11,"(j) the handling of communication with national competent authorities, other relevant 
authorities, including those providing or supporting the access to data, notified 
bodies, other operators, customers or other interested parties;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,12,"(k) systems and procedures for record-keeping of all relevant documentation and 
information;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,13,"(l) resource management, including security-of-supply related measures;"
3,High-risk ai systems,12,3.12,Quality management system,2,3.12.2,14,"(m) an accountability framework setting out the responsibilities of the management and 
other staff with regard to all the aspects listed in this paragraph."
3,High-risk ai systems,12,3.12,Quality management system,3,3.12.3,1,"2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
size of the provider’s organisation. Providers shall in any event comply with the degree of 
rigour and the level of protection required to ensure the compliance of their high-risk AI 
systems with this Regulation."
3,High-risk ai systems,12,3.12,Quality management system,4,3.12.4,1,"3. Providers of high-risk AI systems that are subject to obligations regarding quality 
management systems or an equivalent function under relevant sectorial Union law may 
include the aspects listed in paragraph 1 as part of the quality management systems 
pursuant to that law."
3,High-risk ai systems,12,3.12,Quality management system,5,3.12.5,1,"4. For providers that are financial institutions subject to requirements regarding their 
internal governance, arrangements or processes under Union financial services law, the 
obligation to put in place a quality management system, with the exception of paragraph 
1, points (g), (h) and (i) of this Article, shall be deemed to be fulfilled by complying with 
the rules on internal governance arrangements or processes pursuant to the relevant Union 
financial services law. For this purpose, any harmonised standards referred to in Article 40 
shall be taken into account."
3,High-risk ai systems,13,3.13,Documentation keeping,1,3.13.1,1,"Article 18
Documentation keeping"
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,1,"1. The provider shall, for a period ending 10 years after the high-risk AI system has been 
placed on the market or put into service, keep at the disposal of the national competent 
authorities:"
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,2,(a) the technical documentation referred to in Article 11;
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,3,"(b) the documentation concerning the quality management system referred to in 
Article 17;"
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,4,"(c) the documentation concerning the changes approved by notified bodies, where 
applicable;"
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,5,"(d) the decisions and other documents issued by the notified bodies, where applicable;"
3,High-risk ai systems,13,3.13,Documentation keeping,2,3.13.2,6,(e) the EU declaration of conformity referred to in Article 47.
3,High-risk ai systems,13,3.13,Documentation keeping,3,3.13.3,1,"2. Each Member State shall determine conditions under which the documentation referred 
to in paragraph 1 remains at the disposal of the national competent authorities for the 
period indicated in that paragraph for the cases when a provider or its authorised 
representative established on its territory goes bankrupt or ceases its activity prior to the 
end of that period."
3,High-risk ai systems,13,3.13,Documentation keeping,4,3.13.4,1,"3. Providers that are financial institutions subject to requirements regarding their internal 
governance, arrangements or processes under Union financial services law shall 
maintain the technical documentation as part of the documentation kept under the relevant 
Union financial services law."
3,High-risk ai systems,14,3.14,Automatically generated logs,1,3.14.1,1,"Article 19
Automatically generated logs"
3,High-risk ai systems,14,3.14,Automatically generated logs,2,3.14.2,1,"1. Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), 
automatically generated by their high-risk AI systems, to the extent such logs are under 
their control. Without prejudice to applicable Union or national law, the logs shall be 
kept for a period  appropriate to the intended purpose of the high-risk AI system, of at 
least six months, unless provided otherwise in the applicable Union or national law, in 
particular in Union law on the protection of personal data."
3,High-risk ai systems,14,3.14,Automatically generated logs,3,3.14.3,1,"2. Providers that are financial institutions subject to requirements regarding their internal 
governance, arrangements or processes under Union financial services law shall 
maintain the logs automatically generated by their high-risk AI systems as part of the 
documentation kept under the relevant financial services law."
3,High-risk ai systems,15,3.15,Corrective actions and duty of information,1,3.15.1,1,"Article 20
Corrective actions and duty of information"
3,High-risk ai systems,15,3.15,Corrective actions and duty of information,2,3.15.2,1,"1. Providers of high-risk AI systems which consider or have reason to consider that a high-
risk AI system that they have placed on the market or put into service is not in conformity 
with this Regulation shall immediately take the necessary corrective actions to bring that 
system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
shall inform the distributors of the high-risk AI system concerned and, where applicable, 
the deployers, the authorised representative and importers accordingly."
3,High-risk ai systems,15,3.15,Corrective actions and duty of information,3,3.15.3,1,"2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and 
the provider becomes aware of that risk, it shall immediately investigate the causes, in 
collaboration with the reporting deployer, where applicable, and inform the market 
surveillance authorities of the Member State or Member States in which they made the 
high-risk AI system available on the market and, where applicable, the notified body that 
issued a certificate for that high-risk AI system in accordance with Article 44, in 
particular, of the nature of the non-compliance and of any relevant corrective action 
taken."
3,High-risk ai systems,16,3.16,Cooperation with competent authorities,1,3.16.1,1,"Article 21
Cooperation with competent authorities"
3,High-risk ai systems,16,3.16,Cooperation with competent authorities,2,3.16.2,1,"1. Providers of high-risk AI systems shall, upon a reasoned request by a  competent 
authority, provide that authority  all the information and documentation necessary to 
demonstrate the conformity of the high-risk AI system with the requirements set out in 
Section 2, in a language which can be easily understood by the authority in one of the 
official languages of the institutions of the Union as indicated by the Member State 
concerned."
3,High-risk ai systems,16,3.16,Cooperation with competent authorities,3,3.16.3,1,"2. Upon a reasoned request by a national competent authority, providers shall also give the 
requesting national competent authority, as applicable, access to the automatically 
generated logs of the high-risk AI system referred to in Article 12(1), to the extent such 
logs are under their control."
3,High-risk ai systems,16,3.16,Cooperation with competent authorities,4,3.16.4,1,"3. Any information obtained by a national competent authority pursuant to this Article 
shall be treated in compliance with the confidentiality obligations set out in Article 78."
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,1,3.17.1,1,"Article 22
Authorised representatives of providers of high-risk AI systems"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,2,3.17.2,1,"1. Prior to making their high-risk AI systems available on the Union market,  providers 
established in third countries shall, by written mandate, appoint an authorised 
representative which is established in the Union."
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,3,3.17.3,1,"2. The provider shall enable its authorised representative to perform the tasks specified in 
the mandate received from the provider."
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,1,"3. The authorised representative shall perform the tasks specified in the mandate received 
from the provider. It shall provide a copy of the mandate to the market surveillance 
authorities upon request, in one of the official languages of the institutions of the 
Union, as indicated by the national competent authority. For the purposes of this 
Regulation, the mandate shall empower the authorised representative to carry out the 
following tasks:"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,2,"(a) verify that the EU declaration of conformity and the technical documentation 
referred to in Article 11 have been drawn up and that an appropriate conformity 
assessment procedure has been carried out by the provider;"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,3,"(b) keep at the disposal of the national competent authorities and national authorities 
or bodies referred to in Article 74(10), for a period of 10 years after the high-risk 
AI system has been placed on the market or put into service, the contact details of 
the provider that appointed the authorised representative, a copy of the EU 
declaration of conformity, the technical documentation and, if applicable, the 
certificate issued by the notified body;"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,4,"(c) provide a national competent authority, upon a reasoned request, with all the 
information and documentation, including that referred to in point (b) of this 
subparagraph, necessary to demonstrate the conformity of a high-risk AI system 
with the requirements set out in Section 2, including access to the logs, as referred to 
in Article 12(1), automatically generated by the high-risk AI system, to the extent 
such logs are under the control of the provider ;"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,5,"(d) cooperate with competent  authorities, upon a reasoned request, in any action the 
latter take in relation to the high-risk AI system, in particular to reduce and mitigate 
the risks posed by the high-risk AI system;"
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,4,3.17.4,6,"(e) where applicable, comply with the registration obligations referred in Article 49(1), 
or, if the registration is carried out by the provider itself, ensure that the 
information referred to in Section A of Annex VIII is correct.
The mandate shall empower the authorised representative to be addressed, in addition to 
or instead of the provider, by the competent authorities, on all issues related to ensuring 
compliance with this Regulation."
3,High-risk ai systems,17,3.17,Authorised representatives of providers of high-risk ai systems,5,3.17.5,1,"4. The authorised representative shall terminate the mandate if it considers or has reason 
to consider the provider to be acting contrary to its obligations pursuant to this 
Regulation. In such a case, it shall also immediately inform the market surveillance 
authority of the Member State in which it is located or established, as well as, where 
applicable, the relevant notified body, about the termination of the mandate and the 
reasons therefor."
3,High-risk ai systems,18,3.18,Obligations of importers,1,3.18.1,1,"Article 23
Obligations of importers"
3,High-risk ai systems,18,3.18,Obligations of importers,2,3.18.2,1,"1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
is in conformity with this Regulation by verifying that:"
3,High-risk ai systems,18,3.18,Obligations of importers,2,3.18.2,2,"(a) the relevant conformity assessment procedure referred to in Article 43 has been 
carried out by the provider of the high-risk AI system;"
3,High-risk ai systems,18,3.18,Obligations of importers,2,3.18.2,3,"(b) the provider has drawn up the technical documentation in accordance with Article 11 
and Annex IV;"
3,High-risk ai systems,18,3.18,Obligations of importers,2,3.18.2,4,"(c) the system bears the required CE marking and is accompanied by the EU declaration 
of conformity and instructions for use;"
3,High-risk ai systems,18,3.18,Obligations of importers,2,3.18.2,5,"(d) the provider has appointed an authorised representative in accordance with 
Article 22(1)."
3,High-risk ai systems,18,3.18,Obligations of importers,3,3.18.3,1,"2. Where an importer has sufficient reason to consider that a high-risk AI system is not in 
conformity with this Regulation, or is falsified, or accompanied by falsified 
documentation, it shall not place the system on the market until it has been brought into 
conformity. Where the high-risk AI system presents a risk within the meaning of Article 
79(1), the importer shall inform the provider of the system, the authorised representatives 
and the market surveillance authorities to that effect."
3,High-risk ai systems,18,3.18,Obligations of importers,4,3.18.4,1,"3. Importers shall indicate their name, registered trade name or registered trade mark, and the 
address at which they can be contacted in relation to the high-risk AI system on its 
packaging or its accompanying documentation, where applicable."
3,High-risk ai systems,18,3.18,Obligations of importers,5,3.18.5,1,"4. Importers shall ensure that, while a high-risk AI system is under their responsibility, 
storage or transport conditions, where applicable, do not jeopardise its compliance with the 
requirements set out in Section 2."
3,High-risk ai systems,18,3.18,Obligations of importers,6,3.18.6,1,"5. Importers shall keep, for a period of 10 years after the high-risk AI system has been 
placed on the market or put into service, a copy of the certificate issued by the notified 
body, where applicable, of the instructions for use, and of the EU declaration of 
conformity."
3,High-risk ai systems,18,3.18,Obligations of importers,7,3.18.7,1,"6. Importers shall provide national competent authorities, upon a reasoned request, with all 
the necessary information and documentation, including that kept in accordance with 
paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements 
set out in Section 2 in a language which can be easily understood by them. For this 
purpose, they shall also ensure that the technical documentation can be made available 
to those authorities."
3,High-risk ai systems,18,3.18,Obligations of importers,8,3.18.8,1,"7. Importers shall cooperate with national competent authorities in any action those 
authorities take in relation to a high-risk AI system the importers placed on the market, 
in particular to reduce and mitigate the risks posed by it."
3,High-risk ai systems,19,3.19,Obligations of distributors,1,3.19.1,1,"Article 24
Obligations of distributors"
3,High-risk ai systems,19,3.19,Obligations of distributors,2,3.19.2,1,"1. Before making a high-risk AI system available on the market, distributors shall verify that 
it bears the required CE marking, that it is accompanied by a copy of EU declaration of 
conformity and instructions for use, and that the provider and the importer of the system, 
as applicable, have complied with their respective obligations as laid down in Article 16, 
points (b) and (c) and Article 23(3)."
3,High-risk ai systems,19,3.19,Obligations of distributors,3,3.19.3,1,"2. Where a distributor considers or has reason to consider, on the basis of the information in 
its possession, a high-risk AI system not to be in conformity with the requirements set out 
in Section 2, it shall not make the high-risk AI system available on the market until the 
system has been brought into conformity with those requirements. Furthermore, where the 
high-risk AI system presents a risk within the meaning of Article 79(1), the distributor 
shall inform the provider or the importer of the system, as applicable, to that effect."
3,High-risk ai systems,19,3.19,Obligations of distributors,4,3.19.4,1,"3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, 
where applicable, storage or transport conditions do not jeopardise the compliance of the 
system with the requirements set out in Section 2."
3,High-risk ai systems,19,3.19,Obligations of distributors,5,3.19.5,1,"4. A distributor that considers or has reason to consider, on the basis of the information in its 
possession, a high-risk AI system which it has made available on the market not to be in 
conformity with the requirements set out in Section 2, shall take the corrective actions 
necessary to bring that system into conformity with those requirements, to withdraw it or 
recall it, or shall ensure that the provider, the importer or any relevant operator, as 
appropriate, takes those corrective actions. Where the high-risk AI system presents a risk 
within the meaning of Article 79(1), the distributor shall immediately inform the provider 
or importer of the system and the national competent authorities of the Member States in 
which it has made the product available to that effect, giving details, in particular, of the 
non-compliance and of any corrective actions taken."
3,High-risk ai systems,19,3.19,Obligations of distributors,6,3.19.6,1,"5. Upon a reasoned request from a national competent authority, distributors of a high-risk 
AI system shall provide that authority with all the information and documentation 
regarding its actions pursuant to paragraphs 1 to 4 necessary to demonstrate the 
conformity of that system with the requirements set out in Section 2."
3,High-risk ai systems,19,3.19,Obligations of distributors,7,3.19.7,1,"6. Distributors shall cooperate with national competent authorities in any action those 
authorities take in relation to a high-risk AI system they made available on the market, 
in particular to reduce or mitigate the risk posed by it."
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,1,3.20.1,1,"Article 25
Responsibilities along the AI value chain"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,2,3.20.2,1,"1. Any distributor, importer, deployer or other third-party shall be considered to be a provider 
of a high-risk AI system for the purposes of this Regulation and shall be subject to the 
obligations of the provider under Article 16, in any of the following circumstances:"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,2,3.20.2,2,"(a) they put their name or trademark on a high-risk AI system already placed on the 
market or put into service, without prejudice to contractual arrangements 
stipulating that the obligations therein are allocated otherwise;"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,2,3.20.2,3,"(b) they make a substantial modification to a high-risk AI system that has already been 
placed on the market or has already been put into service in such a way that it 
remains a high-risk AI system pursuant to Article 6;"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,2,3.20.2,4,"(c) they modify the intended purpose of an AI system, including a general-purpose AI 
system, which has not been classified as high-risk and has already been placed on 
the market or put into service in such a way that the AI system concerned becomes 
a high-risk AI system in accordance with Article 6."
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,3,3.20.3,1,"2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed 
the  AI system on the market or put it into service shall no longer be considered to be a 
provider of that specific AI system for the purposes of this Regulation. That initial 
provider shall closely cooperate with new providers and shall make available the 
necessary information and provide the reasonably expected technical access and other 
assistance that are required for the fulfilment of the obligations set out in this 
Regulation, in particular regarding the compliance with the conformity assessment of 
high-risk AI systems. This paragraph shall not apply in cases where the initial provider 
has clearly specified that its AI system is not to be changed into a high-risk AI system 
and therefore does not fall under the obligation to hand over the documentation."
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,4,3.20.4,1,"3. In the case of high-risk AI systems that are safety components of products covered by the 
Union harmonisation legislation listed in Section A of Annex I, the product manufacturer 
shall be considered to be the provider of the high-risk AI system, and shall be subject to 
the obligations under Article 16 under either of the following circumstances:"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,4,3.20.4,2,"(a) the high-risk AI system is placed on the market together with the product under the 
name or trademark of the product manufacturer;"
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,4,3.20.4,3,"(b) the high-risk AI system is put into service under the name or trademark of the 
product manufacturer after the product has been placed on the market."
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,5,3.20.5,1,"4. The provider of a high-risk AI system and the third party that supplies an AI system, 
tools, services, components, or processes that are used or integrated in a high-risk AI 
system shall, by written agreement, specify the necessary information, capabilities, 
technical access and other assistance based on the generally acknowledged state of the 
art, in order to enable the provider of the high-risk AI system to fully comply with the 
obligations set out in this Regulation. This paragraph shall not apply to third parties 
making accessible to the public tools, services, processes, or components, other than 
general-purpose AI models, under a free and open licence.
The AI Office may develop and recommend voluntary model terms for contracts between 
providers of high-risk AI systems and third parties that supply tools, services, 
components or processes that are used for or integrated into high-risk AI systems. When 
developing those voluntary model terms, the AI Office shall take into account possible 
contractual requirements applicable in specific sectors or business cases. The voluntary 
model terms shall be published and be available free of charge in an easily usable 
electronic format."
3,High-risk ai systems,20,3.20,Responsibilities along the ai value chain,6,3.20.6,1,"5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual 
property rights, confidential business information and trade secrets in accordance with 
Union and national law."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,1,3.21.1,1,"Article 26
Obligations of deployers of high-risk AI systems"
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,2,3.21.2,1,"1. Deployers of high-risk AI systems shall take appropriate technical and organisational 
measures to ensure they use such systems in accordance with the instructions for use 
accompanying the systems, pursuant to paragraphs 3 and 6."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,3,3.21.3,1,"2. Deployers shall assign human oversight to natural persons who have the necessary 
competence, training and authority, as well as the necessary support.
."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,4,3.21.4,1,"3. The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer 
obligations under Union or national law and to the deployer’s freedom to organise its own 
resources and activities for the purpose of implementing the human oversight measures 
indicated by the provider."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,5,3.21.5,1,"4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over 
the input data, that deployer shall ensure that input data is relevant and sufficiently 
representative in view of the intended purpose of the high-risk AI system."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,6,3.21.6,1,"5. Deployers shall monitor the operation of the high-risk AI system on the basis of the 
instructions for use and, where relevant, inform providers in accordance with Article 72. 
Where deployers have reason to consider that the use of the high-risk AI system in 
accordance with the instructions may present a risk within the meaning of Article 79(1), 
they shall, without undue delay, inform the provider or distributor and the relevant market 
surveillance authority, and shall suspend the use of that system. Where deployers have 
identified a serious incident, they shall also immediately inform first the provider, and 
then the importer or distributor and the relevant market surveillance authorities of that 
incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis 
mutandis. This obligation shall not cover sensitive operational data of deployers of AI 
systems which are law enforcement authorities.
For deployers that are financial institutions subject to requirements regarding their 
internal governance, arrangements or processes under Union financial services law, the 
monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by 
complying with the rules on internal governance arrangements, processes and mechanisms 
pursuant to the relevant financial service law."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,7,3.21.7,1,"6. Deployers of high-risk AI systems shall keep the logs automatically generated by that 
high-risk AI system  to the extent such logs are under their control,  for a period  
appropriate to the intended purpose of the high-risk AI system, of at least six months, 
unless provided otherwise in applicable Union or national law, in particular in Union law 
on the protection of personal data.
Deployers that are financial institutions subject to requirements regarding their internal 
governance, arrangements or processes under Union financial services law shall 
maintain the logs as part of the documentation kept pursuant to the relevant Union 
financial service law."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,8,3.21.8,1,"7. Before putting into service or using a high-risk AI system at the workplace, deployers 
who are employers shall inform workers’ representatives and the affected workers that 
they will be subject to the use of the high-risk AI system. This information shall be 
provided, where applicable, in accordance with the rules and procedures laid down in 
Union and national law and practice on information of workers and their 
representatives."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,9,3.21.9,1,"8. Deployers of high-risk AI systems that are public authorities, or Union institutions, 
bodies, offices or agencies shall comply with the registration obligations referred to in 
Article 49. When such deployers find that the high-risk AI system that they envisage 
using has not been registered in the EU database referred to in Article 71, they shall not 
use that system and shall inform the provider or the distributor."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,10,3.21.10,1,"9. Where applicable, deployers of high-risk AI systems shall use the information provided 
under Article 13 of this Regulation to comply with their obligation to carry out a data 
protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 
of Directive (EU) 2016/680."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,11,3.21.11,1,"10. Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for 
the targeted search of a person suspected or convicted of having committed a criminal 
offence, the deployer of a high-risk AI system for post-remote biometric identification 
shall request an authorisation, ex-ante, or without undue delay and no later than 48 
hours, by a judicial authority or an administrative authority whose decision is binding 
and subject to judicial review, for the use of that system, except when it is used for the 
initial identification of a potential suspect based on objective and verifiable facts directly 
linked to the offence. Each use shall be limited to what is strictly necessary for the 
investigation of a specific criminal offence.
If the requested authorisation provided for in the first subparagraph is rejected, the use 
of the post-remote biometric identification system linked to that requested authorisation 
shall be stopped with immediate effect and the personal data linked to the use of the 
high-risk AI system for which the authorisation was requested shall be deleted.
In no case shall such high-risk AI system for post-remote biometric identification be 
used for law enforcement purposes in an untargeted way, without any link to a criminal 
offence, a criminal proceeding, a genuine and present or genuine and foreseeable threat 
of a criminal offence, or the search for a specific missing person. It shall be ensured that 
no decision that produces an adverse legal effect on a person may be taken by the law 
enforcement authorities based solely on the output of such post-remote biometric 
identification systems.
This paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and 
Article 10 of Directive (EU) 2016/680 for the processing of biometric data.
Regardless of the purpose or deployer, each use of such high-risk AI systems shall be 
documented in the relevant police file and shall be made available to the relevant market 
surveillance authority and the national data protection authority upon request, 
excluding the disclosure of sensitive operational data related to law enforcement. This 
subparagraph shall be without prejudice to the powers conferred by 
Directive (EU) 2016/680 on supervisory authorities.
Deployers shall submit annual reports to the relevant market surveillance and national 
data protection authorities on their use of post-remote biometric identification systems, 
excluding the disclosure of sensitive operational data related to law enforcement. The 
reports may be aggregated to cover more than one deployment.
Member States may introduce, in accordance with Union law, more restrictive laws on 
the use of post-remote biometric identification systems."
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,12,3.21.12,1,"11. Without prejudice to Article 50 of this Regulation, deployers of high-risk AI systems 
referred to in Annex III that make decisions or assist in making decisions related to 
natural persons shall inform the natural persons that they are subject to the use of the 
high-risk AI system. For high-risk AI systems used for law enforcement purposes Article"
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,13,3.21.13,1,13 of Directive (EU) 2016/680 shall apply.
3,High-risk ai systems,21,3.21,Obligations of deployers of high-risk ai systems,14,3.21.14,1,"12. Deployers shall cooperate with the relevant national competent authorities in any action 
those authorities take in relation to the high-risk AI system in order to implement this 
Regulation."
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,1,3.22.1,1,"Article 27
Fundamental rights impact assessment for high-risk AI systems"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,1,"1. Prior to deploying a high-risk AI system referred to in Article 6(2) into use, with the 
exception of high-risk AI systems intended to be used in the area listed in point 2 of 
Annex III, deployers that are bodies governed by public law, or are private entities 
providing public services, and deployers high-risk AI systems referred to in points 5 (b) 
and (c) of Annex III, shall perform an assessment of the impact on fundamental rights 
that the use of such system may produce. For that purpose, deployers shall perform an 
assessment consisting of:"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,2,"(a) a description of the deployer’s processes in which the high-risk AI system will be 
used in line with its intended purpose;"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,3,"(b) a description of the period of time within which, and the frequency with which, 
each high-risk AI system is intended to be used;"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,4,"(c) the categories of natural persons and groups likely to be affected by its use in the 
specific context;"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,5,"(d) the specific risks of harm likely to have an impact on the categories of persons or 
groups of persons identified pursuant point (c) of this paragraph, taking into 
account the information given by the provider pursuant to Article 13;"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,6,"(e) a description of the implementation of human oversight measures, according to the 
instructions for use;"
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,2,3.22.2,7,"(f) the measures to be taken where those risks materialise, including the 
arrangements for internal governance and complaint mechanisms."
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,3,3.22.3,1,"2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI 
system. The deployer may, in similar cases, rely on previously conducted fundamental 
rights impact assessments or existing impact assessments carried out by provider. If, 
during the use of the high-risk AI system, the deployer considers that any of the elements 
listed in paragraph 1 has changed or is no longer up to date, the deployer shall take the 
necessary steps to update the information."
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,4,3.22.4,1,"3. Once the assessment referred to in paragraph 1 of this Article has been performed, the 
deployer shall notify the market surveillance authority of its results, including filling-out 
and submitting the template referred to in paragraph 5 of this Article as part of the 
notification. In the case referred to in Article 46(1), deployers may be exempt from that 
obligation to notify."
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,5,3.22.5,1,"4. If any of the obligations laid down in this Article is already complied with as a result of 
the data protection impact assessment conducted pursuant to Article 35 of Regulation 
(EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights impact 
assessment referred to in paragraph 1 of this Article shall complement that data 
protection impact assessment."
3,High-risk ai systems,22,3.22,Fundamental rights impact assessment for high-risk ai systems,6,3.22.6,1,"5. The AI Office shall develop a template for a questionnaire, including through an 
automated tool, to facilitate deployers in complying with their obligations under this 
Article in a simplified manner.
Section 4
Notifying authorities and notified bodies"
3,High-risk ai systems,23,3.23,Notifying authorities,1,3.23.1,1,"Article 28
Notifying authorities"
3,High-risk ai systems,23,3.23,Notifying authorities,2,3.23.2,1,"1. Each Member State shall designate or establish at least one notifying authority responsible 
for setting up and carrying out the necessary procedures for the assessment, designation 
and notification of conformity assessment bodies and for their monitoring. Those 
procedures shall be developed in cooperation between the notifying authorities of all 
Member States."
3,High-risk ai systems,23,3.23,Notifying authorities,3,3.23.3,1,"2. Member States may decide that the assessment and monitoring referred to in 
paragraph 1 shall be carried out by a national accreditation body within the meaning of, 
and in accordance with, Regulation (EC) No 765/2008  ."
3,High-risk ai systems,23,3.23,Notifying authorities,4,3.23.4,1,"3. Notifying authorities shall be established, organised and operated in such a way that no 
conflict of interest arises with conformity assessment bodies, and that the objectivity and 
impartiality of their activities are safeguarded."
3,High-risk ai systems,23,3.23,Notifying authorities,5,3.23.5,1,"4. Notifying authorities shall be organised in such a way that decisions relating to the 
notification of conformity assessment bodies are taken by competent persons different 
from those who carried out the assessment of those bodies."
3,High-risk ai systems,23,3.23,Notifying authorities,6,3.23.6,1,"5. Notifying authorities shall offer or provide neither any activities that conformity 
assessment bodies perform, nor any consultancy services on a commercial or competitive 
basis."
3,High-risk ai systems,23,3.23,Notifying authorities,7,3.23.7,1,"6. Notifying authorities shall safeguard the confidentiality of the information they obtain, in 
accordance with Article 78."
3,High-risk ai systems,23,3.23,Notifying authorities,8,3.23.8,1,"7. Notifying authorities shall have an adequate number of competent personnel at their 
disposal for the proper performance of their tasks. Competent personnel shall have the 
necessary expertise, where applicable, for their function, in fields such as information 
technologies, AI and law, including the supervision of fundamental rights."
3,High-risk ai systems,24,3.24,Application of a conformity assessment body for notification,1,3.24.1,1,"Article 29
Application of a conformity assessment body for notification"
3,High-risk ai systems,24,3.24,Application of a conformity assessment body for notification,2,3.24.2,1,"1. Conformity assessment bodies shall submit an application for notification to the notifying 
authority of the Member State in which they are established."
3,High-risk ai systems,24,3.24,Application of a conformity assessment body for notification,3,3.24.3,1,"2. The application for notification shall be accompanied by a description of the conformity 
assessment activities, the conformity assessment module or modules and the types of AI 
systems for which the conformity assessment body claims to be competent, as well as by 
an accreditation certificate, where one exists, issued by a national accreditation body 
attesting that the conformity assessment body fulfils the requirements laid down in 
Article 31. 
Any valid document related to existing designations of the applicant notified body under 
any other Union harmonisation legislation shall be added."
3,High-risk ai systems,24,3.24,Application of a conformity assessment body for notification,4,3.24.4,1,"3. Where the conformity assessment body concerned cannot provide an accreditation 
certificate, it shall provide the notifying authority with all the documentary evidence 
necessary for the verification, recognition and regular monitoring of its compliance with 
the requirements laid down in Article 31."
3,High-risk ai systems,24,3.24,Application of a conformity assessment body for notification,5,3.24.5,1,"4. For notified bodies which are designated under any other Union harmonisation legislation, 
all documents and certificates linked to those designations may be used to support their 
designation procedure under this Regulation, as appropriate. The notified body shall 
update the documentation referred to in paragraphs 2 and 3 of this Article whenever 
relevant changes occur, in order to enable the authority responsible for notified bodies to 
monitor and verify continuous compliance with all the requirements laid down in Article 
31."
3,High-risk ai systems,25,3.25,Notification procedure,1,3.25.1,1,"Article 30
Notification procedure"
3,High-risk ai systems,25,3.25,Notification procedure,2,3.25.2,1,"1. Notifying authorities may  notify only conformity assessment bodies which have 
satisfied the requirements laid down in Article 31."
3,High-risk ai systems,25,3.25,Notification procedure,3,3.25.3,1,"2. Notifying authorities shall notify the Commission and the other Member States, using the 
electronic notification tool developed and managed by the Commission, of each 
conformity assessment body referred to in paragraph 1."
3,High-risk ai systems,25,3.25,Notification procedure,4,3.25.4,1,"3. The notification referred to in paragraph 2 of this Article shall include full details of the 
conformity assessment activities, the conformity assessment module or modules, the types 
of AI systems concerned, and the relevant attestation of competence. Where a 
notification is not based on an accreditation certificate as referred to in Article 29(2), the 
notifying authority shall provide the Commission and the other Member States with 
documentary evidence which attests to the competence of the conformity assessment 
body and to the arrangements in place to ensure that that body will be monitored 
regularly and will continue to satisfy the requirements laid down in Article 31."
3,High-risk ai systems,25,3.25,Notification procedure,5,3.25.5,1,"4. The conformity assessment body concerned may perform the activities of a notified body 
only where no objections are raised by the Commission or the other Member States within 
two weeks of a notification by a notifying authority where it includes an accreditation 
certificate referred to in Article 29(2), or within two months of a notification by the 
notifying authority where it includes documentary evidence referred to in Article 29(3)."
3,High-risk ai systems,25,3.25,Notification procedure,6,3.25.6,1,"5. Where objections are raised, the Commission shall, without delay, enter into 
consultations with the relevant Member States and the conformity assessment body. 
Having regard thereto, the Commission shall decide whether the authorisation is 
justified. The Commission shall address its decision to the Member State concerned and 
the relevant conformity assessment body."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,1,3.26.1,1,"Article 31
Requirements relating to notified bodies"
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,2,3.26.2,1,"1. A notified body shall be established under the national law of a Member State and shall 
have legal personality."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,3,3.26.3,1,"2. Notified bodies shall satisfy the organisational, quality management, resources and process 
requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity 
requirements."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,4,3.26.4,1,"3. The organisational structure, allocation of responsibilities, reporting lines and operation of 
notified bodies shall ensure confidence in their performance, and in the results of the 
conformity assessment activities that the notified bodies conduct."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,5,3.26.5,1,"4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to 
which they perform conformity assessment activities. Notified bodies shall also be 
independent of any other operator having an economic interest in high-risk AI systems 
assessed, as well as of any competitors of the provider. This shall not preclude the use of 
assessed high-risk AI systems that are necessary for the operations of the conformity 
assessment body, or the use of such high-risk AI systems for personal purposes."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,6,3.26.6,1,"5. Neither a conformity assessment body, its top-level management nor the personnel 
responsible for carrying out its conformity assessment tasks shall be directly involved in 
the design, development, marketing or use of high-risk AI systems, nor shall they 
represent the parties engaged in those activities. They shall not engage in any activity 
that might conflict with their independence of judgement or integrity in relation to 
conformity assessment activities for which they are notified. This shall, in particular, 
apply to consultancy services."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,7,3.26.7,1,"6. Notified bodies shall be organised and operated so as to safeguard the independence, 
objectivity and impartiality of their activities. Notified bodies shall document and 
implement a structure and procedures to safeguard impartiality and to promote and apply 
the principles of impartiality throughout their organisation, personnel and assessment 
activities."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,8,3.26.8,1,"7. Notified bodies shall have documented procedures in place ensuring that their personnel, 
committees, subsidiaries, subcontractors and any associated body or personnel of external 
bodies maintain, in accordance with Article 78, the confidentiality of the information 
which comes into their possession during the performance of conformity assessment 
activities, except when its disclosure is required by law. The staff of notified bodies shall 
be bound to observe professional secrecy with regard to all information obtained in 
carrying out their tasks under this Regulation, except in relation to the notifying authorities 
of the Member State in which their activities are carried out."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,9,3.26.9,1,"8. Notified bodies shall have procedures for the performance of activities which take due 
account of the size of a provider, the sector in which it operates, its structure, and the 
degree of complexity of the AI system concerned."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,10,3.26.10,1,"9. Notified bodies shall take out appropriate liability insurance for their conformity 
assessment activities, unless liability is assumed by the Member State in which they are 
established in accordance with national law or that Member State is itself directly 
responsible for the conformity assessment."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,11,3.26.11,1,"10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with 
the highest degree of professional integrity and the requisite competence in the specific 
field, whether those tasks are carried out by notified bodies themselves or on their behalf 
and under their responsibility."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,12,3.26.12,1,"11. Notified bodies shall have sufficient internal competences to be able effectively to evaluate 
the tasks conducted by external parties on their behalf. The notified body shall have 
permanent availability of sufficient administrative, technical, legal and scientific personnel 
who possess experience and knowledge relating to the relevant types of AI systems, data 
and data computing, and relating to the requirements set out in Section 2."
3,High-risk ai systems,26,3.26,Requirements relating to notified bodies,13,3.26.13,1,"12. Notified bodies shall participate in coordination activities as referred to in Article 38. They 
shall also take part directly, or be represented in, European standardisation organisations, 
or ensure that they are aware and up to date in respect of relevant standards."
3,High-risk ai systems,27,3.27,Presumption of conformity with requirements relating to notified bodies,1,3.27.1,1,"Article 32
Presumption of conformity with requirements relating to notified bodies
Where a conformity assessment body demonstrates its conformity with the criteria laid down in 
the relevant harmonised standards or parts thereof, the references of which have been published 
in the Official Journal of the European Union, it shall be presumed to comply with the 
requirements set out in Article 31 in so far as the applicable harmonised standards cover those 
requirements."
3,High-risk ai systems,28,3.28,Subsidiaries of notified bodies and subcontracting,1,3.28.1,1,"Article 33
Subsidiaries of notified bodies and subcontracting"
3,High-risk ai systems,28,3.28,Subsidiaries of notified bodies and subcontracting,2,3.28.2,1,"1. Where a notified body subcontracts specific tasks connected with the conformity 
assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the 
subsidiary meets the requirements laid down in Article 31, and shall inform the notifying 
authority accordingly."
3,High-risk ai systems,28,3.28,Subsidiaries of notified bodies and subcontracting,3,3.28.3,1,"2. Notified bodies shall take full responsibility for their tasks performed by subcontractors or 
subsidiaries."
3,High-risk ai systems,28,3.28,Subsidiaries of notified bodies and subcontracting,4,3.28.4,1,"3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of 
the provider. Notified bodies shall make a list of their subsidiaries publicly available."
3,High-risk ai systems,28,3.28,Subsidiaries of notified bodies and subcontracting,5,3.28.5,1,"4. The relevant documents concerning the assessment of the qualifications of the 
subcontractor or the subsidiary and the work carried out by them under this Regulation 
shall be kept at the disposal of the notifying authority for a period of five years from the 
termination date of the subcontracting activity."
3,High-risk ai systems,29,3.29,Operational obligations of notified bodies,1,3.29.1,1,"Article 34
Operational obligations of notified bodies"
3,High-risk ai systems,29,3.29,Operational obligations of notified bodies,2,3.29.2,1,"1. Notified bodies shall verify the conformity of high-risk AI systems in accordance with 
the conformity assessment procedures set out in Article 43."
3,High-risk ai systems,29,3.29,Operational obligations of notified bodies,3,3.29.3,1,"2. Notified bodies shall avoid unnecessary burdens for providers when performing their 
activities, and take due account of the size of the provider, the sector in which it 
operates, its structure and the degree of complexity of the high-risk AI system 
concerned, in particular in view of minimising administrative burdens and compliance 
costs for micro- and small enterprises within the meaning of Recommendation 
2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the 
level of protection required for the compliance of the high-risk AI system with the 
requirements of this Regulation. ."
3,High-risk ai systems,29,3.29,Operational obligations of notified bodies,4,3.29.4,1,"3. Notified bodies shall make available and submit upon request all relevant 
documentation, including the providers’ documentation, to the notifying authority 
referred to in Article 28 to allow that authority to conduct its assessment, designation, 
notification and monitoring activities, and to facilitate the assessment outlined in this 
Section."
3,High-risk ai systems,30,3.30,Identification numbers and lists of notified bodies,1,3.30.1,1,"Article 35
Identification numbers and lists of notified bodies"
3,High-risk ai systems,30,3.30,Identification numbers and lists of notified bodies,2,3.30.2,1,"1. The Commission shall assign a single identification number to each notified body, even 
where a body is notified under more than one Union act."
3,High-risk ai systems,30,3.30,Identification numbers and lists of notified bodies,3,3.30.3,1,"2. The Commission shall make publicly available the list of the bodies notified under this 
Regulation, including their identification numbers and the activities for which they have 
been notified. The Commission shall ensure that the list is kept up to date."
3,High-risk ai systems,31,3.31,Changes to notifications,1,3.31.1,1,"Article 36
Changes to notifications"
3,High-risk ai systems,31,3.31,Changes to notifications,2,3.31.2,1,"1. The notifying authority shall notify the Commission and the other Member States of any 
relevant changes to the notification of a notified body via the electronic notification tool 
referred to in Article 30(2)."
3,High-risk ai systems,31,3.31,Changes to notifications,3,3.31.3,1,"2. The procedures laid down in Articles 29 and 30 shall apply to extensions of the scope of 
the notification. 
For changes to the notification other than extensions of its scope, the procedures laid 
down in the following paragraphs shall apply."
3,High-risk ai systems,31,3.31,Changes to notifications,4,3.31.4,1,"3. Where a notified body decides to cease its conformity assessment activities, it shall 
inform the notifying authority and the providers concerned as soon as possible and, in 
the case of a planned cessation, at least one year before ceasing its activities. The 
certificates of the notified body may remain valid for a temporary period of nine months 
after cessation of the notified body’s activities, on condition that another notified body 
has confirmed in writing that it will assume responsibilities for the high risk AI systems 
covered by those certificates. The latter notified body shall complete a full assessment of 
the AI systems affected by the end of that nine-month-period before issuing new 
certificates for those systems. Where the notified body has ceased its activity, the 
notifying authority shall withdraw the designation."
3,High-risk ai systems,31,3.31,Changes to notifications,5,3.31.5,1,"4. Where a notifying authority has sufficient reason to consider that a notified body no 
longer meets the requirements laid down in Article 31, or that it is failing to fulfil its 
obligations, the notifying authority shall without delay investigate the matter with the 
utmost diligence. In that context, it shall inform the notified body concerned about the 
objections raised and give it the possibility to make its views known. If the notifying 
authority comes to the conclusion that the notified body  no longer meets the 
requirements laid down in Article 31 or that it is failing to fulfil its obligations, it shall 
restrict, suspend or withdraw the designation as appropriate, depending on the 
seriousness of the failure to meet those requirements or fulfil those obligations. It shall  
immediately inform the Commission and the other Member States accordingly."
3,High-risk ai systems,31,3.31,Changes to notifications,6,3.31.6,1,"5. Where its designation has been suspended, restricted, or fully or partially withdrawn, the 
notified body shall inform the providers concerned at the latest within 10 days."
3,High-risk ai systems,31,3.31,Changes to notifications,7,3.31.7,1,"6. In the event of the restriction, suspension or withdrawal of a designation, the notifying 
authority shall take appropriate steps to ensure that the files of the notified body 
concerned are kept, and to make them available to notifying authorities in other Member 
States and to market surveillance authorities at their request."
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,1,"7. In the event of the restriction, suspension or withdrawal of a designation, the notifying 
authority shall:"
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,2,(a) assess the impact on the certificates issued by the notified body;
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,3,"(b) submit a report on its findings to the Commission and the other Member States 
within three months of having notified the changes to the designation;"
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,4,"(c) require the notified body to suspend or withdraw, within a reasonable period of 
time determined by the authority, any certificates which were unduly issued, in 
order to ensure the continuing conformity of AI systems on the market;"
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,5,"(d) inform the Commission and the Member States about certificates the suspension or 
withdrawal of which it has required;"
3,High-risk ai systems,31,3.31,Changes to notifications,8,3.31.8,6,"(e) provide the national competent authorities of the Member State in which the 
provider has its registered place of business with all relevant information about the 
certificates of which it has required the suspension or withdrawal; that authority 
shall take the appropriate measures, where necessary, to avoid a potential risk to 
health, safety or fundamental rights."
3,High-risk ai systems,31,3.31,Changes to notifications,9,3.31.9,1,"8. With the exception of certificates unduly issued, and where a designation has been 
suspended or restricted, the certificates shall remain valid in one of the following 
circumstances:"
3,High-risk ai systems,31,3.31,Changes to notifications,9,3.31.9,2,"(a) the notifying authority has confirmed, within one month of the suspension or 
restriction, that there is no risk to health, safety or fundamental rights in relation 
to certificates affected by the suspension or restriction, and the notifying authority 
has outlined a timeline for actions to remedy the suspension or restriction; or"
3,High-risk ai systems,31,3.31,Changes to notifications,9,3.31.9,3,"(b) the notifying authority has confirmed that no certificates relevant to the 
suspension will be issued, amended or re-issued during the course of the 
suspension or restriction, and states whether the notified body has the capability of 
continuing to monitor and remain responsible for existing certificates issued for 
the period of the suspension or restriction; In the event that the notifying authority 
determines that the notified body does not have the capability to support existing 
certificates issued, the provider of the system covered by the certificate shall 
confirm in writing to the national competent authorities of the Member State in 
which it has its registered place of business, within three months of the suspension 
or restriction, that another qualified notified body is temporarily assuming the 
functions of the notified body to monitor and remain responsible for the 
certificates during the period of suspension or restriction."
3,High-risk ai systems,31,3.31,Changes to notifications,10,3.31.10,1,"9. With the exception of certificates unduly issued, and where a designation has been 
withdrawn, the certificates shall remain valid for a period of nine months under the 
following circumstances:"
3,High-risk ai systems,31,3.31,Changes to notifications,10,3.31.10,2,"(a) the national competent authority of the Member State in which the provider of the 
AI system covered by the certificate has its registered place of business has 
confirmed that there is no risk to health, safety or fundamental rights associated 
with the high-risk AI systems concerned; and"
3,High-risk ai systems,31,3.31,Changes to notifications,10,3.31.10,3,"(b) another notified body has confirmed in writing that it will assume immediate 
responsibilities for assessing those AI systems and completes its assessment within"
3,High-risk ai systems,31,3.31,Changes to notifications,11,3.31.11,1,"12 months of the withdrawal of the designation.
In the circumstances referred to in the first subparagraph, the national competent 
authority of the Member State in which the provider of the system covered by the 
certificate has its place of business may extend the provisional validity of the certificates 
for additional periods of three months, which shall not exceed 12 months in total.
The national competent authority or the notified body assuming the functions of the 
notified body affected by the change of designation shall immediately inform the 
Commission, the other Member States and the other notified bodies thereof."
3,High-risk ai systems,32,3.32,Challenge to the competence of notified bodies,1,3.32.1,1,"Article 37
Challenge to the competence of notified bodies"
3,High-risk ai systems,32,3.32,Challenge to the competence of notified bodies,2,3.32.2,1,"1. The Commission shall, where necessary, investigate all cases where there are reasons to 
doubt the competence of a notified body or the continued fulfilment by a notified body of 
the requirements laid down in Article 31 and of its applicable responsibilities."
3,High-risk ai systems,32,3.32,Challenge to the competence of notified bodies,3,3.32.3,1,"2. The notifying authority shall provide the Commission, on request, with all relevant 
information relating to the notification or the maintenance of the competence of the 
notified body concerned."
3,High-risk ai systems,32,3.32,Challenge to the competence of notified bodies,4,3.32.4,1,"3. The Commission shall ensure that all sensitive information obtained in the course of its 
investigations pursuant to this Article is treated confidentially in accordance with 
Article 78."
3,High-risk ai systems,32,3.32,Challenge to the competence of notified bodies,5,3.32.5,1,"4. Where the Commission ascertains that a notified body does not meet or no longer meets 
the requirements for its notification, it shall inform the notifying Member State 
accordingly and request it to take the necessary corrective measures, including the 
suspension or withdrawal of the notification if necessary. Where the Member State fails 
to take the necessary corrective measures, the Commission may, by means of an 
implementing act, suspend, restrict or withdraw the designation. That implementing act 
shall be adopted in accordance with the examination procedure referred to in Article 98(2)."
3,High-risk ai systems,33,3.33,Coordination of notified bodies,1,3.33.1,1,"Article 38
Coordination of notified bodies"
3,High-risk ai systems,33,3.33,Coordination of notified bodies,2,3.33.2,1,"1. The Commission shall ensure that, with regard to high-risk AI systems, appropriate 
coordination and cooperation between notified bodies active in the conformity assessment 
procedures  pursuant to this Regulation are put in place and properly operated in the form 
of a sectoral group of notified bodies."
3,High-risk ai systems,33,3.33,Coordination of notified bodies,3,3.33.3,1,"2. Each notifying authority shall ensure that the bodies notified by it participate in the work 
of a group referred to in paragraph 1, directly or through designated representatives."
3,High-risk ai systems,33,3.33,Coordination of notified bodies,4,3.33.4,1,"3. The Commission shall provide for the exchange of knowledge and best practices between 
the notifying authorities of the Member States."
3,High-risk ai systems,34,3.34,Conformity assessment bodies of third countries,1,3.34.1,1,"Article 39
Conformity assessment bodies of third countries
Conformity assessment bodies established under the law of a third country with which the Union 
has concluded an agreement may be authorised to carry out the activities of notified bodies under 
this Regulation, provided that they meet the requirements in Article 31 or they ensure an 
equivalent level of compliance.
Section 5
Standards, conformity assessment, certificates, registration"
3,High-risk ai systems,35,3.35,Harmonised standards and standardisation deliverables,1,3.35.1,1,"Article 40
Harmonised standards and standardisation deliverables"
3,High-risk ai systems,35,3.35,Harmonised standards and standardisation deliverables,2,3.35.2,1,"1. High-risk AI systems which are in conformity with harmonised standards or parts thereof 
the references of which have been published in the Official Journal of the European Union 
in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity 
with the requirements set out in Section 2 of this Chapter or, as applicable, with the 
obligations set out in Chapter IV of this Regulation, to the extent that those standards 
cover those requirements or obligations."
3,High-risk ai systems,35,3.35,Harmonised standards and standardisation deliverables,3,3.35.3,1,"2. The Commission shall issue standardisation requests covering all requirements set out 
in Section 2 of this Chapter and, as applicable, obligations set out in Chapter IV of this 
Regulation, in accordance with Article 10 of Regulation (EU) No 1025/2012, without 
undue delay. The standardisation request shall also ask for deliverables on reporting 
and documentation processes to improve AI systems’ resource performance, such as 
reducing the high-risk AI system’s consumption of energy and other resources 
consumption during its lifecycle, and on the energy-efficient development of general-
purpose AI models. When preparing a standardisation request, the Commission shall 
consult the Board and relevant stakeholders, including the advisory forum.
When issuing a standardisation request to European standardisation organisations, the 
Commission shall specify that standards have to be clear, consistent, including with the 
standards developed in the various sectors for products covered by the existing Union 
harmonisation legislation listed in Annex I, and aiming to ensure that AI systems or AI 
models placed on the market or put into service in the Union meet the relevant 
requirements laid down in this Regulation.
The Commission shall request the European standardisation organisations to provide 
evidence of their best efforts to fulfil the objectives referred to in the first and the second 
subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 
1025/2012."
3,High-risk ai systems,35,3.35,Harmonised standards and standardisation deliverables,4,3.35.4,1,"3. The participants in the standardisation process shall seek to promote investment and 
innovation in AI, including through increasing legal certainty, as well as the 
competitiveness and growth of the Union market, and shall contribute to strengthening 
global cooperation on standardisation and taking into account existing international 
standards in the field of AI that are consistent with Union values, fundamental rights 
and interests, and shall enhance multi-stakeholder governance ensuring a balanced 
representation of interests and the effective participation of all relevant stakeholders in 
accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012."
3,High-risk ai systems,36,3.36,Common specifications,1,3.36.1,1,"Article 41
Common specifications"
3,High-risk ai systems,36,3.36,Common specifications,2,3.36.2,1,"1. The Commission is empowered to adopt, implementing acts establishing common 
specifications for the requirements set out in Section 2 of this Chapter or, as applicable, 
for the obligations set out in Chapter IV where the following conditions have been 
fulfilled:"
3,High-risk ai systems,36,3.36,Common specifications,2,3.36.2,2,"(a) the Commission has requested, pursuant to Article 10(1) of Regulation (EU) 
No 1025/2012, one or more European standardisation organisations to draft a 
harmonised standard for the requirements set out in Section 2 of this Chapter, 
and:"
3,High-risk ai systems,36,3.36,Common specifications,2,3.36.2,3,"(i) the request has not been accepted by any of the European standardisation 
organisations; or
(ii) the harmonised standards addressing that request are not delivered within 
the deadline set in accordance with Article 10(1) of Regulation (EU) 
No 1025/2012; or
(iii) the relevant harmonised standards insufficiently address fundamental rights 
concerns; or
(iv) the harmonised standards do not comply with the request; and
(i) the request has not been accepted by any of the European standardisation 
organisations; or
(ii) the harmonised standards addressing that request are not delivered within 
the deadline set in accordance with Article 10(1) of Regulation (EU) 
No 1025/2012; or
(iii) the relevant harmonised standards insufficiently address fundamental rights 
concerns; or
(iv) the harmonised standards do not comply with the request; and"
3,High-risk ai systems,36,3.36,Common specifications,2,3.36.2,4,"(b) no reference to harmonised standards covering the requirements referred to in 
Section 2 of this Title has been published in the Official Journal of the European 
Union in accordance with Regulation (EU) No 1025/2012, and no such reference 
is expected to be published within a reasonable period.
The implementing acts referred to in the first subparagraph of this paragraph shall be 
adopted in accordance with the examination procedure referred to in Article 98(2), 
after consulting the advisory forum referred to in Article 67."
3,High-risk ai systems,36,3.36,Common specifications,3,3.36.3,1,"2. Before preparing a draft implementing act, the Commission shall inform the committee 
referred to in Article 22 of Regulation (EU) No 1025/2012 that it considers the 
conditions laid down in paragraph 1 of this Article to be fulfilled."
3,High-risk ai systems,36,3.36,Common specifications,4,3.36.4,1,"3. High-risk AI systems which are in conformity with the common specifications referred to 
in paragraph 1, or parts of those specifications, shall be presumed to be in conformity with 
the requirements set out in Section 2, to the extent those common specifications cover 
those requirements."
3,High-risk ai systems,36,3.36,Common specifications,5,3.36.5,1,"4. Where a harmonised standard is adopted by a European standardisation organisation 
and proposed to the Commission for the publication of its reference in the Official 
Journal of the European Union, the Commission shall assess the harmonised standard 
in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised 
standard is published in the Official Journal of the European Union, the Commission 
shall repeal the implementing acts referred to in paragraph 1, or parts thereof which 
cover the same requirements set out in Section 2 of this Chapter."
3,High-risk ai systems,36,3.36,Common specifications,6,3.36.6,1,"5. Where providers of high-risk AI systems do not comply with the common specifications 
referred to in paragraph 1, they shall duly justify that they have adopted technical solutions 
that meet the requirements referred to in Section 2 to a level at least equivalent thereto."
3,High-risk ai systems,36,3.36,Common specifications,7,3.36.7,1,"6. Where a Member State considers that a common specification does not entirely meet the 
requirements set out in Section 2, it shall inform the Commission thereof with a detailed 
explanation. The Commission shall assess that information and, if appropriate, amend 
the implementing act establishing the common specification concerned."
3,High-risk ai systems,37,3.37,Presumption of conformity with certain requirements,1,3.37.1,1,"Article 42
Presumption of conformity with certain requirements"
3,High-risk ai systems,37,3.37,Presumption of conformity with certain requirements,2,3.37.2,1,"1. High-risk AI systems that have been trained and tested on data reflecting the specific 
geographical, behavioural, contextual or functional setting within which they are intended 
to be used shall be presumed to be in compliance with the relevant requirements laid 
down in Article 10(4)."
3,High-risk ai systems,37,3.37,Presumption of conformity with certain requirements,3,3.37.3,1,"2. High-risk AI systems that have been certified or for which a statement of conformity has 
been issued under a cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the 
references of which have been published in the Official Journal of the European Union 
shall be presumed to be in compliance with the cybersecurity requirements set out in 
Article 15 of this Regulation in so far as the cybersecurity certificate or statement of 
conformity or parts thereof cover those requirements."
3,High-risk ai systems,38,3.38,Conformity assessment,1,3.38.1,1,"Article 43
Conformity assessment"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,1,"1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the 
compliance of a high-risk AI system with the requirements set out in Section 2, the 
provider has applied harmonised standards referred to in Article 40, or, where applicable, 
common specifications referred to in Article 41, the provider shall opt for one of the 
following conformity assessment procedures based on:"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,2,(a) the internal control referred to in Annex VI; or
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,3,"(b) the assessment of the quality management system and the assessment of the technical 
documentation, with the involvement of a notified body, referred to in Annex VII.
In demonstrating the compliance of a high-risk AI system with the requirements set out 
in Section 2, the provider shall follow the conformity assessment procedure set out in 
Annex VII where:"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,4,"(a) harmonised standards referred to in Article 40  do not exist, and common 
specifications referred to in Article 41 are not available;"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,5,"(b) the provider has not applied, or has applied only part of, the harmonised standard;"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,6,"(c) the common specifications referred to in point (a) exist, but the provider has not 
applied them;"
3,High-risk ai systems,38,3.38,Conformity assessment,2,3.38.2,7,"(d) one or more of the harmonised standards referred to in point (a) has been 
published with a restriction, and only on the part of the standard that was 
restricted.
For the purposes of the conformity assessment procedure referred to in Annex VII, the 
provider may choose any of the notified bodies. However, where the high-risk AI system is 
intended to be put into service by law enforcement, immigration or asylum authorities or 
by Union institutions, bodies, offices or agencies, the market surveillance authority 
referred to in Article 74(8) or (9), as applicable, shall act as a notified body."
3,High-risk ai systems,38,3.38,Conformity assessment,3,3.38.3,1,"2. For high-risk AI systems referred to in points 2 to 8 of Annex III,  providers shall follow 
the conformity assessment procedure based on internal control as referred to in Annex VI, 
which does not provide for the involvement of a notified body."
3,High-risk ai systems,38,3.38,Conformity assessment,4,3.38.4,1,"3. For high-risk AI systems covered by the Union harmonisation legislation listed in Section 
A of Annex I,,, the provider shall follow the relevant conformity assessment procedure as 
required under those legal acts. The requirements set out in Section 2 of this Chapter shall 
apply to those high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 
4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply.
For the purposes of that assessment, notified bodies which have been notified under those 
legal acts shall be entitled to control the conformity of the high-risk AI systems with the 
requirements set out in Section 2, provided that the compliance of those notified bodies 
with requirements laid down in Article 31(4), (10) and (11) has been assessed in the 
context of the notification procedure under those legal acts.
Where a legal act listed in section A of Annex I enables the product manufacturer to opt 
out from a third-party conformity assessment, provided that that manufacturer has applied 
all harmonised standards covering all the relevant requirements, that manufacturer may use 
that option only if it has also applied harmonised standards or, where applicable, common 
specifications referred to in Article 41, covering the requirements set out in Section 2 of 
this Chapter."
3,High-risk ai systems,38,3.38,Conformity assessment,5,3.38.5,1,"4. High-risk AI systems that have already been subject to a conformity assessment 
procedure shall undergo a new conformity assessment procedure in the event of a 
substantial modification, regardless of whether the modified system is intended to be 
further distributed or continues to be used by the current deployer.
For high-risk AI systems that continue to learn after being placed on the market or put into 
service, changes to the high-risk AI system and its performance that have been pre-
determined by the provider at the moment of the initial conformity assessment and are part 
of the information contained in the technical documentation referred to in point 2(f) of 
Annex IV, shall not constitute a substantial modification."
3,High-risk ai systems,38,3.38,Conformity assessment,6,3.38.6,1,"5. The Commission shall adopt delegated acts in accordance with Article 97 to update 
Annexes VI and VII in  light of technical progress."
3,High-risk ai systems,38,3.38,Conformity assessment,7,3.38.7,1,"6. The Commission shall adopt delegated acts in accordance with Article 97 amending 
paragraphs 1 and 2 of this Article in order to subject high-risk AI systems referred to in 
points 2 to 8 of Annex III to the conformity assessment procedure referred to in Annex VII 
or parts thereof. The Commission shall adopt such delegated acts taking into account the 
effectiveness of the conformity assessment procedure based on internal control referred to 
in Annex VI in preventing or minimising the risks to health and safety and protection of 
fundamental rights posed by such systems, as well as the availability of adequate capacities 
and resources among notified bodies."
3,High-risk ai systems,39,3.39,Certificates,1,3.39.1,1,"Article 44
Certificates"
3,High-risk ai systems,39,3.39,Certificates,2,3.39.2,1,"1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a 
language which can be easily understood by the relevant authorities in the Member State 
in which the notified body is established."
3,High-risk ai systems,39,3.39,Certificates,3,3.39.3,1,"2. Certificates shall be valid for the period they indicate, which shall not exceed five years for 
AI systems covered by Annex I, and four years for AI systems covered by Annex III. On 
the application of the provider, the validity of a certificate may be extended for further 
periods, each not exceeding five years for AI systems covered by Annex I, and four years 
for AI systems covered by Annex III, based on a re-assessment in accordance with the 
applicable conformity assessment procedures. Any supplement to a certificate shall 
remain valid, provided that the certificate which it supplements is valid."
3,High-risk ai systems,39,3.39,Certificates,4,3.39.4,1,"3. Where a notified body finds that an AI system no longer meets the requirements set out in 
Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
the certificate issued or impose restrictions on it, unless compliance with those 
requirements is ensured by appropriate corrective action taken by the provider of the 
system within an appropriate deadline set by the notified body. The notified body shall 
give reasons for its decision.
An appeal procedure against decisions of the notified bodies, including against 
conformity certificates issued, shall be available."
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,1,3.40.1,1,"Article 45
Information obligations of notified bodies"
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,1,1. Notified bodies shall inform the notifying authority of the following:
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,2,"(a) any Union technical documentation assessment certificates, any supplements to those 
certificates, and any quality management system approvals issued in accordance with 
the requirements of Annex VII;"
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,3,"(b) any refusal, restriction, suspension or withdrawal of a Union technical 
documentation assessment certificate or a quality management system approval 
issued in accordance with the requirements of Annex VII;"
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,4,(c) any circumstances affecting the scope of or conditions for notification;
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,5,"(d) any request for information which they have received from market surveillance 
authorities regarding conformity assessment activities;"
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,2,3.40.2,6,"(e) on request, conformity assessment activities performed within the scope of their 
notification and any other activity performed, including cross-border activities and 
subcontracting."
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,3,3.40.3,1,2. Each notified body shall inform the other notified bodies of:
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,3,3.40.3,2,"(a) quality management system approvals which it has refused, suspended or withdrawn, 
and, upon request, of quality system approvals which it has issued;"
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,3,3.40.3,3,"(b) Union technical documentation assessment certificates or any supplements thereto 
which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
request, of the certificates and/or supplements thereto which it has issued."
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,4,3.40.4,1,"3. Each notified body shall provide the other notified bodies carrying out similar conformity 
assessment activities covering the same types of AI systems with relevant information on 
issues relating to negative and, on request, positive conformity assessment results."
3,High-risk ai systems,40,3.40,Information obligations of notified bodies,5,3.40.5,1,"4. The obligations referred to in paragraphs 1, 2 and 3 of this Article shall be complied 
with in accordance with Article 78."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,1,3.41.1,1,"Article 46
Derogation from conformity assessment procedure"
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,2,3.41.2,1,"1. By way of derogation from Article 43 and upon a duly justified request, any market 
surveillance authority may authorise the placing on the market or the putting into service of 
specific high-risk AI systems within the territory of the Member State concerned, for 
exceptional reasons of public security or the protection of life and health of persons, 
environmental protection or the protection of key industrial and infrastructural assets. That 
authorisation shall be for a limited period  while the necessary conformity assessment 
procedures are being carried out, taking into account the exceptional reasons justifying 
the derogation. The completion of those procedures shall be undertaken without undue 
delay."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,3,3.41.3,1,"2. In a duly justified situation of urgency for exceptional reasons of public security or in 
the case of specific, substantial and imminent threat to the life or physical safety of 
natural persons, law-enforcement authorities or civil protection authorities may put a 
specific high-risk AI system into service without the authorisation referred to in 
paragraph 1, provided that such authorisation is requested during or after the use 
without undue delay. If the authorisation referred to in paragraph 1 is refused, the use 
of the high-risk AI system shall be stopped with immediate effect and all the results and 
outputs of such use shall be immediately discarded."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,4,3.41.4,1,"3. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance 
authority concludes that the high-risk AI system complies with the requirements of Section"
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,5,3.41.5,1,"2. The market surveillance authority shall inform the Commission and the other Member 
States of any authorisation issued pursuant to paragraph 1. This obligation shall not cover 
sensitive operational data in relation to the activities of law-enforcement authorities."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,6,3.41.6,1,"4. Where, within 15 calendar days of receipt of the information referred to in paragraph 3, no 
objection has been raised by either a Member State or the Commission in respect of an 
authorisation issued by a market surveillance authority of a Member State in accordance 
with paragraph 1, that authorisation shall be deemed justified."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,7,3.41.7,1,"5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, 
objections are raised by a Member State against an authorisation issued by a market 
surveillance authority of another Member State, or where the Commission considers the 
authorisation to be contrary to Union law, or the conclusion of the Member States 
regarding the compliance of the system as referred to in paragraph 3 to be unfounded, the 
Commission shall, without delay, enter into consultations with the relevant Member State. 
The operators concerned shall be consulted and have the possibility to present their views. 
Having regard thereto, the Commission shall decide whether the authorisation is justified. 
The Commission shall address its decision to the Member State concerned and to the 
relevant operators."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,8,3.41.8,1,"6. Where the Commission considers the authorisation unjustified, it shall be withdrawn by the 
market surveillance authority of the Member State concerned."
3,High-risk ai systems,41,3.41,Derogation from conformity assessment procedure,9,3.41.9,1,"7. For high-risk AI systems related to products covered by Union harmonisation 
legislation listed in Section A of Annex I, only the derogations from the conformity 
assessment established in that Union harmonisation legislation shall apply."
3,High-risk ai systems,42,3.42,Eu declaration of conformity,1,3.42.1,1,"Article 47
EU declaration of conformity"
3,High-risk ai systems,42,3.42,Eu declaration of conformity,2,3.42.2,1,"1. The provider shall draw up a written machine readable, physical or electronically signed 
EU declaration of conformity for each high-risk AI system, and keep it at the disposal of 
the national competent authorities for 10 years after the high-risk AI system has been 
placed on the market or put into service. The EU declaration of conformity shall identify 
the high-risk AI system for which it has been drawn up. A copy of the EU declaration of 
conformity shall be submitted to the relevant national competent authorities upon request."
3,High-risk ai systems,42,3.42,Eu declaration of conformity,3,3.42.3,1,"2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
the requirements set out in Section 2. The EU declaration of conformity shall contain the 
information set out in Annex V, and shall be translated into a language that can be easily 
understood by the national competent authorities of the Member States in which the 
high-risk AI system is placed on the market or made available."
3,High-risk ai systems,42,3.42,Eu declaration of conformity,4,3.42.4,1,"3. Where high-risk AI systems are subject to other Union harmonisation legislation which 
also requires an EU declaration of conformity, a single EU declaration of conformity shall 
be drawn up in respect of all Union law applicable to the high-risk AI system. The 
declaration shall contain all the information required to identify the Union harmonisation 
legislation to which the declaration relates."
3,High-risk ai systems,42,3.42,Eu declaration of conformity,5,3.42.5,1,"4. By drawing up the EU declaration of conformity, the provider shall assume responsibility 
for compliance with the requirements set out in Section 2. The provider shall keep the EU 
declaration of conformity up-to-date as appropriate."
3,High-risk ai systems,42,3.42,Eu declaration of conformity,6,3.42.6,1,"5. The Commission shall adopt delegated acts in accordance with Article 97 for the purpose 
of updating the content of the EU declaration of conformity set out in Annex V, in order to 
introduce elements that become necessary in light of technical progress."
3,High-risk ai systems,43,3.43,Ce marking,1,3.43.1,1,"Article 48
CE marking"
3,High-risk ai systems,43,3.43,Ce marking,2,3.43.2,1,"1. The CE marking shall be subject to the general principles set out in Article 30 of 
Regulation (EC) No 765/2008."
3,High-risk ai systems,43,3.43,Ce marking,3,3.43.3,1,"2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it 
can easily be accessed via the interface from which that system is accessed or via an 
easily accessible machine-readable code or other electronic means."
3,High-risk ai systems,43,3.43,Ce marking,4,3.43.4,1,"3. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. 
Where that is not possible or not warranted on account of the nature of the high-risk AI 
system, it shall be affixed to the packaging or to the accompanying documentation, as 
appropriate."
3,High-risk ai systems,43,3.43,Ce marking,5,3.43.5,1,"4. Where applicable, the CE marking shall be followed by the identification number of the 
notified body responsible for the conformity assessment procedures set out in Article 43. 
The identification number of the notified body shall be affixed by the body itself or, under 
its instructions, by the provider or by the provider’s authorised representative. The 
identification number shall also be indicated in any promotional material which mentions 
that the high-risk AI system fulfils the requirements for CE marking."
3,High-risk ai systems,43,3.43,Ce marking,6,3.43.6,1,"5. Where high-risk AI systems are subject to other Union law which also provides for the 
affixing of the CE marking, the CE marking shall indicate that the high-risk AI system 
also fulfil the requirements of that other law."
3,High-risk ai systems,44,3.44,Registration,1,3.44.1,1,"Article 49
Registration"
3,High-risk ai systems,44,3.44,Registration,2,3.44.2,1,"1. Before placing on the market or putting into service a high-risk AI system listed in 
Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, 
the provider or, where applicable, the authorised representative shall register themselves 
and their system in the EU database referred to in Article 71."
3,High-risk ai systems,44,3.44,Registration,3,3.44.3,1,"2. Before placing on the market or putting into service an AI system for which the provider 
has concluded that it is not high-risk according to Article 6(3), that provider or, where 
applicable, the authorised representative shall register themselves and that system in the 
EU database referred to in Article 71."
3,High-risk ai systems,44,3.44,Registration,4,3.44.4,1,"3. Before putting into service or using a high-risk AI system listed in Annex III, with the 
exception of high-risk AI systems listed in point 2 of Annex III, deployers who are public 
authorities, agencies or bodies or persons acting on their behalf shall register 
themselves, select the system and register its use in the EU database referred to in Article"
3,High-risk ai systems,44,3.44,Registration,5,3.44.5,1,71.
3,High-risk ai systems,44,3.44,Registration,6,3.44.6,1,"4. For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, in the areas of law 
enforcement, migration, asylum and border control management, the registration 
referred to in paragraphs 1, 2 and 3 of this Article shall be in a secure non-public 
section of the EU database referred to in Article 71 and shall include only the following 
information, as applicable, referred to in:"
3,High-risk ai systems,44,3.44,Registration,6,3.44.6,2,"(a) section A, points 1 to 10, of Annex VIII, with the exception of points 5a, 7 and 8;"
3,High-risk ai systems,44,3.44,Registration,6,3.44.6,3,"(b) section C, points 1 to 3, of Annex VIII,;"
3,High-risk ai systems,44,3.44,Registration,6,3.44.6,4,"(c) Section B, points 1 to 5, and points 8 and 9 of Annex VIII;"
3,High-risk ai systems,44,3.44,Registration,6,3.44.6,5,"(d) points 1 to 3, and point 5, of Annex IX.
Only the Commission and national authorities referred to in Article 74(8) shall have 
access to the restricted sections of the EU database listed in the first subparagraph of 
this paragraph."
3,High-risk ai systems,44,3.44,Registration,7,3.44.7,1,"5. High-risk AI systems referred to in point 2 of Annex III shall be registered at national 
level."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,1,4.1.1,1,"Article 50
Transparency obligations for providers and users of certain AI systems"
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,2,4.1.2,1,"1. Providers shall ensure that AI systems intended to interact directly with natural persons are 
designed and developed in such a way that the natural persons concerned are informed that 
they are interacting with an AI system, unless this is obvious from the point of view of a 
natural person who is reasonably well-informed, observant and circumspect, taking into 
account the circumstances and the context of use. This obligation shall not apply to AI 
systems authorised by law to detect, prevent, investigate or prosecute criminal offences, 
subject to appropriate safeguards for the rights and freedoms of third parties, unless 
those systems are available for the public to report a criminal offence."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,3,4.1.3,1,"2. Providers of AI systems, including general-purpose AI systems, generating synthetic 
audio, image, video or text content, shall ensure that the outputs of the AI system are 
marked in a machine-readable format and detectable as artificially generated or 
manipulated. Providers shall ensure their technical solutions are effective, interoperable, 
robust and reliable as far as this is technically feasible, taking into account the 
specificities and limitations of various types of content, the costs of implementation and 
the generally acknowledged state-of-the-art, as may be reflected in relevant technical 
standards. This obligation shall not apply to the extent the AI systems perform an 
assistive function for standard editing or do not substantially alter the input data 
provided by the deployer or the semantics thereof, or where authorised by law to detect, 
prevent, investigate or prosecute criminal offences."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,4,4.1.4,1,"3. Deployers of an emotion recognition system or a biometric categorisation system shall 
inform the natural persons exposed thereto of the operation of the system, and shall 
process the personal data in accordance with Regulations (EU) 2016/679 and (EU) 
2018/1725 and Directive (EU) 2016/680, as applicable. This obligation shall not apply to 
AI systems used for biometric categorisation and emotion recognition, which are 
permitted by law to detect, prevent or investigate criminal offences, subject to appropriate 
safeguards for the rights and freedoms of third parties, and in compliance with Union 
law."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,5,4.1.5,1,"4. Deployers of an AI system that generates or manipulates image, audio or video content 
constituting a deep fake, shall disclose that the content has been artificially generated or 
manipulated. This obligation shall not apply where the use is authorised by law to detect, 
prevent, investigate or prosecute criminal offence. Where the content forms part of an 
evidently artistic, creative, satirical, fictional analogous work or programme, the 
transparency obligations set out in this paragraph are limited to disclosure of the 
existence of such generated or manipulated content in an appropriate manner that does 
not hamper the display or enjoyment of the work.
Deployers of an AI system that generates or manipulates text which is published with the 
purpose of informing the public on matters of public interest shall disclose that the text 
has been artificially generated or manipulated. This obligation shall not apply where the 
use is authorised by law to detect, prevent, investigate or prosecute criminal offences or 
where the AI-generated content has undergone a process of human review or editorial 
control and where a natural or legal person holds editorial responsibility for the 
publication of the content."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,6,4.1.6,1,"5. The information referred to in paragraphs 1 to 4 shall be provided to the natural persons 
concerned in a clear and distinguishable manner at the latest at the time of the first 
interaction or exposure. The information shall conform to the applicable accessibility 
requirements."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,7,4.1.7,1,"6. Paragraphs 1 to 4 shall not affect the requirements and obligations set out in Chapter III, 
and shall be without prejudice to other transparency obligations laid down in Union or 
national law for deployers of AI systems."
4,"Transparency obligations for providers and deployers of certain ai systems",1,4.1,Transparency obligations for providers and users of certain ai systems,8,4.1.8,1,"7. The AI Office shall encourage and facilitate the drawing up of codes of practice at 
Union level to facilitate the effective implementation of the obligations regarding the 
detection and labelling of artificially generated or manipulated content. The 
Commission is empowered to adopt implementing acts to approve those codes of practice 
in accordance with the procedure laid down in Article 56 (6), (7) and (8). If it deems the 
code is not adequate, the Commission is empowered to adopt an implementing act 
specifying common rules for the implementation of those obligations in accordance with 
the examination procedure laid down in Article 98(2)."
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,1,5.1.1,1,"Article 51
Classification of general-purpose AI models as general-purpose AI models with systemic risk"
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,2,5.1.2,1,"1. A general-purpose AI model shall be classified as a general-purpose AI model with 
systemic risk if it meets any of the following requirements:"
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,2,5.1.2,2,"(a) it has high impact capabilities evaluated on the basis of appropriate technical tools 
and methodologies, including indicators and benchmarks;"
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,2,5.1.2,3,"(b) based on a decision of the Commission, ex officio or following a qualified alert 
from the scientific panel, it has capabilities or an impact equivalent to those set out 
in point (a) having regard to the criteria set out in Annex XIII."
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,3,5.1.3,1,"2. A general-purpose AI model shall be presumed to have high impact capabilities 
pursuant to paragraph 1, point (a), when the cumulative amount of computation used 
for its training measured in FLOPs is greater than 10^25."
5,General-purpose ai models,1,5.1,Classification of general-purpose ai models as general-purpose ai models with systemic risk,4,5.1.4,1,"3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
benchmarks and indicators in light of evolving technological developments, such as 
algorithmic improvements or increased hardware efficiency, when necessary, for these 
thresholds to reflect the state of the art."
5,General-purpose ai models,2,5.2,Procedure,1,5.2.1,1,"Article 52
Procedure"
5,General-purpose ai models,2,5.2,Procedure,2,5.2.2,1,"1. Where a general-purpose AI model meets the requirement referred to in Article 51(1), 
point (a), the relevant provider shall notify the Commission without delay and in any 
event within two weeks after that requirement is met or it becomes known that it will be 
met. That notification shall include the information necessary to demonstrate that the 
relevant requirement has been met. If the Commission becomes aware of a general-
purpose AI model presenting systemic risks of which it has not been notified, it may 
decide to designate it as a model with systemic risk."
5,General-purpose ai models,2,5.2,Procedure,3,5.2.3,1,"2. The provider of a general purpose AI model that meets the requirement referred to in 
Article 51(1), point (a), may present, with its notification, sufficiently substantiated 
arguments to demonstrate that, exceptionally, although it meets that requirement, the 
general purpose AI model does not present, due to its specific characteristics, systemic 
risks and therefore should not be classified as a general-purpose AI model with systemic 
risk."
5,General-purpose ai models,2,5.2,Procedure,4,5.2.4,1,"3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 
are not sufficiently substantiated and the relevant provider was not able to demonstrate 
that the general-purpose AI model does not present, due to its specific characteristics, 
systemic risks, it shall reject those arguments, and the general-purpose AI model shall be 
considered to be a general-purpose AI model with systemic risk."
5,General-purpose ai models,2,5.2,Procedure,5,5.2.5,1,"4. The Commission may designate a general-purpose AI model as presenting systemic 
risks, ex officio or following a qualified alert from the scientific panel pursuant to 
Article 90(1), point (a), on the basis of criteria set out in Annex XIII. 
The Commission shall adopt delegated acts in accordance with Article 97 to specify and 
update the criteria set out in Annex XIII."
5,General-purpose ai models,2,5.2,Procedure,6,5.2.6,1,"5. Upon a reasoned request of a provider whose model has been designated as a general-
purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
the request into account and may decide to reassess whether the general-purpose AI 
model can still be considered to present systemic risks on the basis of the criteria set out 
in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
arisen since the designation decision. Providers may request reassessment at the earliest 
six months after the designation decision. Where the Commission, following its 
reassessment, decides to maintain the designation as a general-purpose AI model with 
systemic risk, providers may request reassessment at the earliest six months after that 
decision."
5,General-purpose ai models,2,5.2,Procedure,7,5.2.7,1,"6. The Commission shall ensure that a list of general-purpose AI models with systemic risk 
is published and shall keep that list up to date, without prejudice to the need to observe 
and protect intellectual property rights and confidential business information or trade 
secrets in accordance with Union and national law.
Section 2
Obligations for providers of general-purpose AI models"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,1,5.3.1,1,"Article 53
Obligations for providers of general-purpose AI models"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,1,1. Providers of general-purpose AI models shall:
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,2,"(a) draw up and keep up-to-date the technical documentation of the model, including 
its training and testing process and the results of its evaluation, which shall 
contain, at a minimum, the elements set out in Annex XI for the purpose of 
providing it, upon request, to the AI Office and the national competent authorities;"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,3,"(b) draw up, keep up-to-date and make available information and documentation to 
providers of AI systems who intend to integrate the general-purpose AI model into 
their AI systems. Without prejudice to the need to respect and protect intellectual 
property rights and confidential business information or trade secrets in 
accordance with Union and national law, the information and documentation 
shall:"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,4,"(i) enable providers of AI systems to have a good understanding of the 
capabilities and limitations of the general-purpose AI model and to comply 
with their obligations pursuant to this Regulation; and
(ii) contain, at a minimum, the elements set out in Annex XII;
(i) enable providers of AI systems to have a good understanding of the 
capabilities and limitations of the general-purpose AI model and to comply 
with their obligations pursuant to this Regulation; and
(ii) contain, at a minimum, the elements set out in Annex XII;"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,5,"(c) put in place a policy to comply with Union copyright law, and in particular to 
identify and comply with, including through state of the art technologies, a 
reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,2,5.3.2,6,"(d) draw up and make publicly available a sufficiently detailed summary about the 
content used for training of the general-purpose AI model, according to a template 
provided by the AI Office."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,3,5.3.3,1,"2. The obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of 
AI models that are released under a free and open licence that allows for the access, 
usage, modification, and distribution of the model, and whose parameters, including the 
weights, the information on the model architecture, and the information on model 
usage, are made publicly available. This exception shall not apply to general-purpose AI 
models with systemic risks."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,4,5.3.4,1,"3. Providers of general-purpose AI models shall cooperate as necessary with the 
Commission and the national competent authorities in the exercise of their competences 
and powers pursuant to this Regulation."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,5,5.3.5,1,"4. Providers of general-purpose AI models may rely on codes of practice within the 
meaning of Article 56 to demonstrate compliance with the obligations set out in 
paragraph 1 of this Article, until a harmonised standard is published. Providers who are 
in compliance with a European harmonised standard shall be presumed to be in 
compliance with the obligations set out in paragraph 1 of this Article. Providers of 
general-purpose AI models who do not adhere to an approved code of practice shall 
demonstrate alternative adequate means of compliance for approval by the Commission."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,6,5.3.6,1,"5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and"
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,6,5.3.6,2,"(e) thereof, the Commission shall adopt delegated acts in accordance with Article 97 to 
detail measurement and calculation methodologies with a view to allowing for 
comparable and verifiable documentation."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,7,5.3.7,1,"6. The Commission shall adopt delegated acts in accordance with Article 97(2) to amend 
Annexes XI and XII in the light of evolving technological developments."
5,General-purpose ai models,3,5.3,Obligations for providers of general-purpose ai models,8,5.3.8,1,"7. Any information or documentation obtained pursuant to this Article, including trade 
secrets, shall be treated in compliance with the confidentiality obligations set out in 
Article 78."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,1,5.4.1,1,"Article 54
Authorised representatives of providers of general-purpose AI models"
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,2,5.4.2,1,"1. Prior to placing a general-purpose AI model on the Union market, providers established 
in third countries shall, by written mandate, appoint an authorised representative which 
is established in the Union."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,3,5.4.3,1,"2. The provider shall enable its authorised representative to perform the tasks specified in 
the mandate received from the provider."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,4,5.4.4,1,"2. The authorised representative shall perform the tasks specified in the mandate received 
from the provider. It shall provide a copy of the mandate to the AI Office upon request, 
in one of the official languages of the institutions of the Union. For the purposes of this 
Regulation, the mandate shall empower the authorised representative to carry out the 
following tasks:"
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,4,5.4.4,2,"(a) verify that the technical documentation specified in Annex XI has been drawn up 
and all obligations referred to in Articles 53 and, where applicable, Article 55 have 
been fulfilled by the provider;"
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,4,5.4.4,3,"(b) keep a copy of the technical documentation specified in Annex XI at the disposal 
of the AI Office and national competent authorities, for a period of 10 years after 
the general-purpose AI model has been placed on the market, and keep current the 
contact details of the provider that appointed the authorised representative;"
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,4,5.4.4,4,"(c) provide the AI Office, upon a reasoned request, with all the information and 
documentation, including that referred to in point (b), necessary to demonstrate its 
compliance with the obligations in this Chapter;"
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,4,5.4.4,5,"(d) cooperate with the AI Office and national competent authorities, upon a reasoned 
request, in any action the latter take in relation to a general-purpose AI model with 
systemic risks, including when the model is integrated into AI systems placed on 
the market or put into service in the Union."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,5,5.4.5,1,"3. The mandate shall empower the authorised representative to be addressed, in addition to 
or instead of the provider, by the AI Office or the national competent authorities, on all 
issues related to ensuring compliance with this Regulation."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,6,5.4.6,1,"4. The authorised representative shall terminate the mandate if it considers or has reason 
to consider the provider to be acting contrary to its obligations pursuant to this 
Regulation. In such a case, it shall also immediately inform the AI Office about the 
termination of the mandate and the reasons therefor."
5,General-purpose ai models,4,5.4,Authorised representatives of providers of general-purpose ai models,7,5.4.7,1,"5. The obligation set out in this Article shall not apply to providers of general-purpose AI 
models that are released under a free and open source licence that allows for the access, 
usage, modification, and distribution of the model, and whose parameters, including the 
weights, the information on the model architecture, and the information on model 
usage, are made publicly available, unless the general-purpose AI models present 
systemic risks.
Section 3
Obligations for providers of general-purpose AI models
 with systemic risk"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,1,5.5.1,1,"Article 55
Obligations for providers of general-purpose AI models with systemic risk"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,2,5.5.2,1,"1. In addition to the obligations listed in Article 53, providers of general-purpose AI models 
with systemic risk shall:"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,2,5.5.2,2,"(a) perform model evaluation in accordance with standardised protocols and tools 
reflecting the state-of-the-art, including conducting and documenting adversarial 
testing of the model with a view to identifying and mitigating systemic risk;"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,2,5.5.2,3,"(b) assess and mitigate possible systemic risks at Union level, including their sources, 
that may stem from the development, the placing on the market, or the use of 
general-purpose AI models with systemic risk;"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,2,5.5.2,4,"(c) keep track of, document and report without undue delay to the AI Office and, as 
appropriate, to national competent authorities, relevant information about serious 
incidents and possible corrective measures to address them;"
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,2,5.5.2,5,"(d) ensure an adequate level of cybersecurity protection for the general-purpose AI 
model with systemic risk and the physical infrastructure of the model."
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,3,5.5.3,1,"2. Providers of general-purpose AI models with systemic risk may rely on codes of practice 
within the meaning of Article 56 to demonstrate compliance with the obligations set out 
in paragraph 1 of this Article, until a harmonised standard is published. Providers who 
are in compliance with a European harmonised standard shall be presumed to be in 
compliance with the obligations set out in paragraph 1 of this Article. Providers of 
general-purpose AI models with systemic risks who do not adhere to an approved code of 
practice shall demonstrate alternative adequate means of compliance for approval by the 
Commission."
5,General-purpose ai models,5,5.5,Obligations for providers of general-purpose ai models with systemic risk,4,5.5.4,1,"3. Any information or documentation obtained pursuant to this Article, including trade 
secrets, shall be treated in compliance with the confidentiality obligations set out in 
Article 78."
5,General-purpose ai models,6,5.6,Codes of practice,1,5.6.1,1,"Article 56
Codes of practice"
5,General-purpose ai models,6,5.6,Codes of practice,2,5.6.2,1,"1. The AI Office shall encourage and facilitate the drawing up of codes of practice at 
Union level in order to contribute to the proper application of this Regulation, taking 
into account international approaches."
5,General-purpose ai models,6,5.6,Codes of practice,3,5.6.3,1,"2. The AI Office and the Board shall aim to ensure that the codes of practice cover at least 
the obligations provided for in Articles 53 and 55, including the following issues:"
5,General-purpose ai models,6,5.6,Codes of practice,3,5.6.3,2,"(a) means to ensure that the information referred to in Article 53(1), points (a) and (b), 
is kept up to date in the light of market and technological developments;"
5,General-purpose ai models,6,5.6,Codes of practice,3,5.6.3,3,(b) the adequate level of detail for the summary about the content used for training;
5,General-purpose ai models,6,5.6,Codes of practice,3,5.6.3,4,"(c) the identification of the type and nature of the systemic risks at Union level, 
including their sources, where appropriate;"
5,General-purpose ai models,6,5.6,Codes of practice,3,5.6.3,5,"(d) the measures, procedures and modalities for the assessment and management of 
the systemic risks at Union level, including the documentation thereof, which shall 
be proportionate to the risks, take into consideration their severity and probability 
and take into account the specific challenges of tackling those risks in the light of 
the possible ways in which such risks may emerge and materialise along the AI 
value chain."
5,General-purpose ai models,6,5.6,Codes of practice,4,5.6.4,1,"3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
national competent authorities, to participate in the drawing-up of codes of practice. 
Civil society organisations, industry, academia and other relevant stakeholders, such as 
downstream providers and independent experts, may support the process."
5,General-purpose ai models,6,5.6,Codes of practice,5,5.6.5,1,"4. The AI Office and the Board shall aim to ensure that the codes of practice clearly set out 
their specific objectives and contain commitments or measures, including key 
performance indicators as appropriate, to ensure the achievement of those objectives, 
and that they take due account of the needs and interests of all interested parties, 
including affected persons, at Union level."
5,General-purpose ai models,6,5.6,Codes of practice,6,5.6.6,1,"5. The AI Office shall aim to ensure that participants to the codes of practice report 
regularly to the AI Office on the implementation of the commitments and the measures 
taken and their outcomes, including as measured against the key performance indicators 
as appropriate. Key performance indicators and reporting commitments shall reflect 
differences in size and capacity between various participants."
5,General-purpose ai models,6,5.6,Codes of practice,7,5.6.7,1,"6. The AI Office and the Board shall regularly monitor and evaluate the achievement of 
the objectives of the codes of practice by the participants and their contribution to the 
proper application of this Regulation. The AI Office and the Board shall assess whether 
the codes of practice cover the obligations provided for in Articles 53 and 55, as well as 
the issues listed in paragraph 2 of this Article, and shall regularly monitor and evaluate 
the achievement of their objectives. They shall publish their assessment of the adequacy 
of the codes of practice. 
The Commission may, by way of an implementing act, approve a code of practice and 
give it a general validity within the Union. That implementing act shall be adopted in 
accordance with the examination procedure referred to in Article 98(2)."
5,General-purpose ai models,6,5.6,Codes of practice,8,5.6.8,1,"7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
codes of practice. For providers of general-purpose AI models not presenting systemic 
risks this adherence may be limited to the obligations provided for in Article 53, unless 
they declare explicitly their interest to join the full code."
5,General-purpose ai models,6,5.6,Codes of practice,9,5.6.9,1,"8. The AI Office shall, as appropriate, also encourage and facilitate the review and 
adaptation of the codes of practice, in particular in the light of emerging standards. The 
AI Office shall assist in the assessment of available standards."
5,General-purpose ai models,6,5.6,Codes of practice,10,5.6.10,1,"9. Codes of practice shall be ready at the latest by … [nine months from the date of entry into 
force of this Regulation]. The AI Office shall take the necessary steps, including inviting 
providers pursuant to paragraph 7. 
If, by ... [12 months from the date of entry into force], a code of practice cannot be 
finalised, or if the AI Office deems it is not adequate following its assessment under 
paragraph 6 of this Article, the Commission may provide, by means of implementing 
acts, common rules for the implementation of the obligations provided for in Articles 53 
and 55, including the issues set out in paragraph 2 of this Article. Those implementing 
acts shall be adopted in accordance with the examination procedure referred to in 
Article 98(2)."
