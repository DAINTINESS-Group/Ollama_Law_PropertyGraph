# Checklist Report

*Generated on: 2025-06-22 12:58*

## Table of Contents

- [limited time](#node-limited-time)
- [examination, test and validation procedures](#node-examination-test-and-validation-procedures)
- [union ethical guidelines for trustworthy ai](#node-union-ethical-guidelines-for-trustworthy-ai)
- [development or design](#node-development-or-design)
- [systemic risk](#node-systemic-risk)
- [legal certainty](#node-legal-certainty)
- [microenterprises](#node-microenterprises)
- [union or member states](#node-union-or-member-states)
- [sufficiently transparent operation](#node-sufficiently-transparent-operation)
- [supplying incorrect, incomplete or misleading information](#node-supplying-incorrect-incomplete-or-misleading-information)
- [degree of rigour](#node-degree-of-rigour)
- [impact and effectiveness evaluation](#node-impact-and-effectiveness-evaluation)
- [appropriate corrective action](#node-appropriate-corrective-action)
- [suspension](#node-suspension)
- [provider of an ai system](#node-provider-of-an-ai-system)
- [systemic risks at union level](#node-systemic-risks-at-union-level)
- [safety component](#node-safety-component)
- [same operator for the same infringement](#node-same-operator-for-the-same-infringement)
- [documentation](#node-documentation)
- [ai](#node-ai)
- [product manufacturer](#node-product-manufacturer)
- [proportional](#node-proportional)
- [harmonised standard](#node-harmonised-standard)
- [training, validation and testing data sets](#node-training-validation-and-testing-data-sets)
- [product presenting a risk](#node-product-presenting-a-risk)
- [section 4](#node-section-4)
- [importers](#node-importers)
- [public and national security interests](#node-public-and-national-security-interests)
- [other union law](#node-other-union-law)
- [fines](#node-fines)
- [smes including start-ups](#node-smes-including-start-ups)
- [questionnaire template (automated tool)](#node-questionnaire-template-automated-tool)
- [consistent practices across the union](#node-consistent-practices-across-the-union)
- [law-enforcement authorities or civil protection authorities](#node-law-enforcement-authorities-or-civil-protection-authorities)
- [relevant documents concerning assessment of qualifications](#node-relevant-documents-concerning-assessment-of-qualifications)
- [personal data or business secrets](#node-personal-data-or-business-secrets)
- [sector in which it operates](#node-sector-in-which-it-operates)
- [design choices](#node-design-choices)
- [board in its reasoned request](#node-board-in-its-reasoned-request)
- [lawyers duly authorised to act](#node-lawyers-duly-authorised-to-act)
- [size of the provider’s organisation](#node-size-of-the-provider-s-organisation)
- [publicly accessible space](#node-publicly-accessible-space)
- [level of protection required](#node-level-of-protection-required)
- [industry](#node-industry)
- [stakeholders](#node-stakeholders)
- [market surveillance governance and enforcement](#node-market-surveillance-governance-and-enforcement)
- [commercial or competitive consultancy services](#node-commercial-or-competitive-consultancy-services)
- [ai systems](#node-ai-systems)
- [regulation (eu) no 1025/2012](#node-regulation-eu-no-1025-2012)
- [risks that cannot be eliminated](#node-risks-that-cannot-be-eliminated)
- [committee procedure](#node-committee-procedure)
- [instructions for use](#node-instructions-for-use)
- [commission](#node-commission)
- [appropriate type and degree of transparency](#node-appropriate-type-and-degree-of-transparency)
- [five-year period](#node-five-year-period)
- [health and safety](#node-health-and-safety)
- [common rules for implementation](#node-common-rules-for-implementation)
- [sandbox plan](#node-sandbox-plan)
- [legitimate interest of individuals or undertakings](#node-legitimate-interest-of-individuals-or-undertakings)
- [parties concerned](#node-parties-concerned)
- [union law protecting fundamental rights](#node-union-law-protecting-fundamental-rights)
- [competent national courts or other bodies](#node-competent-national-courts-or-other-bodies)
- [voluntary application of specific requirements](#node-voluntary-application-of-specific-requirements)
- [union technical documentation assessment certificates](#node-union-technical-documentation-assessment-certificates)
- [achievement of those objectives](#node-achievement-of-those-objectives)
- [natural person](#node-natural-person)
- [special categories of personal data](#node-special-categories-of-personal-data)
- [interpret a system's output](#node-interpret-a-system-s-output)
- [european data protection supervisor](#node-european-data-protection-supervisor)
- [interest to join the full code](#node-interest-to-join-the-full-code)
- [best practices](#node-best-practices)
- [specific ai sandbox project](#node-specific-ai-sandbox-project)
- [high-impact capabilities of general-purpose ai models](#node-high-impact-capabilities-of-general-purpose-ai-models)
- [subsidiaries](#node-subsidiaries)
- [natural persons or groups of persons](#node-natural-persons-or-groups-of-persons)
- [automatic recording of events (logs)](#node-automatic-recording-of-events-logs)
- [dissuasive](#node-dissuasive)
- [evaluation and review](#node-evaluation-and-review)
- [methodology for evaluation of risk levels](#node-methodology-for-evaluation-of-risk-levels)
- [independent experts appointed for this task](#node-independent-experts-appointed-for-this-task)
- [penalties](#node-penalties)
- [critical infrastructure](#node-critical-infrastructure)
- [new conditions or modified conditions](#node-new-conditions-or-modified-conditions)
- [partial application](#node-partial-application)
- [appropriate procedural safeguards](#node-appropriate-procedural-safeguards)
- [physical place](#node-physical-place)
- [legal basis](#node-legal-basis)
- [operator](#node-operator)
- [subliminal techniques beyond a person's consciousness](#node-subliminal-techniques-beyond-a-person-s-consciousness)
- [comparable and verifiable documentation](#node-comparable-and-verifiable-documentation)
- [entry into force](#node-entry-into-force)
- [free and open source licences](#node-free-and-open-source-licences)
- [informed consent](#node-informed-consent)
- [documentation of assessment](#node-documentation-of-assessment)
- [organization implementing the quality management system](#node-organization-implementing-the-quality-management-system)
- [detrimental or unfavourable treatment](#node-detrimental-or-unfavourable-treatment)
- [common specification](#node-common-specification)
- [subcontracting](#node-subcontracting)
- [distributors](#node-distributors)
- [european parliament or council](#node-european-parliament-or-council)
- [co-chairs](#node-co-chairs)
- [other groups of vulnerable persons](#node-other-groups-of-vulnerable-persons)
- [regional or local level](#node-regional-or-local-level)
- [validation data set](#node-validation-data-set)
- [intellectual property rights](#node-intellectual-property-rights)
- [administrative fine](#node-administrative-fine)
- [users](#node-users)
- [union safeguard procedure](#node-union-safeguard-procedure)
- [personal data](#node-personal-data)
- [operators or notified bodies](#node-operators-or-notified-bodies)
- [identification number of the notified body](#node-identification-number-of-the-notified-body)
- [outcome produced involving an ai system](#node-outcome-produced-involving-an-ai-system)
- [single contact point vis-à-vis board](#node-single-contact-point-vis-vis-board)
- [implementing acts](#node-implementing-acts)
- [laws, regulations or administrative provisions](#node-laws-regulations-or-administrative-provisions)
- [ai model](#node-ai-model)
- [particular expertise and competence](#node-particular-expertise-and-competence)
- [controlled environment](#node-controlled-environment)
- [financial resources](#node-financial-resources)
- [['process of drawing-up codes of practice']](#node-process-of-drawing-up-codes-of-practice)
- [degree of responsibility of the operator](#node-degree-of-responsibility-of-the-operator)
- [interface](#node-interface)
- [investment and innovation in ai](#node-investment-and-innovation-in-ai)
- [widespread infringement](#node-widespread-infringement)
- [authorised representatives of providers](#node-authorised-representatives-of-providers)
- [automation bias](#node-automation-bias)
- [confidentiality of information and data obtained](#node-confidentiality-of-information-and-data-obtained)
- [real number](#node-real-number)
- [single point of contact](#node-single-point-of-contact)
- [member states or ai office](#node-member-states-or-ai-office)
- [national accreditation body](#node-national-accreditation-body)
- [public authorities in a third country](#node-public-authorities-in-a-third-country)
- [effective judicial remedies and due process](#node-effective-judicial-remedies-and-due-process)
- [fundamental rights](#node-fundamental-rights)
- [affected workers](#node-affected-workers)
- [standardisation requests](#node-standardisation-requests)
- [criminal proceedings](#node-criminal-proceedings)
- [real-time remote biometric identification system](#node-real-time-remote-biometric-identification-system)
- [ai regulatory sandbox](#node-ai-regulatory-sandbox)
- [cost-effectiveness](#node-cost-effectiveness)
- [union ai testing support structures](#node-union-ai-testing-support-structures)
- [artificial intelligence act](#node-artificial-intelligence-act)
- [related litigation or judicial proceedings](#node-related-litigation-or-judicial-proceedings)
- [relevant data gaps or shortcomings](#node-relevant-data-gaps-or-shortcomings)
- [harmonised standards and common specifications](#node-harmonised-standards-and-common-specifications)
- [emotion recognition system](#node-emotion-recognition-system)
- [european economic and social committee](#node-european-economic-and-social-committee)
- [advisory forum](#node-advisory-forum)
- [impartiality and objectivity](#node-impartiality-and-objectivity)
- [objectivity](#node-objectivity)
- [instructions for use accompanying the systems](#node-instructions-for-use-accompanying-the-systems)
- [compliance with those requirements](#node-compliance-with-those-requirements)
- [deployers](#node-deployers)
- [level of autonomy](#node-level-of-autonomy)
- [requisite competence in the specific field](#node-requisite-competence-in-the-specific-field)
- [guidance](#node-guidance)
- [board and the commission](#node-board-and-the-commission)
- [criteria outlined in article 68(2)](#node-criteria-outlined-in-article-68-2)
- [request for access](#node-request-for-access)
- [risk](#node-risk)
- [innovation and competitiveness](#node-innovation-and-competitiveness)
- [eu declaration of conformity](#node-eu-declaration-of-conformity)
- [areas listed in annex iii](#node-areas-listed-in-annex-iii)
- [ai office](#node-ai-office)
- [codes of conduct](#node-codes-of-conduct)
- [highest degree of professional integrity](#node-highest-degree-of-professional-integrity)
- [authorities informed accordingly](#node-authorities-informed-accordingly)
- [expert](#node-expert)
- [national level registration](#node-national-level-registration)
- [limited period authorization](#node-limited-period-authorization)
- [overall residual risk](#node-overall-residual-risk)
- [sensitive operational data](#node-sensitive-operational-data)
- [union and national law](#node-union-and-national-law)
- [environmental sustainability](#node-environmental-sustainability)
- [general-purpose ai models](#node-general-purpose-ai-models)
- [number of affected persons](#node-number-of-affected-persons)
- [guidelines issued by](#node-guidelines-issued-by)
- [communication with authorities](#node-communication-with-authorities)
- [storage conditions](#node-storage-conditions)
- [legally designated representative](#node-legally-designated-representative)
- [relevant capacities and limitations](#node-relevant-capacities-and-limitations)
- [data necessary for the identification](#node-data-necessary-for-the-identification)
- [obligations laid down in this regulation](#node-obligations-laid-down-in-this-regulation)
- [social score](#node-social-score)
- [relevant provider](#node-relevant-provider)
- [detailed arrangements and conditions of evaluations](#node-detailed-arrangements-and-conditions-of-evaluations)
- [operators of high-risk ai systems](#node-operators-of-high-risk-ai-systems)
- [purpose of request](#node-purpose-of-request)
- [(a) strategy for regulatory compliance](#node-a-strategy-for-regulatory-compliance)
- [testing](#node-testing)
- [registration in the eu database](#node-registration-in-the-eu-database)
- [obligations pursuant to article 16](#node-obligations-pursuant-to-article-16)
- [fair gender and geographical representation](#node-fair-gender-and-geographical-representation)
- [easily corrigible or reversible](#node-easily-corrigible-or-reversible)
- [serious incident](#node-serious-incident)
- [context of use](#node-context-of-use)
- [providers of intermediary services](#node-providers-of-intermediary-services)
- [individual decision-making](#node-individual-decision-making)
- [national competent authorities](#node-national-competent-authorities)
- [occurrence of harm](#node-occurrence-of-harm)
- [specific requirements](#node-specific-requirements)
- [lessons learnt](#node-lessons-learnt)
- [transparency](#node-transparency)
- [information technologies](#node-information-technologies)
- [collective agreements](#node-collective-agreements)
- [requirements set out in this section](#node-requirements-set-out-in-this-section)
- [ai systems or ai models](#node-ai-systems-or-ai-models)
- [quality management system approvals](#node-quality-management-system-approvals)
- [testing in real-world conditions](#node-testing-in-real-world-conditions)
- [downstream provider](#node-downstream-provider)
- [['industry', 'start-ups', 'smes', 'civil society', 'academia']](#node-industry-start-ups-smes-civil-society-academia)
- [physical or virtual environments](#node-physical-or-virtual-environments)
- [confidentiality](#node-confidentiality)
- [ai value chain](#node-ai-value-chain)
- [confidence in their performance](#node-confidence-in-their-performance)
- [data collection processes](#node-data-collection-processes)
- [enforcement measures](#node-enforcement-measures)
- [real-world testing plan](#node-real-world-testing-plan)
- [elements necessary for technical progress](#node-elements-necessary-for-technical-progress)
- [union agency](#node-union-agency)
- [mitigation and control measures](#node-mitigation-and-control-measures)
- [risk involved](#node-risk-involved)
- [sufficiently representative input data](#node-sufficiently-representative-input-data)
- [notifying authority](#node-notifying-authority)
- [chapters i and ii](#node-chapters-i-and-ii)
- [inspections, investigations or audits](#node-inspections-investigations-or-audits)
- [product](#node-product)
- [deployers (who are employers)](#node-deployers-who-are-employers)
- [experts designated by each member state](#node-experts-designated-by-each-member-state)
- [shortcomings](#node-shortcomings)
- [evidence-based regulatory learning](#node-evidence-based-regulatory-learning)
- [measures to support innovation](#node-measures-to-support-innovation)
- [manipulative or deceptive techniques](#node-manipulative-or-deceptive-techniques)
- [data governance and management practices](#node-data-governance-and-management-practices)
- [reasoned complaints](#node-reasoned-complaints)
- [direct applicability](#node-direct-applicability)
- [benchmarks and measurement methodologies](#node-benchmarks-and-measurement-methodologies)
- [independent experts](#node-independent-experts)
- [['smes', 'other undertakings']](#node-smes-other-undertakings)
- [principle of proportionality](#node-principle-of-proportionality)
- [market and technological developments](#node-market-and-technological-developments)
- [military, defence or national security purposes](#node-military-defence-or-national-security-purposes)
- [logs referred to in article 12(1)](#node-logs-referred-to-in-article-12-1)
- [persons under the age of 18](#node-persons-under-the-age-of-18)
- [adequate level of cybersecurity measures](#node-adequate-level-of-cybersecurity-measures)
- [board](#node-board)
- [clear objectives](#node-clear-objectives)
- [market monitoring](#node-market-monitoring)
- [data](#node-data)
- [subject](#node-subject)
- [intended purpose](#node-intended-purpose)
- [amendments](#node-amendments)
- [simplified manner](#node-simplified-manner)
- [biometric data](#node-biometric-data)
- [narrow procedural task](#node-narrow-procedural-task)
- [experts and other stakeholders](#node-experts-and-other-stakeholders)
- [responsibilities](#node-responsibilities)
- [floating-point operation](#node-floating-point-operation)
- [academia](#node-academia)
- [internal market](#node-internal-market)
- [ethical review](#node-ethical-review)
- [up to date with relevant standards](#node-up-to-date-with-relevant-standards)
- [representative](#node-representative)
- [union institutions, bodies, offices and agencies](#node-union-institutions-bodies-offices-and-agencies)
- [provision of information](#node-provision-of-information)
- [workers' rights](#node-workers-rights)
- [delegated act](#node-delegated-act)
- [law](#node-law)
- [ce marking](#node-ce-marking)
- [high-impact capabilities](#node-high-impact-capabilities)
- [liability](#node-liability)
- [digital ce marking](#node-digital-ce-marking)
- [bias detection and correction](#node-bias-detection-and-correction)
- [['providers', 'prospective providers', 'deployers']](#node-providers-prospective-providers-deployers)
- [council](#node-council)
- [innovative ai systems](#node-innovative-ai-systems)
- [member states and their relevant authorities](#node-member-states-and-their-relevant-authorities)
- [effects and possible interaction](#node-effects-and-possible-interaction)
- [data governance](#node-data-governance)
- [board's rules of procedure](#node-board-s-rules-of-procedure)
- [right to explanation](#node-right-to-explanation)
- [key performance indicators](#node-key-performance-indicators)
- [implementation of regulation](#node-implementation-of-regulation)
- [data management](#node-data-management)
- [union institution](#node-union-institution)
- [certificate](#node-certificate)
- [scientific research and development](#node-scientific-research-and-development)
- [residual risk associated with each hazard](#node-residual-risk-associated-with-each-hazard)
- [human-centric and trustworthy artificial intelligence](#node-human-centric-and-trustworthy-artificial-intelligence)
- [other relevant bodies or sources](#node-other-relevant-bodies-or-sources)
- [effective](#node-effective)
- [arguments submitted pursuant to paragraph 2](#node-arguments-submitted-pursuant-to-paragraph-2)
- [anomalies, dysfunctions and unexpected performance](#node-anomalies-dysfunctions-and-unexpected-performance)
- [public](#node-public)
- [fines imposed under this article](#node-fines-imposed-under-this-article)
- [level of damage suffered by them](#node-level-of-damage-suffered-by-them)
- [third country](#node-third-country)
- [market surveillance authorities, commission](#node-market-surveillance-authorities-commission)
- [annex iii](#node-annex-iii)
- [directive (eu) 2022/2557](#node-directive-eu-2022-2557)
- [article 6(1)](#node-article-6-1)
- [conformity assessment body](#node-conformity-assessment-body)
- [competent personnel](#node-competent-personnel)
- [eu database](#node-eu-database)
- [failure to provide access](#node-failure-to-provide-access)
- [placing on the market](#node-placing-on-the-market)
- [ai literacy](#node-ai-literacy)
- [types of ai systems concerned](#node-types-of-ai-systems-concerned)
- [up-to-date scientific or technical expertise](#node-up-to-date-scientific-or-technical-expertise)
- [regulation](#node-regulation)
- [assessment activities](#node-assessment-activities)
- [promotional material](#node-promotional-material)
- [providers](#node-providers)

---

## Node: limited time
<a name="node-limited-time"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **For a limited time ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.



---

## Node: examination, test and validation procedures
<a name="node-examination-test-and-validation-procedures"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **To be carried out → examination, test and validation procedures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

### Incoming relationships

- **To be carried out ← examination, test and validation procedures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.



---

## Node: union ethical guidelines for trustworthy ai
<a name="node-union-ethical-guidelines-for-trustworthy-ai"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **(a) → union ethical guidelines for trustworthy ai**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

### Incoming relationships

- **(a) ← union ethical guidelines for trustworthy ai**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.



---

## Node: development or design
<a name="node-development-or-design"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Can mitigate or eliminate ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 4

    > 3. The risks referred to in this Article shall concern only those which may be reasonably 
    > mitigated or eliminated through the development or design of the high-risk AI system, or 
    > the provision of adequate technical information.



---

## Node: systemic risk
<a name="node-systemic-risk"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Involved in ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 5: Obligations for providers of general-purpose ai models with systemic risk
  - Paragraph 1

    > Article 55
    > Obligations for providers of general-purpose AI models with systemic risk

- **Pursuant to paragraph 4 ← commission**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **specific to ← high-impact capabilities of general-purpose ai models**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 66

    > (65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-
    > purpose AI models, having a significant impact on the Union market due to their reach, 
    > or due to actual or reasonably foreseeable negative effects on public health, safety, 
    > public security, fundamental rights, or the society as a whole, that can be propagated at 
    > scale across the value chain;



---

## Node: legal certainty
<a name="node-legal-certainty"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Promote → investment and innovation in ai**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 4

    > 3. The participants in the standardisation process shall seek to promote investment and 
    > innovation in AI, including through increasing legal certainty, as well as the 
    > competitiveness and growth of the Union market, and shall contribute to strengthening 
    > global cooperation on standardisation and taking into account existing international 
    > standards in the field of AI that are consistent with Union values, fundamental rights 
    > and interests, and shall enhance multi-stakeholder governance ensuring a balanced 
    > representation of interests and the effective participation of all relevant stakeholders in 
    > accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.

### Incoming relationships

- **Contribute to ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.



---

## Node: microenterprises
<a name="node-microenterprises"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Applicable to → simplified manner**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 2

    > 1. Microenterprises within the meaning of Recommendation 2003/361/EC, may comply 
    > with certain elements of the quality management system required by Article 17 of this 
    > Regulation in a simplified manner, provided that they do not have partner enterprises or 
    > linked enterprises within the meaning of that Recommendation. For that purpose, the 
    > Commission shall develop guidelines on the elements of the quality management system 
    > which may be complied with in a simplified manner considering the needs of 
    > microenterprises, without affecting the level of protection or the need for compliance 
    > with the requirements in respect of high-risk AI systems.

### Incoming relationships

- **Considers ← commission**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 2

    > 1. Microenterprises within the meaning of Recommendation 2003/361/EC, may comply 
    > with certain elements of the quality management system required by Article 17 of this 
    > Regulation in a simplified manner, provided that they do not have partner enterprises or 
    > linked enterprises within the meaning of that Recommendation. For that purpose, the 
    > Commission shall develop guidelines on the elements of the quality management system 
    > which may be complied with in a simplified manner considering the needs of 
    > microenterprises, without affecting the level of protection or the need for compliance 
    > with the requirements in respect of high-risk AI systems.



---

## Node: union or member states
<a name="node-union-or-member-states"></a>

*21 outgoing, 21 incoming*

### Outgoing relationships

- **Applicable to infringements of this Regulation by → operator**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.

- **Lay down the rules on → penalties**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.

- **Lay down the rules on → enforcement measures**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.

- **Receives annual reports → commission**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 13

    > 11. Member States shall, on an annual basis, report to the Commission about the 
    > administrative fines they have issued during that year, in accordance with this Article, 
    > and about any related litigation or judicial proceedings.

- **Bound by → direct applicability**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Ensure → national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 2

    > 1. Member States shall ensure that their competent authorities establish at least one AI 
    > regulatory sandbox at national level, which shall be operational by … [24 months from 
    > the date of entry into force of this Regulation]. That sandbox may also be established 
    > jointly with the competent authorities of one or more other Member States. The 
    > Commission may provide technical support, advice and tools for the establishment and 
    > operation of AI regulatory sandboxes.
    > The obligation under the first subparagraph may also be fulfilled by participating in an 
    > existing sandbox in so far as that participation provides an equivalent level of national 
    > coverage for the participating Member States.

- **Identify → public authorities in a third country**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 3

    > 2. By … [three months after the entry into force of this Regulation], each Member State shall 
    > identify the public authorities or bodies referred to in paragraph 1 and make a list of them 
    > publicly available  . Member States shall notify the list to the Commission and to the 
    > other Member States, and shall keep the list up to date.

- **Immediately informs → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Resulted from → national level registration**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Established under national law → notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 2

    > 1. A notified body shall be established under the national law of a Member State and shall 
    > have legal personality.

- **Designates → representative**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 4

    > 3. Each representative shall be designated by their Member State for a period of three 
    > years, renewable once.

- **establishes or designates → national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 2

    > 1. Each Member State shall establish or designate as national competent authorities at least 
    > one notifying authority and at least one market surveillance authority for the purposes of 
    > this Regulation. Those national competent authorities shall exercise their powers 
    > independently, impartially and without bias so as to safeguard the objectivity of their 
    > activities and tasks, and to ensure the application and implementation of this Regulation. 
    > The members of those authorities shall refrain from any action incompatible with their 
    > duties. Provided that those principles are observed, such activities and tasks may be 
    > performed by one or more designated authorities, in accordance with the organisational 
    > needs of the Member State.

- **Designated as a single contact point → single contact point vis-à-vis board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **Single contact point for stakeholders → stakeholders**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **Designated representatives of → board's rules of procedure**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 6

    > 5. The designated representatives of the Member States shall adopt the Board’s rules of 
    > procedure by a two-thirds majority. The rules of procedure shall, in particular, lay down 
    > procedures for the selection process, the duration of the mandate of, and specifications 
    > of the tasks of, the Chair, detailed arrangements for voting, and the organisation of the 
    > Board’s activities and those of its sub-groups.

- **requirement for payment of fees → expert**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 3

    > 2. The Member States may be required to pay fees for the advice and support provided by 
    > the experts. The structure and the level of fees as well as the scale and structure of 
    > recoverable costs shall be set out in the implementing act referred to in Article 68(1), 
    > taking into account the objectives of the adequate implementation of this Regulation, 
    > cost-effectiveness and the necessity of ensuring effective access to experts for all 
    > Member States.

- **Communicate identity of → market surveillance governance and enforcement**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 3

    > 2. Member States shall communicate to the Commission the identity of the notifying 
    > authorities and the market surveillance authorities and the tasks of those authorities, as 
    > well as any subsequent changes thereto. Member States shall make publicly available 
    > information on how competent authorities and single points of contact can be contacted, 
    > through electronic communication means by… [12 months from the date of entry into 
    > force of this Regulation]. Member States shall designate a market surveillance authority 
    > to act as the single point of contact for this Regulation, and shall notify the Commission 
    > of the identity of the single point of contact. The Commission shall make a list of the 
    > single points of contact publicly available.

- **Designate → single point of contact**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 3

    > 2. Member States shall communicate to the Commission the identity of the notifying 
    > authorities and the market surveillance authorities and the tasks of those authorities, as 
    > well as any subsequent changes thereto. Member States shall make publicly available 
    > information on how competent authorities and single points of contact can be contacted, 
    > through electronic communication means by… [12 months from the date of entry into 
    > force of this Regulation]. Member States shall designate a market surveillance authority 
    > to act as the single point of contact for this Regulation, and shall notify the Commission 
    > of the identity of the single point of contact. The Commission shall make a list of the 
    > single points of contact publicly available.

- **balanced with regard to commercial and non-commercial interests → ['industry', 'start-ups', 'smes', 'civil society', 'academia']**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.

- **balanced with regard to commercial interests → ['smes', 'other undertakings']**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.

- **selected from among → co-chairs**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 7

    > 6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from 
    > among its members, in accordance with criteria set out in paragraph 2. The term of 
    > office of the co-chairs shall be two years, renewable once.

### Incoming relationships

- **Encouraging or allowing ← collective agreements**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **Reports financial and human resources ← commission**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 7

    > 6. By …, [one year from the date of entry into force of this Regulation] and once every two 
    > years thereafter, Member States shall report to the Commission  on the status of the 
    > financial and human resources of the national competent authorities, with an assessment of 
    > their adequacy. The Commission shall transmit that information to the Board for 
    > discussion and possible recommendations.

- **Provides exchange for knowledge and best practices ← commission**
  - Chapter 3: High-risk ai systems
  - Article 33: Coordination of notified bodies
  - Paragraph 4

    > 3. The Commission shall provide for the exchange of knowledge and best practices between 
    > the notifying authorities of the Member States.

- **Communicates decision to ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 6

    > 5. The Commission shall immediately communicate its decision to the Member States 
    > concerned and to the relevant operators. It shall also inform the other Member States.

- **Maintaining or introducing ← laws, regulations or administrative provisions**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **Providing equivalent level of national coverage ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 2

    > 1. Member States shall ensure that their competent authorities establish at least one AI 
    > regulatory sandbox at national level, which shall be operational by … [24 months from 
    > the date of entry into force of this Regulation]. That sandbox may also be established 
    > jointly with the competent authorities of one or more other Member States. The 
    > Commission may provide technical support, advice and tools for the establishment and 
    > operation of AI regulatory sandboxes.
    > The obligation under the first subparagraph may also be fulfilled by participating in an 
    > existing sandbox in so far as that participation provides an equivalent level of national 
    > coverage for the participating Member States.

- **Does not preclude ← regulation**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **Facilitate ← ai office**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Facilitates tasks entrusted ← ai office**
  - Chapter 7: Governance
  - Article 1: Ai office
  - Paragraph 3

    > 2. Member States shall facilitate the tasks entrusted to the AI Office, as reflected in this 
    > Regulation.

- **Encourage and facilitate ← codes of conduct**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 2

    > 1. The AI Office and the Member States shall encourage and facilitate the drawing up of 
    > codes of conduct, including related governance mechanisms, intended to foster the 
    > voluntary application to AI systems, other than high-risk AI systems, of some or all of the 
    > requirements set out in Chapter III, Section 2 taking into account the available technical 
    > solutions and industry best practices allowing for the application of such requirements.

- **Immediately informs ← union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Notifies ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Determines conditions for documentation availability ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 13: Documentation keeping
  - Paragraph 3

    > 2. Each Member State shall determine conditions under which the documentation referred 
    > to in paragraph 1 remains at the disposal of the national competent authorities for the 
    > period indicated in that paragraph for the cases when a provider or its authorised 
    > representative established on its territory goes bankrupt or ceases its activity prior to the 
    > end of that period.

- **Contribute actively to the achievement of tasks ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **applied by ← administrative fine**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **Informs ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 4

    > 3. Where the market surveillance authority considers that the use of the AI system 
    > concerned is not restricted to its national territory, it shall inform the Commission and 
    > the other Member States without undue delay of the results of the evaluation and of the 
    > actions which it has required the provider to take.

- **interrupts through a stop button or similar procedure ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **Informs ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 2

    > 1. The notifying authority shall notify the Commission and the other Member States of any 
    > relevant changes to the notification of a notified body via the electronic notification tool 
    > referred to in Article 30(2).

- **Informs ← market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 5

    > 2. The market surveillance authority shall inform the Commission and the other Member 
    > States of any authorisation issued pursuant to paragraph 1. This obligation shall not cover 
    > sensitive operational data in relation to the activities of law-enforcement authorities.

- **Related to ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Requests measures ← notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 1

    > Article 93
    > Power to request measures



---

## Node: sufficiently transparent operation
<a name="node-sufficiently-transparent-operation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.



---

## Node: supplying incorrect, incomplete or misleading information
<a name="node-supplying-incorrect-incomplete-or-misleading-information"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Associated with ← logs referred to in article 12(1)**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.



---

## Node: degree of rigour
<a name="node-degree-of-rigour"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Compliance with this Regulation ← providers**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.



---

## Node: impact and effectiveness evaluation
<a name="node-impact-and-effectiveness-evaluation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Undergoes ← codes of conduct**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 8

    > 7. By … [four years from the date of entry into force of this Regulation] and every three 
    > years thereafter, the Commission shall evaluate the impact and effectiveness of voluntary 
    > codes of conduct to foster the application of the requirements set out in Chapter II, Section



---

## Node: appropriate corrective action
<a name="node-appropriate-corrective-action"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← operator**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 5

    > 4. The operator shall ensure that all appropriate corrective action is taken in respect of all the 
    > AI systems concerned that it has made available on the Union market.

- **Taken in respect of ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 6

    > 5. The provider shall ensure that all appropriate corrective action is taken in respect of all 
    > the AI systems concerned that it has made available on the Union market.

- **Fails to take ← providers of intermediary services**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 7

    > 6. Where the provider of the AI system concerned does not take adequate corrective action 
    > within the period referred to in paragraph 2 of this Article, Article 79(5) to (9) shall 
    > apply.



---

## Node: suspension
<a name="node-suspension"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **shall suspend the testing in real world conditions ← authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.



---

## Node: provider of an ai system
<a name="node-provider-of-an-ai-system"></a>

*8 outgoing, 5 incoming*

### Outgoing relationships

- **Represents → authorised representatives of providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 6

    > (5) ‘authorised representative’ means a natural or legal person located or established in the 
    > Union who has received and accepted a written mandate from a provider of an AI system 
    > or a general-purpose AI model to, respectively, perform and carry out on its behalf the 
    > obligations and procedures established by this Regulation;

- **includes → ai model**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 69

    > (68) ‘downstream provider’ means a provider of an AI system, including a general-purpose 
    > AI system, which integrates an AI model, regardless of whether the model is provided by 
    > themselves and vertically integrated or provided by another entity based on contractual 
    > relations.

- **Collects and reviews experience gained from use → market monitoring**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 26

    > (25) ‘post-market monitoring system’ means all activities carried out by providers of AI 
    > systems to  collect and review experience gained from the use of AI systems they place 
    > on the market or put into service for the purpose of identifying any need to immediately 
    > apply any necessary corrective or preventive actions;

- **Imposes → proportional**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 1

    > Article 26
    > Obligations of deployers of high-risk AI systems

- **Use in accordance with → instructions for use accompanying the systems**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 2

    > 1. Deployers of high-risk AI systems shall take appropriate technical and organisational 
    > measures to ensure they use such systems in accordance with the instructions for use 
    > accompanying the systems, pursuant to paragraphs 3 and 6.

- **Appropriate to → operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 7

    > 6. Deployers of high-risk AI systems shall keep the logs automatically generated by that 
    > high-risk AI system  to the extent such logs are under their control,  for a period  
    > appropriate to the intended purpose of the high-risk AI system, of at least six months, 
    > unless provided otherwise in applicable Union or national law, in particular in Union law 
    > on the protection of personal data.
    > Deployers that are financial institutions subject to requirements regarding their internal 
    > governance, arrangements or processes under Union financial services law shall 
    > maintain the logs as part of the documentation kept pursuant to the relevant Union 
    > financial service law.

- **Unless provided otherwise in applicable → union and national law**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 7

    > 6. Deployers of high-risk AI systems shall keep the logs automatically generated by that 
    > high-risk AI system  to the extent such logs are under their control,  for a period  
    > appropriate to the intended purpose of the high-risk AI system, of at least six months, 
    > unless provided otherwise in applicable Union or national law, in particular in Union law 
    > on the protection of personal data.
    > Deployers that are financial institutions subject to requirements regarding their internal 
    > governance, arrangements or processes under Union financial services law shall 
    > maintain the logs as part of the documentation kept pursuant to the relevant Union 
    > financial service law.

- **Transparency obligations → types of ai systems concerned**
  - Chapter 4: Transparency obligations for providers and deployers of certain ai systems 
  - Article 1: Transparency obligations for providers and users of certain ai systems
  - Paragraph 1

    > Article 50
    > Transparency obligations for providers and users of certain AI systems

### Incoming relationships

- **Provide information and documentation ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 2

    > 1. Providers of general-purpose AI models shall:
    > (a) draw up and keep up-to-date the technical documentation of the model, including 
    > its training and testing process and the results of its evaluation, which shall 
    > contain, at a minimum, the elements set out in Annex XI for the purpose of 
    > providing it, upon request, to the AI Office and the national competent authorities;
    > (b) draw up, keep up-to-date and make available information and documentation to 
    > providers of AI systems who intend to integrate the general-purpose AI model into 
    > their AI systems. Without prejudice to the need to respect and protect intellectual 
    > property rights and confidential business information or trade secrets in 
    > accordance with Union and national law, the information and documentation 
    > shall:
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > 
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (c) put in place a policy to comply with Union copyright law, and in particular to 
    > identify and comply with, including through state of the art technologies, a 
    > reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;
    > (d) draw up and make publicly available a sufficiently detailed summary about the 
    > content used for training of the general-purpose AI model, according to a template 
    > provided by the AI Office.

- **open to any applying ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 3

    > 2. The implementing acts referred to in paragraph 1 shall ensure that:
    > (a) AI regulatory sandboxes are open to any applying prospective provider of an AI 
    > system who fulfils eligibility and selection criteria, which shall be transparent and 
    > fair and national competent authorities inform applicants of their decision within 
    > three months of the application;
    > (b) AI regulatory sandboxes allow broad and equal access and keep up with demand 
    > for participation; prospective providers may also submit applications in 
    > partnerships with users and other relevant third parties;
    > (c) the detailed arrangements for and conditions concerning AI regulatory sandboxes 
    > to the best extent possible support flexibility for national competent authorities to 
    > establish and operate their AI regulatory sandboxes;
    > (d) access to the AI regulatory sandboxes is free of charge for SMEs, including start-
    > ups, without prejudice to exceptional costs that national competent authorities may 
    > recover in a fair and proportionate manner;
    > (e) they facilitate prospective providers, by means of the learning outcomes of the AI 
    > regulatory sandboxes, in complying with conformity assessment obligations under 
    > this Regulation and the voluntary application of the codes of conduct referred to in 
    > Article 95;
    > (f) AI regulatory sandboxes facilitate the involvement of other relevant actors within 
    > the AI ecosystem, such as notified bodies and standardisation organisations, 
    > SMEs, start-ups, enterprises, innovators, testing and experimentation facilities, 
    > research and experimentation labs and European Digital Innovation Hubs, centres 
    > of excellence, individual researchers, in order to allow and facilitate cooperation 
    > with the public and private sectors;
    > (g) procedures, processes and administrative requirements for application, selection, 
    > participation and exiting the AI regulatory sandbox are simple, easily intelligible, 
    > and clearly communicated in order to facilitate the participation of SMEs, 
    > including start-ups, with limited legal and administrative capacities and are 
    > streamlined across the Union, in order to avoid fragmentation and that 
    > participation in an AI regulatory sandbox established by a Member State, or by the 
    > European Data Protection Supervisor is mutually and uniformly recognised and 
    > carries the same legal effects across the Union;
    > (h) participation in the AI regulatory sandbox is limited to a period that is appropriate 
    > to the complexity and scale of the project, which may be extended by the national 
    > competent authority;
    > (i) AI regulatory sandboxes facilitate the development of tools and infrastructure for 
    > testing, benchmarking, assessing and explaining dimensions of AI systems 
    > relevant for regulatory learning, such as accuracy, robustness and cybersecurity, 
    > as well as measures to mitigate risks to fundamental rights and society at large.

- **considers ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **gives reasons for its decision ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **Informs ← distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 3

    > 2. Where a distributor considers or has reason to consider, on the basis of the information in 
    > its possession, a high-risk AI system not to be in conformity with the requirements set out 
    > in Section 2, it shall not make the high-risk AI system available on the market until the 
    > system has been brought into conformity with those requirements. Furthermore, where the 
    > high-risk AI system presents a risk within the meaning of Article 79(1), the distributor 
    > shall inform the provider or the importer of the system, as applicable, to that effect.



---

## Node: systemic risks at union level
<a name="node-systemic-risks-at-union-level"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Encourages and facilitates drawing up ← ai office**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 2

    > 1. The AI Office shall encourage and facilitate the drawing up of codes of practice at 
    > Union level in order to contribute to the proper application of this Regulation, taking 
    > into account international approaches.



---

## Node: safety component
<a name="node-safety-component"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Intended to be used as ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 2

    > 1. Irrespective of whether an AI system is placed on the market or put into service 
    > independently from the products referred to in points (a) and (b), that AI system shall be 
    > considered to be high-risk where both of the following conditions are fulfilled:
    > (a) the AI system is intended to be used as a safety component of a product, or the AI 
    > system is itself a product, covered by the Union harmonisation legislation listed in 
    > Annex I;
    > (b) the product whose safety component pursuant to point (a) is the AI system, or the 
    > AI system itself as a product, is required to undergo a third-party conformity 
    > assessment, with a view to the placing on the market or the putting into service of 
    > that product pursuant to the Union harmonisation legislation listed in Annex I.



---

## Node: same operator for the same infringement
<a name="node-same-operator-for-the-same-infringement"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **regard is given to ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.



---

## Node: documentation
<a name="node-documentation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Full access granted to ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 13

    > 12. Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and 
    > where relevant and limited to what is necessary to fulfil their tasks, the market 
    > surveillance authorities shall be granted full access by providers to the documentation as 
    > well as the training, validation and testing data sets used for the development of high-
    > risk AI systems, including, where appropriate and subject to security safeguards, 
    > through application programming interfaces (‘API’) or other relevant technical means 
    > and tools enabling remote access.



---

## Node: ai
<a name="node-ai"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Expertise in fields such as ← competent personnel**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.



---

## Node: product manufacturer
<a name="node-product-manufacturer"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **can be a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

### Incoming relationships

- **Considered as ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 4

    > 3. In the case of high-risk AI systems that are safety components of products covered by the 
    > Union harmonisation legislation listed in Section A of Annex I, the product manufacturer 
    > shall be considered to be the provider of the high-risk AI system, and shall be subject to 
    > the obligations under Article 16 under either of the following circumstances:
    > (a) the high-risk AI system is placed on the market together with the product under the 
    > name or trademark of the product manufacturer;
    > (b) the high-risk AI system is put into service under the name or trademark of the 
    > product manufacturer after the product has been placed on the market.



---

## Node: proportional
<a name="node-proportional"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Performs ← conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 1

    > Article 29
    > Application of a conformity assessment body for notification

- **Performs ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 1

    > Article 21
    > Cooperation with competent authorities

- **Requires ← fines**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.

- **Imposes ← provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 1

    > Article 26
    > Obligations of deployers of high-risk AI systems



---

## Node: harmonised standard
<a name="node-harmonised-standard"></a>

*1 outgoing, 3 incoming*

### Outgoing relationships

- **Addressing request → standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 2

    > 1. The Commission is empowered to adopt, implementing acts establishing common 
    > specifications for the requirements set out in Section 2 of this Chapter or, as applicable, 
    > for the obligations set out in Chapter IV where the following conditions have been 
    > fulfilled:
    > (a) the Commission has requested, pursuant to Article 10(1) of Regulation (EU) 
    > No 1025/2012, one or more European standardisation organisations to draft a 
    > harmonised standard for the requirements set out in Section 2 of this Chapter, 
    > and:
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > 
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > (b) no reference to harmonised standards covering the requirements referred to in 
    > Section 2 of this Title has been published in the Official Journal of the European 
    > Union in accordance with Regulation (EU) No 1025/2012, and no such reference 
    > is expected to be published within a reasonable period.
    > The implementing acts referred to in the first subparagraph of this paragraph shall be 
    > adopted in accordance with the examination procedure referred to in Article 98(2), 
    > after consulting the advisory forum referred to in Article 67.

### Incoming relationships

- **defines ← regulation (eu) no 1025/2012**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 28

    > (27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of 
    > Regulation (EU) No 1025/2012;

- **Refers to in Official Journal of the European Union ← commission**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 5

    > 4. Where a harmonised standard is adopted by a European standardisation organisation 
    > and proposed to the Commission for the publication of its reference in the Official 
    > Journal of the European Union, the Commission shall assess the harmonised standard 
    > in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised 
    > standard is published in the Official Journal of the European Union, the Commission 
    > shall repeal the implementing acts referred to in paragraph 1, or parts thereof which 
    > cover the same requirements set out in Section 2 of this Chapter.

- **Conformity with ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 2

    > 1. High-risk AI systems which are in conformity with harmonised standards or parts thereof 
    > the references of which have been published in the Official Journal of the European Union 
    > in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity 
    > with the requirements set out in Section 2 of this Chapter or, as applicable, with the 
    > obligations set out in Chapter IV of this Regulation, to the extent that those standards 
    > cover those requirements or obligations.



---

## Node: training, validation and testing data sets
<a name="node-training-validation-and-testing-data-sets"></a>

*7 outgoing, 1 incoming*

### Outgoing relationships

- **Subject to → data governance and management practices**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Concern → design choices**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Origin of → data collection processes**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Applies to → personal data**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Formulation of → regulation**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Identify → relevant data gaps or shortcomings**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Have → appropriate procedural safeguards**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 4

    > 3. Training, validation and testing data sets shall be relevant, sufficiently representative, and 
    > to the best extent possible, free of errors and complete in view of the intended purpose. 
    > They shall have the appropriate statistical properties, including, where applicable, as 
    > regards the persons or groups of persons in relation to whom the high-risk AI system is 
    > intended to be used. Those characteristics of the data sets may be met at the level of 
    > individual data sets or at the level of a combination thereof.

### Incoming relationships

- **Intended purpose of ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.



---

## Node: product presenting a risk
<a name="node-product-presenting-a-risk"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **similar function ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 2

    > 1. AI systems presenting a risk shall be understood as a “product presenting a risk” as defined 
    > in Article 3, point 19 of Regulation (EU) 2019/1020, in so far as they present risks to the 
    > health or safety, or to  fundamental rights, of persons.



---

## Node: section 4
<a name="node-section-4"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Belongs to → informed consent**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 12

    > 12. National competent authorities shall immediately notify the Commission of any serious 
    > incident, whether or not it they have taken action on it, in accordance with Article 20 of 
    > Regulation (EU) 2019/1020.
    > Section 3
    > Enforcement

### Incoming relationships

- **Related to ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies



---

## Node: importers
<a name="node-importers"></a>

*9 outgoing, 4 incoming*

### Outgoing relationships

- **More favourable to → ai systems or ai models**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **is a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **comply with → obligations pursuant to article 16**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **May be accompanied by → obligations laid down in this regulation**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 4

    > 3. By … [four years from the date of entry into force of this Regulation] and every four years 
    > thereafter, the Commission shall submit a report on the evaluation and review of this 
    > Regulation to the European Parliament and to the Council. The report shall include an 
    > assessment with regard to the structure of enforcement and the possible need for a 
    > Union agency to resolve any identified shortcomings. On the basis of the findings, that 
    > report shall, where appropriate, be accompanied by a proposal for amendment of this 
    > Regulation. The reports shall be made public.

- **Recipient of → european parliament or council**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Recipient of → council**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Recipient of → european economic and social committee**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Adheres to → partial application**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 1

    > Article 23
    > Obligations of importers

- **Applicable to → distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 2

    > 1. Before making a high-risk AI system available on the market, distributors shall verify that 
    > it bears the required CE marking, that it is accompanied by a copy of EU declaration of 
    > conformity and instructions for use, and that the provider and the importer of the system, 
    > as applicable, have complied with their respective obligations as laid down in Article 16, 
    > points (b) and (c) and Article 23(3).

### Incoming relationships

- **place on the market ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **focuses on ← national competent authorities**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **Cooperate in action taken ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 8

    > 7. Importers shall cooperate with national competent authorities in any action those 
    > authorities take in relation to a high-risk AI system the importers placed on the market, 
    > in particular to reduce and mitigate the risks posed by it.

- **Inform ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.



---

## Node: public and national security interests
<a name="node-public-and-national-security-interests"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Does not contravene ← directive (eu) 2022/2557**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.



---

## Node: other union law
<a name="node-other-union-law"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Decreases overall level of protection ← commission**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > list in Annex III by removing high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the high-risk AI system concerned no longer poses any significant risks to 
    > fundamental rights, health or safety, taking into account the criteria listed in 
    > paragraph 2;
    > (b) the deletion does not decrease the overall level of protection of health, safety and 
    > fundamental rights under Union law.
    > Section 2
    > Requirements for high-risk AI systems

- **Infringes ← ai office**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 9

    > 8. This Article shall not affect the prohibitions that apply where an AI practice infringes 
    > other Union law.

- **Not otherwise provided for ← logs referred to in article 12(1)**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 15: Right to explanation of individual decision-making
  - Paragraph 4

    > 3. This Article shall apply only to the extent that the right referred to in paragraph 1 is not 
    > otherwise provided for under Union law.



---

## Node: fines
<a name="node-fines"></a>

*4 outgoing, 3 incoming*

### Outgoing relationships

- **Causes → effective**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.

- **Requires → proportional**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.

- **Aims to be → dissuasive**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.

- **taking into account → cost-effectiveness**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 3

    > 2. The Member States may be required to pay fees for the advice and support provided by 
    > the experts. The structure and the level of fees as well as the scale and structure of 
    > recoverable costs shall be set out in the implementing act referred to in Article 68(1), 
    > taking into account the objectives of the adequate implementation of this Regulation, 
    > cost-effectiveness and the necessity of ensuring effective access to experts for all 
    > Member States.

### Incoming relationships

- **Subject to ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 5

    > 4. The provider shall ensure that all necessary action is taken to bring the AI system into 
    > compliance with the requirements and obligations laid down in this Regulation. Where 
    > the provider of an AI system concerned does not bring the AI system into compliance 
    > with those requirements and obligations within the period referred to in paragraph 2 of 
    > this Article, the provider shall be subject to fines in accordance with Article 99.

- **Applicable to ← smes including start-ups**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 8

    > 6. In the case of SMEs, including start-ups, each fine referred to in this Article shall be up 
    > to the percentages or amount referred to paragraphs 3, 4 and 5, whichever thereof is 
    > lower.

- **Imposes ← general-purpose ai models**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 1

    > Article 101
    > Fines for providers of general-purpose AI models



---

## Node: smes including start-ups
<a name="node-smes-including-start-ups"></a>

*1 outgoing, 3 incoming*

### Outgoing relationships

- **Applicable to → fines**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 8

    > 6. In the case of SMEs, including start-ups, each fine referred to in this Article shall be up 
    > to the percentages or amount referred to paragraphs 3, 4 and 5, whichever thereof is 
    > lower.

### Incoming relationships

- **free of charge ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 3

    > 2. The implementing acts referred to in paragraph 1 shall ensure that:
    > (a) AI regulatory sandboxes are open to any applying prospective provider of an AI 
    > system who fulfils eligibility and selection criteria, which shall be transparent and 
    > fair and national competent authorities inform applicants of their decision within 
    > three months of the application;
    > (b) AI regulatory sandboxes allow broad and equal access and keep up with demand 
    > for participation; prospective providers may also submit applications in 
    > partnerships with users and other relevant third parties;
    > (c) the detailed arrangements for and conditions concerning AI regulatory sandboxes 
    > to the best extent possible support flexibility for national competent authorities to 
    > establish and operate their AI regulatory sandboxes;
    > (d) access to the AI regulatory sandboxes is free of charge for SMEs, including start-
    > ups, without prejudice to exceptional costs that national competent authorities may 
    > recover in a fair and proportionate manner;
    > (e) they facilitate prospective providers, by means of the learning outcomes of the AI 
    > regulatory sandboxes, in complying with conformity assessment obligations under 
    > this Regulation and the voluntary application of the codes of conduct referred to in 
    > Article 95;
    > (f) AI regulatory sandboxes facilitate the involvement of other relevant actors within 
    > the AI ecosystem, such as notified bodies and standardisation organisations, 
    > SMEs, start-ups, enterprises, innovators, testing and experimentation facilities, 
    > research and experimentation labs and European Digital Innovation Hubs, centres 
    > of excellence, individual researchers, in order to allow and facilitate cooperation 
    > with the public and private sectors;
    > (g) procedures, processes and administrative requirements for application, selection, 
    > participation and exiting the AI regulatory sandbox are simple, easily intelligible, 
    > and clearly communicated in order to facilitate the participation of SMEs, 
    > including start-ups, with limited legal and administrative capacities and are 
    > streamlined across the Union, in order to avoid fragmentation and that 
    > participation in an AI regulatory sandbox established by a Member State, or by the 
    > European Data Protection Supervisor is mutually and uniformly recognised and 
    > carries the same legal effects across the Union;
    > (h) participation in the AI regulatory sandbox is limited to a period that is appropriate 
    > to the complexity and scale of the project, which may be extended by the national 
    > competent authority;
    > (i) AI regulatory sandboxes facilitate the development of tools and infrastructure for 
    > testing, benchmarking, assessing and explaining dimensions of AI systems 
    > relevant for regulatory learning, such as accuracy, robustness and cybersecurity, 
    > as well as measures to mitigate risks to fundamental rights and society at large.

- **Encourages and facilitates the drawing up of codes of conduct ← ai office**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 5

    > 4. The AI Office and the Member States shall take into account the specific interests and 
    > needs of SMEs, including start-ups, when encouraging and facilitating the drawing up of 
    > codes of conduct.

- **Provide guidance and advice to ← national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 9

    > 8. National competent authorities may provide guidance and advice on the implementation of 
    > this Regulation, in particular to SMEs including start-ups, taking into account the 
    > guidance and advice of the Board and the Commission, as appropriate. Whenever 
    > national competent authorities intend to provide guidance and advice with regard to an AI 
    > system in areas covered by other Union law, the competent national authorities under that 
    > Union law shall be consulted, as appropriate.



---

## Node: questionnaire template (automated tool)
<a name="node-questionnaire-template-automated-tool"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Develops ← ai office**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies



---

## Node: consistent practices across the union
<a name="node-consistent-practices-across-the-union"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Cooperate with other national competent authorities ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 5

    > 4. Where national competent authorities consider authorising testing in real world 
    > conditions supervised within the framework of an AI regulatory sandbox to be 
    > established under this Article, they shall specifically agree with the participants on the 
    > terms and conditions of such testing and in particular on the appropriate safeguards 
    > with a view to protecting fundamental rights, health and safety. Where appropriate, they 
    > shall cooperate with other national competent authorities with a view to ensuring 
    > consistent practices across the Union.



---

## Node: law-enforcement authorities or civil protection authorities
<a name="node-law-enforcement-authorities-or-civil-protection-authorities"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **putting into service ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 3

    > 2. In a duly justified situation of urgency for exceptional reasons of public security or in 
    > the case of specific, substantial and imminent threat to the life or physical safety of 
    > natural persons, law-enforcement authorities or civil protection authorities may put a 
    > specific high-risk AI system into service without the authorisation referred to in 
    > paragraph 1, provided that such authorisation is requested during or after the use 
    > without undue delay. If the authorisation referred to in paragraph 1 is refused, the use 
    > of the high-risk AI system shall be stopped with immediate effect and all the results and 
    > outputs of such use shall be immediately discarded.



---

## Node: relevant documents concerning assessment of qualifications
<a name="node-relevant-documents-concerning-assessment-of-qualifications"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Maintained for five years after subcontracting activity termination date ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 5

    > 4. The relevant documents concerning the assessment of the qualifications of the 
    > subcontractor or the subsidiary and the work carried out by them under this Regulation 
    > shall be kept at the disposal of the notifying authority for a period of five years from the 
    > termination date of the subcontracting activity.



---

## Node: personal data or business secrets
<a name="node-personal-data-or-business-secrets"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Protection ← legitimate interest of individuals or undertakings**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 6

    > 5. The rights of defence of the parties concerned shall be fully respected in the proceedings. 
    > They shall be entitled to have access to the European Data Protection Supervisor’s file, 
    > subject to the legitimate interest of individuals or undertakings in the protection of their 
    > personal data or business secrets.



---

## Node: sector in which it operates
<a name="node-sector-in-which-it-operates"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Takes due account of ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 3

    > 2. Notified bodies shall avoid unnecessary burdens for providers when performing their 
    > activities, and take due account of the size of the provider, the sector in which it 
    > operates, its structure and the degree of complexity of the high-risk AI system 
    > concerned, in particular in view of minimising administrative burdens and compliance 
    > costs for micro- and small enterprises within the meaning of Recommendation 
    > 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the 
    > level of protection required for the compliance of the high-risk AI system with the 
    > requirements of this Regulation. .



---

## Node: design choices
<a name="node-design-choices"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Concern ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **withdraws ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.



---

## Node: board in its reasoned request
<a name="node-board-in-its-reasoned-request"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Follows the specifications of ← ai office**
  - Chapter 6: Measures in support of innovation
  - Article 6: Measures for  providers and deployers, in particular smes, including start-ups
  - Paragraph 4

    > 3. The AI Office shall undertake the following actions:
    > (a) provide standardised templates for areas covered by this Regulation, as specified by 
    > the Board in its reasoned request;
    > (b) develop and maintain a single information platform providing easy to use 
    > information in relation to this Regulation for all operators across the Union;
    > (c) organise appropriate communication campaigns to raise awareness about the 
    > obligations arising from this Regulation;
    > (d) evaluate and promote the convergence of best practices in public procurement 
    > procedures in relation to AI systems.



---

## Node: lawyers duly authorised to act
<a name="node-lawyers-duly-authorised-to-act"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Legal representation through authorisation ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 6

    > 5. The provider of the general-purpose AI model concerned, or its representative shall 
    > supply the information requested. In the case of legal persons, companies or firms, or 
    > where the provider has no legal personality, the persons authorised to represent them by 
    > law or by their statutes, shall supply the information requested on behalf of the provider 
    > of the general-purpose AI model concerned. Lawyers duly authorised to act may supply 
    > information on behalf of their clients. The clients shall nevertheless remain fully 
    > responsible if the information supplied is incomplete, incorrect or misleading.



---

## Node: size of the provider’s organisation
<a name="node-size-of-the-provider-s-organisation"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **may support → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

### Incoming relationships

- **Proportionality ← providers**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.



---

## Node: publicly accessible space
<a name="node-publicly-accessible-space"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **located in → physical place**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 45

    > (44) ‘publicly accessible space’ means any publicly or privately owned physical place 
    > accessible to an undetermined number of natural persons, regardless of whether certain 
    > conditions for access may apply, and regardless of the potential capacity restrictions;

### Incoming relationships

- **Makes ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 11

    > 10. The advisory forum shall prepare an annual report on its activities. That report shall be 
    > made publicly available.



---

## Node: level of protection required
<a name="node-level-of-protection-required"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Compliance with this Regulation ← providers**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.



---

## Node: industry
<a name="node-industry"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **may support → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

### Incoming relationships

_(none)_



---

## Node: stakeholders
<a name="node-stakeholders"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Single contact point for stakeholders ← union or member states**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.



---

## Node: market surveillance governance and enforcement
<a name="node-market-surveillance-governance-and-enforcement"></a>

*29 outgoing, 14 incoming*

### Outgoing relationships

- **Part of → market surveillance governance and enforcement**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Exercises powers under → appropriate procedural safeguards**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.

- **Subject to → union and national law**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.

- **Subject to → effective judicial remedies and due process**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.

- **Takes → appropriate procedural safeguards**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 9

    > 9. The market surveillance authority shall take appropriate measures, as provided for in 
    > Article 19 of Regulation (EU) 2019/1020, within seven days from the date it received the 
    > notification referred to in paragraph 1 of this Article, and shall follow the notification 
    > procedures as provided in that Regulation.

- **Informs → operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 1

    > Article 75
    > Mutual assistance, market surveillance and control of general-purpose AI systems

- **Ensures that → testing in real-world conditions**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 2

    > 1. Market surveillance authorities shall have competences and powers to ensure that 
    > testing in real world conditions is in accordance with this Regulation.

- **Informs of decision/objection → authorised representatives of providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 5

    > 4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of 
    > this Article, or has issued an objection within the meaning of Article 60(4), point (b), the 
    > decision or the objection shall indicate the grounds therefor and how the provider or 
    > prospective provider can challenge the decision or objection.

- **Requires modification of testing in real world conditions → users**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 4

    > 3. Where a market surveillance authority has been informed by the prospective provider, 
    > the provider or any third party of a serious incident or has other grounds for considering 
    > that the conditions set out in Articles 60 and 61 are not met, it may take either of the 
    > following decisions on its territory, as appropriate:
    > (a) to suspend or terminate the testing in real world conditions;
    > (b) to require the provider or prospective provider and users to modify any aspect of 
    > the testing in real world conditions.

- **Notifies → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Insufficient documentation for infringement determination → union law protecting fundamental rights**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 4

    > 3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an 
    > infringement of obligations under Union law protecting fundamental rights has occurred, 
    > the public authority or body referred to in paragraph 1 may make a reasoned request to the 
    > market surveillance authority, to organise testing of the high-risk AI system through 
    > technical means. The market surveillance authority shall organise the testing with the close 
    > involvement of the requesting public authority or body within a reasonable time following 
    > the request.

- **Reasoned request for testing through technical means → public authorities in a third country**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 4

    > 3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an 
    > infringement of obligations under Union law protecting fundamental rights has occurred, 
    > the public authority or body referred to in paragraph 1 may make a reasoned request to the 
    > market surveillance authority, to organise testing of the high-risk AI system through 
    > technical means. The market surveillance authority shall organise the testing with the close 
    > involvement of the requesting public authority or body within a reasonable time following 
    > the request.

- **Takes appropriate restrictive measures against → types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 10

    > 9. The market surveillance authorities of the Member States shall ensure that appropriate 
    > restrictive measures are taken in respect of the product or the AI system concerned, such as 
    > withdrawal of the product or the AI system from their market, without undue delay.

- **informs accordingly → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 3

    > 2. Where the market surveillance authority of a Member State has sufficient reason to 
    > consider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall 
    > carry out an evaluation of the AI system concerned in respect of its compliance with all the 
    > requirements and obligations laid down in this Regulation. Particular attention shall be 
    > given to AI systems presenting a risk to groups of vulnerable persons referred to in 
    > Article 5. Where risks to fundamental rights of persons are identified, the market 
    > surveillance authority shall also inform and fully cooperate with the relevant national 
    > public authorities or bodies referred to in Article 77(1). The relevant operators shall 
    > cooperate as necessary with the market surveillance authority and with the other national 
    > public authorities or bodies referred to in Article 77(1).
    > Where, in the course of that evaluation, the market surveillance authority or, where 
    > applicable the market surveillance authority in cooperation with the national public 
    > authority referred to in Article 77(1), finds that the AI system does not comply with the 
    > requirements and obligations laid down in this Regulation, it shall without undue delay 
    > require the relevant operator to take all appropriate corrective actions to bring the AI 
    > system into compliance, to withdraw the AI system from the market, or to recall it within a 
    > period the market surveillance authority may prescribe, and in any event within the 
    > shorter of 15 working days, or as provided for in the relevant Union harmonisation 
    > legislation.
    > The market surveillance authority shall inform the relevant notified body accordingly. 
    > Article 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the 
    > second subparagraph of this paragraph.

- **Withdraws from national market → product**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Recalls from national market → providers of intermediary services**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **prescribes appropriate corrective action period → relevant provider**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 3

    > 2. Where, in the course of that evaluation, the market surveillance authority finds that the 
    > AI system concerned is high-risk, it shall without undue delay require the relevant 
    > provider to take all necessary actions to bring the AI system into compliance with the 
    > requirements and obligations laid down in this Regulation, as well as take appropriate 
    > corrective action within a period the market surveillance authority may prescribe.

- **Provides independent technical or scientific advice upon request → union ai testing support structures**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies

- **Submits reasoned complaints to → natural person**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.

- **report annually to → national competent authorities**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 3

    > 2. As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020, 
    > the market surveillance authorities shall report annually to the Commission and relevant 
    > national competition authorities any information identified in the course of market 
    > surveillance activities that may be of potential interest for the application of Union law on 
    > competition rules. They shall also annually report to the Commission about the use of 
    > prohibited practices that occurred during that year and about the measures taken.

- **inform accordingly → market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 3

    > 2. Where the relevant market surveillance authorities have sufficient reason to consider 
    > general-purpose AI systems that can be used directly by deployers for at least one 
    > purpose that is classified as high-risk pursuant to this Regulation to be non-compliant 
    > with the requirements laid down in this Regulation, they shall cooperate with the AI 
    > Office to carry out compliance evaluations, and shall inform the Board and other market 
    > surveillance authorities accordingly.

- **Full access granted to → documentation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 13

    > 12. Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and 
    > where relevant and limited to what is necessary to fulfil their tasks, the market 
    > surveillance authorities shall be granted full access by providers to the documentation as 
    > well as the training, validation and testing data sets used for the development of high-
    > risk AI systems, including, where appropriate and subject to security safeguards, 
    > through application programming interfaces (‘API’) or other relevant technical means 
    > and tools enabling remote access.

- **cooperate to carry out compliance evaluations → ai office**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 3

    > 2. Where the relevant market surveillance authorities have sufficient reason to consider 
    > general-purpose AI systems that can be used directly by deployers for at least one 
    > purpose that is classified as high-risk pursuant to this Regulation to be non-compliant 
    > with the requirements laid down in this Regulation, they shall cooperate with the AI 
    > Office to carry out compliance evaluations, and shall inform the Board and other market 
    > surveillance authorities accordingly.

- **Supervisory role → ai regulatory sandbox**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 3

    > 2. Where testing in real world conditions is conducted for AI systems that are supervised 
    > within an AI regulatory sandbox under Article 59, the market surveillance authorities 
    > shall verify the compliance with the provisions of Article 60 as part of their supervisory 
    > role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the 
    > testing in real world conditions to be conducted by the provider or prospective provider, 
    > in derogation from the conditions set out in Article 60(4), points (f) and (g).

- **unable to access certain information related to → ai model**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 4

    > 3. Where a national market surveillance authority is unable to conclude its investigation of 
    > the high-risk AI system because of its inability to access certain information related to 
    > the AI model despite having made all appropriate efforts to obtain that information, it 
    > may submit a reasoned request to the AI Office, by which access to that information 
    > shall be enforced. In that case, the AI Office shall supply to the applicant authority 
    > without delay, and in any event within 30 days, any information that the AI Office 
    > considers to be relevant in order to establish whether a high-risk AI system is non-
    > compliant. National market authorities shall safeguard the confidentiality of the 
    > information they obtain in accordance with Article 78 of this Regulation. The procedure 
    > provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis mutandis.

- **Taken into account for the purpose of conducting → reasoned complaints**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.

- **Grants for exceptional reasons → limited period authorization**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 2

    > 1. By way of derogation from Article 43 and upon a duly justified request, any market 
    > surveillance authority may authorise the placing on the market or the putting into service of 
    > specific high-risk AI systems within the territory of the Member State concerned, for 
    > exceptional reasons of public security or the protection of life and health of persons, 
    > environmental protection or the protection of key industrial and infrastructural assets. That 
    > authorisation shall be for a limited period  while the necessary conformity assessment 
    > procedures are being carried out, taking into account the exceptional reasons justifying 
    > the derogation. The completion of those procedures shall be undertaken without undue 
    > delay.

- **Informs → union or member states**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 5

    > 2. The market surveillance authority shall inform the Commission and the other Member 
    > States of any authorisation issued pursuant to paragraph 1. This obligation shall not cover 
    > sensitive operational data in relation to the activities of law-enforcement authorities.

- **Required to provide information → authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 7

    > 6. In accordance with Article 75, Member States shall confer on their market surveillance 
    > authorities the powers of requiring providers and prospective providers to provide 
    > information, of carrying out unannounced remote or on-site inspections, and of 
    > performing checks on the development of the testing in real world conditions and the 
    > related products. Market surveillance authorities shall use those powers to ensure the 
    > safe development of testing in real world conditions.

### Incoming relationships

- **Part of ← market surveillance governance and enforcement**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Informs ← commission**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 5

    > 2. The market surveillance authority shall inform the Commission and the other Member 
    > States of any authorisation issued pursuant to paragraph 1. This obligation shall not cover 
    > sensitive operational data in relation to the activities of law-enforcement authorities.

- **Informed about measures adopted ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 8

    > 7. The market surveillance authorities of the Member States other than the market 
    > surveillance authority of the Member State initiating the procedure shall, without undue 
    > delay, inform the Commission and the other Member States of any measures adopted and 
    > of any additional information at their disposal relating to the non-compliance of the AI 
    > system concerned, and, in the event of disagreement with the notified national measure, of 
    > their objections.

- **equivalent to ← notifying authority**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 49

    > (48) ‘national competent authority’ means a notifying authority or a market surveillance 
    > authority;;

- **supports cross-border market surveillance activities ← ai office**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 4

    > 3. The scientific panel shall advise and support the AI Office, in particular with regard to 
    > the following tasks:
    > (a) supporting the implementation and enforcement of this Regulation as regards 
    > general-purpose AI models and systems, in particular by:
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > 
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (b) supporting the work of market surveillance authorities, at their request;
    > (c) supporting cross-border market surveillance activities as referred to in 
    > Article 74(11), without prejudice to the powers of market surveillance authorities;
    > (d) supporting the AI Office in carrying out its duties in the context of the safeguard 
    > clause pursuant to Article 81.

- **Requires non-compliance to end ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **inform accordingly ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 3

    > 2. Where the relevant market surveillance authorities have sufficient reason to consider 
    > general-purpose AI systems that can be used directly by deployers for at least one 
    > purpose that is classified as high-risk pursuant to this Regulation to be non-compliant 
    > with the requirements laid down in this Regulation, they shall cooperate with the AI 
    > Office to carry out compliance evaluations, and shall inform the Board and other market 
    > surveillance authorities accordingly.

- **provided with contact details and documentation by authorised representative ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 4

    > 3. The authorised representative shall perform the tasks specified in the mandate received 
    > from the provider. It shall provide a copy of the mandate to the market surveillance 
    > authorities upon request, in one of the official languages of the institutions of the 
    > Union, as indicated by the national competent authority. For the purposes of this 
    > Regulation, the mandate shall empower the authorised representative to carry out the 
    > following tasks:
    > (a) verify that the EU declaration of conformity and the technical documentation 
    > referred to in Article 11 have been drawn up and that an appropriate conformity 
    > assessment procedure has been carried out by the provider;
    > (b) keep at the disposal of the national competent authorities and national authorities 
    > or bodies referred to in Article 74(10), for a period of 10 years after the high-risk 
    > AI system has been placed on the market or put into service, the contact details of 
    > the provider that appointed the authorised representative, a copy of the EU 
    > declaration of conformity, the technical documentation and, if applicable, the 
    > certificate issued by the notified body;
    > (c) provide a national competent authority, upon a reasoned request, with all the 
    > information and documentation, including that referred to in point (b) of this 
    > subparagraph, necessary to demonstrate the conformity of a high-risk AI system 
    > with the requirements set out in Section 2, including access to the logs, as referred to 
    > in Article 12(1), automatically generated by the high-risk AI system, to the extent 
    > such logs are under the control of the provider ;
    > (d) cooperate with competent  authorities, upon a reasoned request, in any action the 
    > latter take in relation to the high-risk AI system, in particular to reduce and mitigate 
    > the risks posed by the high-risk AI system;
    > (e) where applicable, comply with the registration obligations referred in Article 49(1), 
    > or, if the registration is carried out by the provider itself, ensure that the 
    > information referred to in Section A of Annex VIII is correct.
    > The mandate shall empower the authorised representative to be addressed, in addition to 
    > or instead of the provider, by the competent authorities, on all issues related to ensuring 
    > compliance with this Regulation.

- **Prohibits or restricts being made available on national market ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Authorises ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 4

    > 3. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance 
    > authority concludes that the high-risk AI system complies with the requirements of Section

- **Ensures recall or withdrawal from the market ← operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 3

    > 2. Where the non-compliance referred to in paragraph 1 persists, the market surveillance 
    > authority of the Member State concerned shall take appropriate and proportionate 
    > measures to restrict or prohibit the high-risk AI system being made available on the market 
    > or to ensure that it is recalled or withdrawn from the market without delay.

- **Communicate identity of ← union or member states**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 3

    > 2. Member States shall communicate to the Commission the identity of the notifying 
    > authorities and the market surveillance authorities and the tasks of those authorities, as 
    > well as any subsequent changes thereto. Member States shall make publicly available 
    > information on how competent authorities and single points of contact can be contacted, 
    > through electronic communication means by… [12 months from the date of entry into 
    > force of this Regulation]. Member States shall designate a market surveillance authority 
    > to act as the single point of contact for this Regulation, and shall notify the Commission 
    > of the identity of the single point of contact. The Commission shall make a list of the 
    > single points of contact publicly available.

- **make available to ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 7

    > 6. In the event of the restriction, suspension or withdrawal of a designation, the notifying 
    > authority shall take appropriate steps to ensure that the files of the notified body 
    > concerned are kept, and to make them available to notifying authorities in other Member 
    > States and to market surveillance authorities at their request.

- **Suspend Use and Inform ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 6

    > 5. Deployers shall monitor the operation of the high-risk AI system on the basis of the 
    > instructions for use and, where relevant, inform providers in accordance with Article 72. 
    > Where deployers have reason to consider that the use of the high-risk AI system in 
    > accordance with the instructions may present a risk within the meaning of Article 79(1), 
    > they shall, without undue delay, inform the provider or distributor and the relevant market 
    > surveillance authority, and shall suspend the use of that system. Where deployers have 
    > identified a serious incident, they shall also immediately inform first the provider, and 
    > then the importer or distributor and the relevant market surveillance authorities of that 
    > incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis 
    > mutandis. This obligation shall not cover sensitive operational data of deployers of AI 
    > systems which are law enforcement authorities.
    > For deployers that are financial institutions subject to requirements regarding their 
    > internal governance, arrangements or processes under Union financial services law, the 
    > monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by 
    > complying with the rules on internal governance arrangements, processes and mechanisms 
    > pursuant to the relevant financial service law.



---

## Node: commercial or competitive consultancy services
<a name="node-commercial-or-competitive-consultancy-services"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Offer nor provide ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 6

    > 5. Notifying authorities shall offer or provide neither any activities that conformity 
    > assessment bodies perform, nor any consultancy services on a commercial or competitive 
    > basis.



---

## Node: ai systems
<a name="node-ai-systems"></a>

*28 outgoing, 6 incoming*

### Outgoing relationships

- **Used in → operators of high-risk ai systems**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **have their place of establishment or are located → deployers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **place on the market → importers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **place on the market → distributors**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **exclusively for → military, defence or national security purposes**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 5

    > 3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in 
    > any event, affect the competences of the Member States concerning national security, 
    > regardless of the type of entity entrusted by the Member States with carrying out tasks in 
    > relation to those competences.
    > This Regulation does not apply to AI systems where and in so far they are placed on the 
    > market, put into service, or used with or without modification exclusively for military, 
    > defence or national security purposes, regardless of the type of entity carrying out those 
    > activities.
    > This Regulation does not apply to AI systems which are not placed on the market or put 
    > into service in the Union, where the output is used in the Union exclusively for military, 
    > defence or national security purposes, regardless of the type of entity carrying out those 
    > activities.

- **Use → third country**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 6

    > 4. This Regulation applies neither to public authorities in a third country nor to international 
    > organisations falling within the scope of this Regulation pursuant to paragraph 1, where 
    > those authorities or organisations use AI systems in the framework of international 
    > cooperation or agreements for law enforcement and judicial cooperation with the Union or 
    > with one or more Member States, provided that such a third country or international 
    > organisation provides adequate safeguards with respect to the protection of fundamental 
    > rights and freedoms of individuals.

- **Applicability exemption → public authorities in a third country**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 6

    > 4. This Regulation applies neither to public authorities in a third country nor to international 
    > organisations falling within the scope of this Regulation pursuant to paragraph 1, where 
    > those authorities or organisations use AI systems in the framework of international 
    > cooperation or agreements for law enforcement and judicial cooperation with the Union or 
    > with one or more Member States, provided that such a third country or international 
    > organisation provides adequate safeguards with respect to the protection of fundamental 
    > rights and freedoms of individuals.

- **Tested in → testing in real-world conditions**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 58

    > (57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its 
    > intended purpose in real-world conditions outside a laboratory or otherwise simulated 
    > environment, with a view to gathering reliable and robust data and to assessing and 
    > verifying the conformity of the AI system with the requirements of this Regulation and it 
    > is not considered to be placing the AI system on the market or putting it into service 
    > within the meaning of this Regulation, provided that all the conditions laid down in 
    > Article 57 or 60 are fulfilled;

- **influences → physical or virtual environments**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 2

    > (1) ‘AI system’ means a machine-based system designed to operate with varying levels of 
    > autonomy, that may exhibit adaptiveness after deployment and that, for explicit or 
    > implicit objectives, infers, from the input it receives, how to generate outputs such as 
    > predictions, content, recommendations, or decisions that can influence physical or virtual 
    > environments;

- **For → intended purpose**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 58

    > (57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its 
    > intended purpose in real-world conditions outside a laboratory or otherwise simulated 
    > environment, with a view to gathering reliable and robust data and to assessing and 
    > verifying the conformity of the AI system with the requirements of this Regulation and it 
    > is not considered to be placing the AI system on the market or putting it into service 
    > within the meaning of this Regulation, provided that all the conditions laid down in 
    > Article 57 or 60 are fulfilled;

- **Recall initiated by → providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 17

    > (16) ‘recall of an AI system’ means any measure aiming to achieve the return to the provider or 
    > taking out of service or disabling the use of an AI system made available to deployers;

- **Used for providing an independent evaluation → personal data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 33

    > (32) ‘testing data’ means data used for providing an independent evaluation of the  AI system 
    > in order to confirm the expected performance of that system before its placing on the 
    > market or putting into service;

- **Used for → validation data set**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 31

    > (30) ‘validation data’ means data used for providing an evaluation of the trained AI system and 
    > for tuning its non-learnable parameters and its learning process in order, inter alia, to 
    > prevent underfitting or overfitting;

- **Provides or directly acquires → biometric data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 34

    > (33) ‘input data’ means data provided to or directly acquired by an AI system on the basis of 
    > which the system produces an output;

- **Influences → commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Impacts → fundamental rights**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Affects → health and safety**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **exploits → manipulative or deceptive techniques**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.

- **Processes → special categories of personal data**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 3

    > 2. When assessing the condition under paragraph 1, point (b),, the Commission shall take into 
    > account the following criteria:
    > (a) the intended purpose of the AI system;
    > (b) the extent to which an AI system has been used or is likely to be used;
    > (c) the nature and amount of the data processed and used by the AI system, in 
    > particular whether special categories of personal data are processed;
    > (d) the extent to which the AI system acts autonomously and the possibility for a 
    > human to override a decision or recommendations that may lead to potential harm;
    > (e) the extent to which the use of an AI system has already caused harm to  health and 
    > safety, has had an adverse impact on  fundamental rights or has given rise to 
    > significant concerns in relation to the likelihood of such harm or adverse impact, as 
    > demonstrated, for example, by reports or documented allegations submitted to 
    > national competent authorities or by other reports, as appropriate;
    > (f) the potential extent of such harm or such adverse impact, in particular in terms of its 
    > intensity and its ability to affect multiple persons or to disproportionately affect a 
    > particular group of persons;
    > (g) the extent to which persons who are potentially harmed or suffer an adverse impact 
    > are dependent on the outcome produced with an AI system, in particular because for 
    > practical or legal reasons it is not reasonably possible to opt-out from that outcome;
    > (h) the extent to which there is an imbalance of power, or the persons who are 
    > potentially harmed or suffer an adverse impact are in a vulnerable position in relation 
    > to the deployer of an AI system, in particular due to status, authority, knowledge, 
    > economic or social circumstances, or age;
    > (i) the extent to which the outcome produced involving an AI system is easily corrigible 
    > or reversible, taking into account the technical solutions available to correct or 
    > reverse it, whereby outcomes having an adverse impact on  health, safety or 
    > fundamental rights, shall not be considered to be easily corrigible or reversible;
    > (j) the magnitude and likelihood of benefit of the deployment of the AI system for 
    > individuals, groups, or society at large, including possible improvements in product 
    > safety;
    > (k) the extent to which existing Union law provides for:
    > (i) effective measures of redress in relation to the risks posed by an AI system, 
    > with the exclusion of claims for damages;
    > (ii) effective measures to prevent or substantially minimise those risks.

- **Registered in → logs referred to in article 12(1)**
  - Chapter 3: High-risk ai systems
  - Article 44: Registration
  - Paragraph 3

    > 2. Before placing on the market or putting into service an AI system for which the provider 
    > has concluded that it is not high-risk according to Article 6(3), that provider or, where 
    > applicable, the authorised representative shall register themselves and that system in the 
    > EU database referred to in Article 71.

- **Testing in real world conditions → subject**
  - Chapter 6: Measures in support of innovation
  - Article 5: Informed consent to participate in testing in real world conditions
  - Paragraph 2

    > 1. For the purpose of testing in real world conditions under Article 60, freely-given 
    > informed consent shall obtained from the subjects of testing prior to their participation 
    > in such testing and after their having been duly informed with concise, clear, relevant, 
    > and understandable information regarding:
    > (a) the nature and objectives of the testing in real world conditions and the possible 
    > inconvenience that may be linked to their participation;
    > (b) the conditions under which the testing in real world conditions is to be conducted, 
    > including the expected duration of the subject or subjects' participation;
    > (c) their rights, and the guarantees regarding their participation, in particular their 
    > right to refuse to participate in, and the right to withdraw from, testing in real 
    > world conditions at any time without any resulting detriment and without having to 
    > provide any justification;
    > (d) the arrangements for requesting the reversal or the disregard of the predictions, 
    > recommendations or decisions of the AI system;
    > (e) the Union-wide unique single identification number of the testing in real world 
    > conditions in accordance with Article 60(4) point (c), and the contact details of the 
    > provider or its legal representative from whom further information can be 
    > obtained.

- **Causal link between → serious incident**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 3

    > 2. The report referred to in paragraph 1 shall be made immediately after the provider has 
    > established a causal link between the AI system and the serious incident or  the 
    > reasonable likelihood of such a link, and, in any event, not later than 15 days after the 
    > provider or, where applicable, the deployer, becomes aware of the serious incident.
    > The period for the reporting referred to in the first subparagraph shall take account of 
    > the severity of the serious incident  .

- **Developed by the same provider → general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 2

    > 1. Where an AI system is based on a general-purpose AI model, and the model and the 
    > system are developed by the same provider, the AI Office shall have powers to monitor 
    > and supervise compliance of that AI system with obligations under this Regulation. To 
    > carry out its monitoring and supervision tasks, the AI Office shall have all the powers of 
    > a market surveillance authority within the meaning of Regulation (EU) 2019/1020.

- **Prohibits or restricts being made available on national market → market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Misclassification → operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 2

    > 1. Where a market surveillance authority has sufficient reason to consider that an AI 
    > system classified by the provider is not high-risk pursuant to Article 6(3)I is indeed high-
    > risk, the market surveillance authority shall carry out an evaluation of the AI system 
    > concerned in respect of its classification as a high-risk AI system based on the conditions 
    > set out in Article 6(3) and the Commission guidelines.

- **Informs → commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 4

    > 3. Where the market surveillance authority considers that the use of the AI system 
    > concerned is not restricted to its national territory, it shall inform the Commission and 
    > the other Member States without undue delay of the results of the evaluation and of the 
    > actions which it has required the provider to take.

- **Informs → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 4

    > 3. Where the market surveillance authority considers that the use of the AI system 
    > concerned is not restricted to its national territory, it shall inform the Commission and 
    > the other Member States without undue delay of the results of the evaluation and of the 
    > actions which it has required the provider to take.

- **Taken in respect of → appropriate corrective action**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 6

    > 5. The provider shall ensure that all appropriate corrective action is taken in respect of all 
    > the AI systems concerned that it has made available on the Union market.

### Incoming relationships

- **Enable good understanding of capabilities and limitations ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 2

    > 1. Providers of general-purpose AI models shall:
    > (a) draw up and keep up-to-date the technical documentation of the model, including 
    > its training and testing process and the results of its evaluation, which shall 
    > contain, at a minimum, the elements set out in Annex XI for the purpose of 
    > providing it, upon request, to the AI Office and the national competent authorities;
    > (b) draw up, keep up-to-date and make available information and documentation to 
    > providers of AI systems who intend to integrate the general-purpose AI model into 
    > their AI systems. Without prejudice to the need to respect and protect intellectual 
    > property rights and confidential business information or trade secrets in 
    > accordance with Union and national law, the information and documentation 
    > shall:
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > 
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (c) put in place a policy to comply with Union copyright law, and in particular to 
    > identify and comply with, including through state of the art technologies, a 
    > reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;
    > (d) draw up and make publicly available a sufficiently detailed summary about the 
    > content used for training of the general-purpose AI model, according to a template 
    > provided by the AI Office.

- **uses ← deployers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 5

    > (4) ‘deployer’ means a natural or legal person, public authority, agency or other body using an 
    > AI system under its authority  except where the AI system is used in the course of a 
    > personal non-professional activity;

- **Makes available on the market ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 6

    > 5. The provider shall ensure that all appropriate corrective action is taken in respect of all 
    > the AI systems concerned that it has made available on the Union market.

- **Voluntary application ← codes of conduct**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **On the basis of ← specific requirements**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Concerning ← clear objectives**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.



---

## Node: regulation (eu) no 1025/2012
<a name="node-regulation-eu-no-1025-2012"></a>

*4 outgoing, 3 incoming*

### Outgoing relationships

- **defines → harmonised standard**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 28

    > (27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of 
    > Regulation (EU) No 1025/2012;

- **defined in Article → personal data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 52

    > (51) ‘non-personal data’ means data other than personal data as defined in Article 4, point 
    > (1), of Regulation (EU) 2016/679;

- **Is a type of → regulation (eu) no 1025/2012**
  - Chapter 13: Final provisions 
  - Article 7: Amendment to regulation (eu) 2018/1139
  - Paragraph 1

    > Article 108
    > Amendment to Regulation (EU) 2018/1139
    > Regulation (EU) 2018/1139 is amended as follows:

- **Within the meaning of, and in accordance with → national accreditation body**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 3

    > 2. Member States may decide that the assessment and monitoring referred to in 
    > paragraph 1 shall be carried out by a national accreditation body within the meaning of, 
    > and in accordance with, Regulation (EC) No 765/2008  .

### Incoming relationships

- **Defined in ← personal data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 51

    > (50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation 
    > (EU) 2016/679;

- **Is a type of ← regulation (eu) no 1025/2012**
  - Chapter 13: Final provisions 
  - Article 7: Amendment to regulation (eu) 2018/1139
  - Paragraph 1

    > Article 108
    > Amendment to Regulation (EU) 2018/1139
    > Regulation (EU) 2018/1139 is amended as follows:

- **Takes into account ← artificial intelligence act**
  - Chapter 13: Final provisions 
  - Article 6: Amendment to regulation (eu) 2018/858
  - Paragraph 2

    > 2 of that Regulation shall be taken into account.
    > ________________
    > * Regulation (EU) 2024/… of the European Parliament and of the Council of … laying down 
    > harmonised rules on artificial intelligence (Artificial intelligence act) and amending certain 
    > Union legislative acts (OJ L, …, ELI: …).’
    > + OJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete 
    > the corresponding footnote.



---

## Node: risks that cannot be eliminated
<a name="node-risks-that-cannot-be-eliminated"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **address ← mitigation and control measures**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.



---

## Node: committee procedure
<a name="node-committee-procedure"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Involves → real-world testing plan**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 8

    > 7. Testing procedures may include testing in real-world conditions in accordance with 
    > Article 60.

- **Promotes and applies → liability**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.

### Incoming relationships

- **Involves ← article 6(1)**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 2: Committee procedure
  - Paragraph 1

    > Article 98
    > Committee procedure

- **Documents and implements ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.



---

## Node: instructions for use
<a name="node-instructions-for-use"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **required for conformity with this Regulation ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).



---

## Node: commission
<a name="node-commission"></a>

*74 outgoing, 7 incoming*

### Outgoing relationships

- **Oversees → implementation of regulation**
  - Chapter 10: Codes of conduct and guidelines
  - Article 2: Guidelines from the commission on the implementation of this regulation
  - Paragraph 1

    > Article 96
    > Guidelines from the Commission on the implementation of this Regulation

- **Requests guidelines updates → member states or ai office**
  - Chapter 10: Codes of conduct and guidelines
  - Article 2: Guidelines from the commission on the implementation of this regulation
  - Paragraph 3

    > 2. Upon request of the Member States or the AI Office, or on its own initiative, the 
    > Commission shall update guidelines previously adopted when deemed necessary.

- **Assisted by → commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 2: Committee procedure
  - Paragraph 2

    > 1. The Commission shall be assisted by a committee. That committee shall be a committee 
    > within the meaning of Regulation (EU) No 182/2011.

- **Has power to adopt → delegated act**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 2

    > 1. The power to adopt delegated acts is conferred on the Commission subject to the 
    > conditions laid down in this Article.

- **Notifies simultaneously → european parliament or council**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 6

    > 5. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the 
    > European Parliament and to the Council.

- **Delegation of power for → five-year period**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 3

    > 2. The power to adopt delegated acts referred to in Article 6(6), Article 7(1) and (3), Article 
    > 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) 
    > and (6) shall be conferred on the Commission for a period of five years from … [date of 
    > entry into force of this Regulation]. The Commission shall draw up a report in respect of 
    > the delegation of power not later than nine months before the end of the five-year period. 
    > The delegation of power shall be tacitly extended for periods of an identical duration, 
    > unless the European Parliament or the Council opposes such extension not later than 
    > three months before the end of each period.

- **Consults → experts designated by each member state**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 5

    > 4. Before adopting a delegated act, the Commission shall consult experts designated by each 
    > Member State in accordance with the principles laid down in the Interinstitutional 
    > Agreement of 13 April 2016 on Better Law-Making.

- **Takes into account positions and findings → council**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.

- **Pursuant to Article 96. → guidelines issued by**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.

- **Is informed about → related litigation or judicial proceedings**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 13

    > 11. Member States shall, on an annual basis, report to the Commission about the 
    > administrative fines they have issued during that year, in accordance with this Article, 
    > and about any related litigation or judicial proceedings.

- **Notifies annually → european data protection supervisor**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 8

    > 7. The European Data Protection Supervisor shall, on an annual basis, notify the 
    > Commission of the administrative fines it has imposed pursuant to this Article and of 
    > any litigation or judicial proceedings it has initiated.

- **Imposes fines on → general-purpose ai models**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 2

    > 1. The Commission may impose on providers of general purpose AI models fines not 
    > exceeding 3 % of their total worldwide turnover in the preceding financial year or 15 
    > million EUR, whichever is higher., when the Commission finds that the provider 
    > intentionally or negligently:
    > (a) infringed the relevant provisions of this Regulation;
    > (b) failed to comply with a request for a document or for information pursuant to 
    > Article 91, or supplied incorrect, incomplete or misleading information;
    > (c) failed to comply with a measure requested under Article 93;
    > (d) failed to make available to the Commission access to the general-purpose AI model 
    > or general-purpose AI model with systemic risk with a view to conducting an 
    > evaluation pursuant to Article 92.
    > In fixing the amount of the fine or periodic penalty payment, regard shall be had to the 
    > nature, gravity and duration of the infringement, taking due account of the principles of 
    > proportionality and appropriateness. The Commission shall also into account 
    > commitments made in accordance with Article 93(3) or made in relevant codes of 
    > practice in accordance with Article 56.

- **Takes into account positions and findings → european parliament or council**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.

- **Takes into account positions and findings → other relevant bodies or sources**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.

- **Submits → appropriate procedural safeguards**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Shall → obligations laid down in this regulation**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **If necessary → regulation**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Need for establishment → union agency**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Identification of → shortcomings**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Assessment of → informed consent**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **Consults → artificial intelligence act**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 6

    > 5. The Commission shall, after consulting the European Artificial Intelligence Board (the 
    > ‘Board’), and no later than … [18 months from the date of entry into force of this 
    > Regulation], provide guidelines specifying the practical implementation of this Article in 
    > line with Article 96 together with a comprehensive list of practical examples of use cases 
    > of AI systems that are high-risk and not high-risk.

- **Adds or modifies → new conditions or modified conditions**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.

- **Takes account of → obligations pursuant to article 16**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.

- **Considers or takes into account → market and technological developments**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.

- **Updates through delegated acts → annex iii**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 6

    > 5. The Commission shall adopt delegated acts in accordance with Article 97 to update 
    > Annexes VI and VII in  light of technical progress.

- **Decreases overall level of protection → other union law**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > list in Annex III by removing high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the high-risk AI system concerned no longer poses any significant risks to 
    > fundamental rights, health or safety, taking into account the criteria listed in 
    > paragraph 2;
    > (b) the deletion does not decrease the overall level of protection of health, safety and 
    > fundamental rights under Union law.
    > Section 2
    > Requirements for high-risk AI systems

- **Encourages the development of → benchmarks and measurement methodologies**
  - Chapter 3: High-risk ai systems
  - Article 10: Accuracy, robustness and cybersecurity
  - Paragraph 3

    > 2. To address the technical aspects of how to measure the appropriate levels of accuracy 
    > and robustness set out in paragraph 1 and any other relevant performance metrics, the 
    > Commission shall, in cooperation with relevant stakeholder and organisations such as 
    > metrology and benchmarking authorities, encourage, as appropriate, the development of 
    > benchmarks and measurement methodologies.

- **Exchange of information → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.

- **Reports financial and human resources → union or member states**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 7

    > 6. By …, [one year from the date of entry into force of this Regulation] and once every two 
    > years thereafter, Member States shall report to the Commission  on the status of the 
    > financial and human resources of the national competent authorities, with an assessment of 
    > their adequacy. The Commission shall transmit that information to the Board for 
    > discussion and possible recommendations.

- **Consultations initiated by → conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 6

    > 5. Where objections are raised, the Commission shall, without delay, enter into 
    > consultations with the relevant Member States and the conformity assessment body. 
    > Having regard thereto, the Commission shall decide whether the authorisation is 
    > justified. The Commission shall address its decision to the Member State concerned and 
    > the relevant conformity assessment body.

- **Obtains → provision of information**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Ensures → confidentiality**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Complies with → article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Provides exchange for knowledge and best practices → union or member states**
  - Chapter 3: High-risk ai systems
  - Article 33: Coordination of notified bodies
  - Paragraph 4

    > 3. The Commission shall provide for the exchange of knowledge and best practices between 
    > the notifying authorities of the Member States.

- **Assesses harmonised standard → standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 5

    > 4. Where a harmonised standard is adopted by a European standardisation organisation 
    > and proposed to the Commission for the publication of its reference in the Official 
    > Journal of the European Union, the Commission shall assess the harmonised standard 
    > in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised 
    > standard is published in the Official Journal of the European Union, the Commission 
    > shall repeal the implementing acts referred to in paragraph 1, or parts thereof which 
    > cover the same requirements set out in Section 2 of this Chapter.

- **Transmits information to → board**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 7

    > 6. By …, [one year from the date of entry into force of this Regulation] and once every two 
    > years thereafter, Member States shall report to the Commission  on the status of the 
    > financial and human resources of the national competent authorities, with an assessment of 
    > their adequacy. The Commission shall transmit that information to the Board for 
    > discussion and possible recommendations.

- **Requests deliverables on → high-impact capabilities of general-purpose ai models**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 3

    > 2. The Commission shall issue standardisation requests covering all requirements set out 
    > in Section 2 of this Chapter and, as applicable, obligations set out in Chapter IV of this 
    > Regulation, in accordance with Article 10 of Regulation (EU) No 1025/2012, without 
    > undue delay. The standardisation request shall also ask for deliverables on reporting 
    > and documentation processes to improve AI systems’ resource performance, such as 
    > reducing the high-risk AI system’s consumption of energy and other resources 
    > consumption during its lifecycle, and on the energy-efficient development of general-
    > purpose AI models. When preparing a standardisation request, the Commission shall 
    > consult the Board and relevant stakeholders, including the advisory forum.
    > When issuing a standardisation request to European standardisation organisations, the 
    > Commission shall specify that standards have to be clear, consistent, including with the 
    > standards developed in the various sectors for products covered by the existing Union 
    > harmonisation legislation listed in Annex I, and aiming to ensure that AI systems or AI 
    > models placed on the market or put into service in the Union meet the relevant 
    > requirements laid down in this Regulation.
    > The Commission shall request the European standardisation organisations to provide 
    > evidence of their best efforts to fulfil the objectives referred to in the first and the second 
    > subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 
    > 1025/2012.

- **Refers to in Official Journal of the European Union → harmonised standard**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 5

    > 4. Where a harmonised standard is adopted by a European standardisation organisation 
    > and proposed to the Commission for the publication of its reference in the Official 
    > Journal of the European Union, the Commission shall assess the harmonised standard 
    > in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised 
    > standard is published in the Official Journal of the European Union, the Commission 
    > shall repeal the implementing acts referred to in paragraph 1, or parts thereof which 
    > cover the same requirements set out in Section 2 of this Chapter.

- **Informs → market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 5

    > 2. The market surveillance authority shall inform the Commission and the other Member 
    > States of any authorisation issued pursuant to paragraph 1. This obligation shall not cover 
    > sensitive operational data in relation to the activities of law-enforcement authorities.

- **Consults and provides opportunity for presentation of views → types of ai systems concerned**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 7

    > 5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, 
    > objections are raised by a Member State against an authorisation issued by a market 
    > surveillance authority of another Member State, or where the Commission considers the 
    > authorisation to be contrary to Union law, or the conclusion of the Member States 
    > regarding the compliance of the system as referred to in paragraph 3 to be unfounded, the 
    > Commission shall, without delay, enter into consultations with the relevant Member State. 
    > The operators concerned shall be consulted and have the possibility to present their views. 
    > Having regard thereto, the Commission shall decide whether the authorisation is justified. 
    > The Commission shall address its decision to the Member State concerned and to the 
    > relevant operators.

- **Introduces in delegated acts → elements necessary for technical progress**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 6

    > 5. The Commission shall adopt delegated acts in accordance with Article 97 for the purpose 
    > of updating the content of the EU declaration of conformity set out in Annex V, in order to 
    > introduce elements that become necessary in light of technical progress.

- **Performs → delegated act**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.

- **Supports → key performance indicators**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.

- **Cooperates with → general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 4

    > 3. Providers of general-purpose AI models shall cooperate as necessary with the 
    > Commission and the national competent authorities in the exercise of their competences 
    > and powers pursuant to this Regulation.

- **Upon a reasoned request → providers**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **Pursuant to paragraph 4 → systemic risk**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **Facilitates compliance with Annex XI (d) and (e) → comparable and verifiable documentation**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 6

    > 5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and
    > (e) thereof, the Commission shall adopt delegated acts in accordance with Article 97 to 
    > detail measurement and calculation methodologies with a view to allowing for 
    > comparable and verifiable documentation.

- **Amends → chapters i and ii**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97(2) to amend 
    > Annexes XI and XII in the light of evolving technological developments.

- **Provides → common rules for implementation**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 10

    > 9. Codes of practice shall be ready at the latest by … [nine months from the date of entry into 
    > force of this Regulation]. The AI Office shall take the necessary steps, including inviting 
    > providers pursuant to paragraph 7. 
    > If, by ... [12 months from the date of entry into force], a code of practice cannot be 
    > finalised, or if the AI Office deems it is not adequate following its assessment under 
    > paragraph 6 of this Article, the Commission may provide, by means of implementing 
    > acts, common rules for the implementation of the obligations provided for in Articles 53 
    > and 55, including the issues set out in paragraph 2 of this Article. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).

- **Takes into account → financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Considers → microenterprises**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 2

    > 1. Microenterprises within the meaning of Recommendation 2003/361/EC, may comply 
    > with certain elements of the quality management system required by Article 17 of this 
    > Regulation in a simplified manner, provided that they do not have partner enterprises or 
    > linked enterprises within the meaning of that Recommendation. For that purpose, the 
    > Commission shall develop guidelines on the elements of the quality management system 
    > which may be complied with in a simplified manner considering the needs of 
    > microenterprises, without affecting the level of protection or the need for compliance 
    > with the requirements in respect of high-risk AI systems.

- **Supervises and enforces Chapter V → ai office**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 17: Enforcement of the obligations of providers of general-purpose ai models
  - Paragraph 2

    > 1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking 
    > into account the procedural guarantees under Article 94. The Commission shall entrust 
    > the implementation of these tasks to the AI Office, without prejudice to the powers of 
    > organisation of the Commission and the division of competences between Member States 
    > and the Union based on the Treaties.

- **Appoints → advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 4

    > 3. The Commission shall appoint the members of the advisory forum, in accordance with 
    > the criteria set out in paragraph 2, from amongst stakeholders with recognised expertise 
    > in the field of AI.

- **Establishes → certificate**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **Selects → expert**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **Requires → up-to-date scientific or technical expertise**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **Ensures → fair gender and geographical representation**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **Facilitates exchange of experience → national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 8

    > 7. The Commission shall facilitate the exchange of experience between national competent 
    > authorities.

- **Makes available to → ['providers', 'prospective providers', 'deployers']**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 7

    > 6. The Commission shall be the controller of the EU database. It shall make available to 
    > providers, prospective providers and deployers adequate technical and administrative 
    > support. The EU database shall comply with the applicable accessibility requirements.

- **Develops → guidance**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 8

    > 8. Upon receiving a notification related to a serious incident referred to in Article 3, point 
    > (44)(c), the relevant market surveillance authority shall inform the national public 
    > authorities or bodies referred to in Article 77(1). The Commission shall develop dedicated 
    > guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. 
    > That guidance shall be issued by … [12 months after the entry into force of this 
    > Regulation], and shall be assessed regularly.

- **confidential basis → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 5

    > 3. Without prejudice to paragraphs 1 and 2, information exchanged on a confidential basis 
    > between the national competent authorities or between national competent authorities and 
    > the Commission shall not be disclosed without prior consultation of the originating 
    > national competent authority and the deployer when high-risk AI systems referred to in 
    > point 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or 
    > asylum authorities and when such disclosure would jeopardise public and national security 
    > interests. This exchange of information shall not cover sensitive operational data in 
    > relation to the activities of law enforcement, border control, immigration or asylum 
    > authorities.
    > When the law enforcement, immigration or asylum authorities are providers of high-risk 
    > AI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred 
    > to in Annex IV shall remain within the premises of those authorities. Those authorities 
    > shall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as 
    > applicable, can, upon request, immediately access the documentation or obtain a copy 
    > thereof. Only staff of the market surveillance authority holding the appropriate level of 
    > security clearance shall be allowed to access that documentation or any copy thereof.

- **Informed about measures adopted → market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 8

    > 7. The market surveillance authorities of the Member States other than the market 
    > surveillance authority of the Member State initiating the procedure shall, without undue 
    > delay, inform the Commission and the other Member States of any measures adopted and 
    > of any additional information at their disposal relating to the non-compliance of the AI 
    > system concerned, and, in the event of disagreement with the notified national measure, of 
    > their objections.

- **Dissemination of warnings → member states and their relevant authorities**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.

- **Exchange confidential information with → public authorities in a third country**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 7

    > 5. The Commission and Member States may exchange, where necessary and in accordance 
    > with relevant provisions of international and trade agreements, confidential information 
    > with regulatory authorities of third countries with which they have concluded bilateral or 
    > multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.

- **Communicates decision to → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 6

    > 5. The Commission shall immediately communicate its decision to the Member States 
    > concerned and to the relevant operators. It shall also inform the other Member States.

- **Consults with → relevant provider**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 5

    > 4. The Commission shall without undue delay enter into consultation with the Member State 
    > or member States concerned and the relevant operators, and shall evaluate the national 
    > measures taken. On the basis of the results of that evaluation, the Commission shall decide 
    > whether the measure is justified and, where necessary, propose other appropriate measures.

- **Evaluates → national level registration**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 5

    > 4. The Commission shall without undue delay enter into consultation with the Member State 
    > or member States concerned and the relevant operators, and shall evaluate the national 
    > measures taken. On the basis of the results of that evaluation, the Commission shall decide 
    > whether the measure is justified and, where necessary, propose other appropriate measures.

- **Seeks independent technical or scientific advice → union ai testing support structures**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies

- **Requests information → general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 4

    > 3. Upon a duly substantiated request from the scientific panel, the Commission may issue a 
    > request for information to a provider of a general-purpose AI model, where the access to 
    > information is necessary and proportionate for the fulfilment of the tasks of the 
    > scientific panel under Article 68(2).

- **Appoints → independent experts appointed for this task**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 3

    > 2. The Commission may decide to appoint independent experts to carry out evaluations on 
    > its behalf, including from the scientific panel established pursuant to Article 68. 
    > Independent experts appointed for this task shall meet the criteria outlined in Article 
    > 68(2).

- **Involves → independent experts**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 7

    > 6. The Commission shall adopt implementing acts setting out the detailed arrangements 
    > and the conditions of the evaluations, including the detailed arrangements for involving 
    > independent experts, and the procedure for the selection thereof. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).

- **Sets out → detailed arrangements and conditions of evaluations**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 7

    > 6. The Commission shall adopt implementing acts setting out the detailed arrangements 
    > and the conditions of the evaluations, including the detailed arrangements for involving 
    > independent experts, and the procedure for the selection thereof. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).

- **Require → automation bias**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

- **Restrict making available on the market, withdraw or recall → ai model**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

### Incoming relationships

- **Designated as ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **Influences ← ai systems**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Assisted by ← commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 2: Committee procedure
  - Paragraph 2

    > 1. The Commission shall be assisted by a committee. That committee shall be a committee 
    > within the meaning of Regulation (EU) No 182/2011.

- **Carried out by ← ai office**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 48

    > (47) ‘AI Office’ means the Commission’s function of contributing to the implementation, 
    > monitoring and supervision of AI systems and AI governance carried out by the 
    > European Artificial Intelligence Office established by Commission Decision of 
    > 24.1.2024; references in this Regulation to the AI Office shall be construed as references 
    > to the Commission;

- **Request ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

- **Receives annual reports ← union or member states**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 13

    > 11. Member States shall, on an annual basis, report to the Commission about the 
    > administrative fines they have issued during that year, in accordance with this Article, 
    > and about any related litigation or judicial proceedings.

- **Informs ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 4

    > 3. Where the market surveillance authority considers that the use of the AI system 
    > concerned is not restricted to its national territory, it shall inform the Commission and 
    > the other Member States without undue delay of the results of the evaluation and of the 
    > actions which it has required the provider to take.



---

## Node: appropriate type and degree of transparency
<a name="node-appropriate-type-and-degree-of-transparency"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Ensures → compliance with those requirements**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.

### Incoming relationships

_(none)_



---

## Node: five-year period
<a name="node-five-year-period"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Delegation of power for ← commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 3

    > 2. The power to adopt delegated acts referred to in Article 6(6), Article 7(1) and (3), Article 
    > 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) 
    > and (6) shall be conferred on the Commission for a period of five years from … [date of 
    > entry into force of this Regulation]. The Commission shall draw up a report in respect of 
    > the delegation of power not later than nine months before the end of the five-year period. 
    > The delegation of power shall be tacitly extended for periods of an identical duration, 
    > unless the European Parliament or the Council opposes such extension not later than 
    > three months before the end of each period.

- **ceased activities ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.



---

## Node: health and safety
<a name="node-health-and-safety"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Affects ← ai systems**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Pose a risk of harm to ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.



---

## Node: common rules for implementation
<a name="node-common-rules-for-implementation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Provides ← commission**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 10

    > 9. Codes of practice shall be ready at the latest by … [nine months from the date of entry into 
    > force of this Regulation]. The AI Office shall take the necessary steps, including inviting 
    > providers pursuant to paragraph 7. 
    > If, by ... [12 months from the date of entry into force], a code of practice cannot be 
    > finalised, or if the AI Office deems it is not adequate following its assessment under 
    > paragraph 6 of this Article, the Commission may provide, by means of implementing 
    > acts, common rules for the implementation of the obligations provided for in Articles 53 
    > and 55, including the issues set out in paragraph 2 of this Article. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).



---

## Node: sandbox plan
<a name="node-sandbox-plan"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **agrees on ← notifying authority**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 55

    > (54) ‘sandbox plan’ means a document agreed between the participating provider and the 
    > competent authority describing the objectives, conditions, timeframe, methodology and 
    > requirements for the activities carried out within the sandbox;



---

## Node: legitimate interest of individuals or undertakings
<a name="node-legitimate-interest-of-individuals-or-undertakings"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Protection → personal data or business secrets**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 6

    > 5. The rights of defence of the parties concerned shall be fully respected in the proceedings. 
    > They shall be entitled to have access to the European Data Protection Supervisor’s file, 
    > subject to the legitimate interest of individuals or undertakings in the protection of their 
    > personal data or business secrets.

### Incoming relationships

_(none)_



---

## Node: parties concerned
<a name="node-parties-concerned"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Access to → european data protection supervisor**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 6

    > 5. The rights of defence of the parties concerned shall be fully respected in the proceedings. 
    > They shall be entitled to have access to the European Data Protection Supervisor’s file, 
    > subject to the legitimate interest of individuals or undertakings in the protection of their 
    > personal data or business secrets.

### Incoming relationships

_(none)_



---

## Node: union law protecting fundamental rights
<a name="node-union-law-protecting-fundamental-rights"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Insufficient documentation for infringement determination ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 4

    > 3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an 
    > infringement of obligations under Union law protecting fundamental rights has occurred, 
    > the public authority or body referred to in paragraph 1 may make a reasoned request to the 
    > market surveillance authority, to organise testing of the high-risk AI system through 
    > technical means. The market surveillance authority shall organise the testing with the close 
    > involvement of the requesting public authority or body within a reasonable time following 
    > the request.



---

## Node: competent national courts or other bodies
<a name="node-competent-national-courts-or-other-bodies"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Applied in Member States with different legal systems ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 11

    > 9. Depending on the legal system of the Member States, the rules on administrative fines may 
    > be applied in such a manner that the fines are imposed by competent national courts or by 
    > other bodies, as applicable in those Member States. The application of such rules in those 
    > Member States shall have an equivalent effect.



---

## Node: voluntary application of specific requirements
<a name="node-voluntary-application-of-specific-requirements"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Related to ← codes of conduct**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 1

    > Article 95
    > Codes of conduct for voluntary application of specific requirements



---

## Node: union technical documentation assessment certificates
<a name="node-union-technical-documentation-assessment-certificates"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **issues (with request) ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.



---

## Node: achievement of those objectives
<a name="node-achievement-of-those-objectives"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **To measure ← key performance indicators**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.



---

## Node: natural person
<a name="node-natural-person"></a>

*1 outgoing, 7 incoming*

### Outgoing relationships

- **Considers that there has been an infringement of the → obligations laid down in this regulation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.

### Incoming relationships

- **performs identification on ← biometric data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 37

    > (36) ‘biometric verification’ means the automated, one-to-one verification, including 
    > authentication, of the identity of natural persons by comparing their biometric data to 
    > previously provided biometric data;

- **assigns to specific categories based on biometric data ← emotion recognition system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 41

    > (40) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural 
    > persons to specific categories on the basis of their biometric data, unless it is ancillary to 
    > another commercial service and strictly necessary for objective technical reasons;

- **Not subject to regulation (exemption) ← deployers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 12

    > 10. This Regulation does not apply to obligations of deployers who are natural persons 
    > using AI systems in the course of a purely personal non-professional activity.

- **Informed of interaction with AI system ← providers**
  - Chapter 4: Transparency obligations for providers and deployers of certain ai systems 
  - Article 1: Transparency obligations for providers and users of certain ai systems
  - Paragraph 2

    > 1. Providers shall ensure that AI systems intended to interact directly with natural persons are 
    > designed and developed in such a way that the natural persons concerned are informed that 
    > they are interacting with an AI system, unless this is obvious from the point of view of a 
    > natural person who is reasonably well-informed, observant and circumspect, taking into 
    > account the circumstances and the context of use. This obligation shall not apply to AI 
    > systems authorised by law to detect, prevent, investigate or prosecute criminal offences, 
    > subject to appropriate safeguards for the rights and freedoms of third parties, unless 
    > those systems are available for the public to report a criminal offence.

- **Submits reasoned complaints to ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.

- **Always considered high-risk ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 4

    > 3. By derogation from paragraph 2, an AI system shall not be considered to be high-risk if 
    > it does not pose a significant risk of harm to the health, safety or fundamental rights of 
    > natural persons, including by not materially influencing the outcome of decision 
    > making. This shall be the case where one or more of the following conditions are 
    > fulfilled:
    > (a) the AI system is intended to perform a narrow procedural task;
    > (b) the AI system is intended to improve the result of a previously completed human 
    > activity;
    > (c) the AI system is intended to detect decision-making patterns or deviations from 
    > prior decision-making patterns and is not meant to replace or influence the 
    > previously completed human assessment, without proper human review; or
    > (d) the AI system is intended to perform a preparatory task to an assessment relevant 
    > for the purposes of the use cases listed in Annex III.
    > Notwithstanding the first subparagraph, an AI system referred to in Annex III shall 
    > always be considered to be high-risk where the AI system performs profiling of natural 
    > persons.

- **Assign human oversight to ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 3

    > 2. Deployers shall assign human oversight to natural persons who have the necessary 
    > competence, training and authority, as well as the necessary support.
    > .



---

## Node: special categories of personal data
<a name="node-special-categories-of-personal-data"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **Related to → bias detection and correction**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 6

    > 5. To the extent that it is strictly necessary for the purpose of ensuring bias  detection and 
    > correction in relation to the high-risk AI systems in accordance with paragraph (2), points
    > (f) and (g) of this Article, the providers of such systems may exceptionally process special 
    > categories of personal data, subject to appropriate safeguards for the fundamental rights 
    > and freedoms of natural persons. In addition to the provisions set out in Regulation (EU) 
    > 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following 
    > conditions shall apply in order for such processing to occur:
    > (a) the bias detection and correction cannot be effectively fulfilled by processing other 
    > data, including synthetic or anonymised data;
    > (b) the special categories of personal data are subject to technical limitations on the 
    > re-use of the personal data, and state of the art security and privacy-preserving 
    > measures, including pseudonymisation;
    > (c) the special categories of personal data are subject to measures to ensure that the 
    > personal data processed are secured, protected, subject to suitable safeguards, 
    > including strict controls and documentation of the access, to avoid misuse and 
    > ensure that only authorised persons with appropriate confidentiality obligations 
    > have access to those personal data;
    > (d) the personal data in the special categories of personal data are not to be 
    > transmitted, transferred or otherwise accessed by other parties;
    > (e) the personal data in the special categories of personal data are deleted once the 
    > bias has been corrected or the personal data has reached the end of its retention 
    > period, whichever comes first;
    > (f) the records of processing activities pursuant to Regulations (EU) 2016/679 and 
    > (EU) 2018/1725 and Directive (EU) 2016/680include the reasons why the 
    > processing of special categories of personal data was strictly necessary to detect 
    > and correct biases, and why that objective could not be achieved by processing 
    > other data.

### Incoming relationships

- **Processes ← ai systems**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 3

    > 2. When assessing the condition under paragraph 1, point (b),, the Commission shall take into 
    > account the following criteria:
    > (a) the intended purpose of the AI system;
    > (b) the extent to which an AI system has been used or is likely to be used;
    > (c) the nature and amount of the data processed and used by the AI system, in 
    > particular whether special categories of personal data are processed;
    > (d) the extent to which the AI system acts autonomously and the possibility for a 
    > human to override a decision or recommendations that may lead to potential harm;
    > (e) the extent to which the use of an AI system has already caused harm to  health and 
    > safety, has had an adverse impact on  fundamental rights or has given rise to 
    > significant concerns in relation to the likelihood of such harm or adverse impact, as 
    > demonstrated, for example, by reports or documented allegations submitted to 
    > national competent authorities or by other reports, as appropriate;
    > (f) the potential extent of such harm or such adverse impact, in particular in terms of its 
    > intensity and its ability to affect multiple persons or to disproportionately affect a 
    > particular group of persons;
    > (g) the extent to which persons who are potentially harmed or suffer an adverse impact 
    > are dependent on the outcome produced with an AI system, in particular because for 
    > practical or legal reasons it is not reasonably possible to opt-out from that outcome;
    > (h) the extent to which there is an imbalance of power, or the persons who are 
    > potentially harmed or suffer an adverse impact are in a vulnerable position in relation 
    > to the deployer of an AI system, in particular due to status, authority, knowledge, 
    > economic or social circumstances, or age;
    > (i) the extent to which the outcome produced involving an AI system is easily corrigible 
    > or reversible, taking into account the technical solutions available to correct or 
    > reverse it, whereby outcomes having an adverse impact on  health, safety or 
    > fundamental rights, shall not be considered to be easily corrigible or reversible;
    > (j) the magnitude and likelihood of benefit of the deployment of the AI system for 
    > individuals, groups, or society at large, including possible improvements in product 
    > safety;
    > (k) the extent to which existing Union law provides for:
    > (i) effective measures of redress in relation to the risks posed by an AI system, 
    > with the exclusion of claims for damages;
    > (ii) effective measures to prevent or substantially minimise those risks.

- **Exceptionally processed ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 6

    > 5. To the extent that it is strictly necessary for the purpose of ensuring bias  detection and 
    > correction in relation to the high-risk AI systems in accordance with paragraph (2), points
    > (f) and (g) of this Article, the providers of such systems may exceptionally process special 
    > categories of personal data, subject to appropriate safeguards for the fundamental rights 
    > and freedoms of natural persons. In addition to the provisions set out in Regulation (EU) 
    > 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following 
    > conditions shall apply in order for such processing to occur:
    > (a) the bias detection and correction cannot be effectively fulfilled by processing other 
    > data, including synthetic or anonymised data;
    > (b) the special categories of personal data are subject to technical limitations on the 
    > re-use of the personal data, and state of the art security and privacy-preserving 
    > measures, including pseudonymisation;
    > (c) the special categories of personal data are subject to measures to ensure that the 
    > personal data processed are secured, protected, subject to suitable safeguards, 
    > including strict controls and documentation of the access, to avoid misuse and 
    > ensure that only authorised persons with appropriate confidentiality obligations 
    > have access to those personal data;
    > (d) the personal data in the special categories of personal data are not to be 
    > transmitted, transferred or otherwise accessed by other parties;
    > (e) the personal data in the special categories of personal data are deleted once the 
    > bias has been corrected or the personal data has reached the end of its retention 
    > period, whichever comes first;
    > (f) the records of processing activities pursuant to Regulations (EU) 2016/679 and 
    > (EU) 2018/1725 and Directive (EU) 2016/680include the reasons why the 
    > processing of special categories of personal data was strictly necessary to detect 
    > and correct biases, and why that objective could not be achieved by processing 
    > other data.



---

## Node: interpret a system's output
<a name="node-interpret-a-system-s-output"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Allows ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.



---

## Node: european data protection supervisor
<a name="node-european-data-protection-supervisor"></a>

*2 outgoing, 7 incoming*

### Outgoing relationships

- **Establishes an AI regulatory sandbox for → union institutions, bodies, offices and agencies**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 4

    > 3. The European Data Protection Supervisor may also establish an AI regulatory sandbox 
    > for Union institutions, bodies, offices and agencies, and may exercise the roles and the 
    > tasks of national competent authorities in accordance with this Chapter.

- **Market surveillance authority → union institutions, bodies, offices and agencies**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 10

    > 9. Where Union institutions, bodies, offices or agencies fall within the scope of this 
    > Regulation, the European Data Protection Supervisor shall act as their market surveillance 
    > authority, except in relation to the Court of Justice of the European Union acting in its 
    > judicial capacity.

### Incoming relationships

- **Notifies annually ← commission**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 8

    > 7. The European Data Protection Supervisor shall, on an annual basis, notify the 
    > Commission of the administrative fines it has imposed pursuant to this Article and of 
    > any litigation or judicial proceedings it has initiated.

- **may impose ← administrative fine**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 2

    > 1. The European Data Protection Supervisor may impose administrative fines on Union 
    > institutions, bodies, offices and agencies falling within the scope of this Regulation. When 
    > deciding whether to impose an administrative fine and when deciding on the amount of the 
    > administrative fine in each individual case, all relevant circumstances of the specific 
    > situation shall be taken into account and due regard shall be given to the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences; taking 
    > into account the purpose of the AI system concerned as well as the number of 
    > affected persons and the level of damage suffered by them, and any relevant 
    > previous infringement;
    > (b) the degree of responsibility of the Union institution, body, office or agency, taking 
    > into account technical and organisational measures implemented by them;
    > (c) any action taken by the Union institution, body, office or agency to mitigate the 
    > damage suffered by affected persons;
    > (d) the degree of cooperation with the European Data Protection Supervisor in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement, including compliance with any of the measures previously ordered by 
    > the European Data Protection Supervisor against the Union institution, body, office 
    > or agency concerned with regard to the same subject matter;
    > (e) any similar previous infringements by the Union institution, body, office or agency;
    > (f) the manner in which the infringement became known to the European Data 
    > Protection Supervisor, in particular whether, and if so to what extent, the Union 
    > institution, body, office or agency notified the infringement;
    > (g) the annual budget of the Union institution, body, office or agency.

- **Associated with the operation of ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 11

    > 10. National competent authorities shall ensure that, to the extent the innovative AI systems 
    > involve the processing of personal data or otherwise fall under the supervisory remit of 
    > other national authorities or competent authorities providing or supporting access to data, 
    > the national data protection authorities and those other national or competent authorities 
    > are associated with the operation of the AI regulatory sandbox and involved in the 
    > supervision of those aspects to the extent of their respective tasks and powers.

- **Access to ← parties concerned**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 6

    > 5. The rights of defence of the parties concerned shall be fully respected in the proceedings. 
    > They shall be entitled to have access to the European Data Protection Supervisor’s file, 
    > subject to the legitimate interest of individuals or undertakings in the protection of their 
    > personal data or business secrets.

- **Observer ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 3

    > 2. The Board shall be composed of one representative per Member State. The European 
    > Data Protection Supervisor shall participate as observer. The AI Office shall also attend 
    > the Board’s meetings, without taking part in the votes. Other national and Union 
    > authorities, bodies or experts may be invited to the meetings by the Board on a case by 
    > case basis, where the issues discussed are of relevance for them.

- **Associated with the operation of ← innovative ai systems**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 11

    > 10. National competent authorities shall ensure that, to the extent the innovative AI systems 
    > involve the processing of personal data or otherwise fall under the supervisory remit of 
    > other national authorities or competent authorities providing or supporting access to data, 
    > the national data protection authorities and those other national or competent authorities 
    > are associated with the operation of the AI regulatory sandbox and involved in the 
    > supervision of those aspects to the extent of their respective tasks and powers.

- **Fall within scope ← union institutions, bodies, offices and agencies**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 10

    > 9. Where Union institutions, bodies, offices or agencies fall within the scope of this 
    > Regulation, the European Data Protection Supervisor shall act as the competent authority 
    > for their supervision.



---

## Node: interest to join the full code
<a name="node-interest-to-join-the-full-code"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Declares explicitly ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.



---

## Node: best practices
<a name="node-best-practices"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Support through cooperation ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.

- **Include ← financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.



---

## Node: specific ai sandbox project
<a name="node-specific-ai-sandbox-project"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Implement legal provisions with discretionary powers ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 12

    > 11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the 
    > competent authorities supervising the sandboxes, including at regional or local level. Any 
    > significant risks to health and safety and fundamental rights identified during the 
    > development and testing of such AI systems shall result in an adequate mitigation. 
    > National competent authorities shall have the power to temporarily or permanently 
    > suspend the testing process, or the participation in the sandbox if no effective mitigation 
    > is possible, and shall inform the AI Office of such decision. National competent 
    > authorities shall exercise their supervisory powers within the limits of the relevant law, 
    > using their discretionary powers when implementing legal provisions in respect of a 
    > specific AI sandbox project, with the objective of supporting innovation in AI in the 
    > Union.



---

## Node: high-impact capabilities of general-purpose ai models
<a name="node-high-impact-capabilities-of-general-purpose-ai-models"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **specific to → systemic risk**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 66

    > (65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-
    > purpose AI models, having a significant impact on the Union market due to their reach, 
    > or due to actual or reasonably foreseeable negative effects on public health, safety, 
    > public security, fundamental rights, or the society as a whole, that can be propagated at 
    > scale across the value chain;

### Incoming relationships

- **Requests deliverables on ← commission**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 3

    > 2. The Commission shall issue standardisation requests covering all requirements set out 
    > in Section 2 of this Chapter and, as applicable, obligations set out in Chapter IV of this 
    > Regulation, in accordance with Article 10 of Regulation (EU) No 1025/2012, without 
    > undue delay. The standardisation request shall also ask for deliverables on reporting 
    > and documentation processes to improve AI systems’ resource performance, such as 
    > reducing the high-risk AI system’s consumption of energy and other resources 
    > consumption during its lifecycle, and on the energy-efficient development of general-
    > purpose AI models. When preparing a standardisation request, the Commission shall 
    > consult the Board and relevant stakeholders, including the advisory forum.
    > When issuing a standardisation request to European standardisation organisations, the 
    > Commission shall specify that standards have to be clear, consistent, including with the 
    > standards developed in the various sectors for products covered by the existing Union 
    > harmonisation legislation listed in Annex I, and aiming to ensure that AI systems or AI 
    > models placed on the market or put into service in the Union meet the relevant 
    > requirements laid down in this Regulation.
    > The Commission shall request the European standardisation organisations to provide 
    > evidence of their best efforts to fulfil the objectives referred to in the first and the second 
    > subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 
    > 1025/2012.



---

## Node: subsidiaries
<a name="node-subsidiaries"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **ownership relationship ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 1

    > Article 33
    > Subsidiaries of notified bodies and subcontracting

- **subcontracting or activity delegation ← providers**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 4

    > 3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of 
    > the provider. Notified bodies shall make a list of their subsidiaries publicly available.



---

## Node: natural persons or groups of persons
<a name="node-natural-persons-or-groups-of-persons"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **exploits ← ai office**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.



---

## Node: automatic recording of events (logs)
<a name="node-automatic-recording-of-events-logs"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Allows for ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 7: Record-keeping
  - Paragraph 2

    > 1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) 
    > over their lifetime.



---

## Node: dissuasive
<a name="node-dissuasive"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Aims to be ← fines**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.



---

## Node: evaluation and review
<a name="node-evaluation-and-review"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Related to ← article 6(1)**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 1

    > Article 112
    > Evaluation and review



---

## Node: methodology for evaluation of risk levels
<a name="node-methodology-for-evaluation-of-risk-levels"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **develops ← ai office**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 13

    > 11. To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the 
    > AI Office shall undertake to develop an objective and participative methodology for the 
    > evaluation of risk levels based on the criteria outlined in the relevant Articles and the 
    > inclusion of new systems in:
    > (a) the list in Annex III, including the extension of existing area headings or the 
    > addition of new area headings in that Annex;
    > (b) the list of prohibited practices laid down in Article 5; and,
    > (c) the list of AI systems requiring additional transparency measures pursuant to 
    > Article 50.



---

## Node: independent experts appointed for this task
<a name="node-independent-experts-appointed-for-this-task"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Must meet → criteria outlined in article 68(2)**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 3

    > 2. The Commission may decide to appoint independent experts to carry out evaluations on 
    > its behalf, including from the scientific panel established pursuant to Article 68. 
    > Independent experts appointed for this task shall meet the criteria outlined in Article 
    > 68(2).

### Incoming relationships

- **Appoints ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 3

    > 2. The Commission may decide to appoint independent experts to carry out evaluations on 
    > its behalf, including from the scientific panel established pursuant to Article 68. 
    > Independent experts appointed for this task shall meet the criteria outlined in Article 
    > 68(2).



---

## Node: penalties
<a name="node-penalties"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Lay down the rules on ← union or member states**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.



---

## Node: critical infrastructure
<a name="node-critical-infrastructure"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **defines → directive (eu) 2022/2557**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 63

    > (62) ‘critical infrastructure’ means critical infrastructure as defined in Article 2, point (4), of 
    > Directive (EU) 2022/2557;

### Incoming relationships

_(none)_



---

## Node: new conditions or modified conditions
<a name="node-new-conditions-or-modified-conditions"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Adds or modifies ← commission**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.



---

## Node: partial application
<a name="node-partial-application"></a>

*1 outgoing, 10 incoming*

### Outgoing relationships

- **To be applied → partial application**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

### Incoming relationships

- **Applicable from ← article 6(1)**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Applicable from ← chapters i and ii**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Provides for ← regulation**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **drawn up in accordance with Article 11 and Annex IV ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).

- **Performs ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 1

    > Article 34
    > Operational obligations of notified bodies

- **consideration of ← enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 5

    > 4. The risk management measures referred to in paragraph 2, point (d), shall give due 
    > consideration to the effects and possible interaction resulting from the combined 
    > application of the requirements set out in this Section, with a view to minimising risks 
    > more effectively while achieving an appropriate balance in implementing the measures 
    > to fulfil those requirements.

- **Requires ← article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 6: Technical documentation
  - Paragraph 1

    > Article 11
    > Technical documentation

- **Applicable to ← article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 1

    > Article 41
    > Common specifications

- **To be applied ← partial application**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

- **Adheres to ← importers**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 1

    > Article 23
    > Obligations of importers



---

## Node: appropriate procedural safeguards
<a name="node-appropriate-procedural-safeguards"></a>

*0 outgoing, 6 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Submits ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Established under this Article ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 5

    > 4. Where national competent authorities consider authorising testing in real world 
    > conditions supervised within the framework of an AI regulatory sandbox to be 
    > established under this Article, they shall specifically agree with the participants on the 
    > terms and conditions of such testing and in particular on the appropriate safeguards 
    > with a view to protecting fundamental rights, health and safety. Where appropriate, they 
    > shall cooperate with other national competent authorities with a view to ensuring 
    > consistent practices across the Union.

- **Exercises powers under ← market surveillance governance and enforcement**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.

- **Takes ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 9

    > 9. The market surveillance authority shall take appropriate measures, as provided for in 
    > Article 19 of Regulation (EU) 2019/1020, within seven days from the date it received the 
    > notification referred to in paragraph 1 of this Article, and shall follow the notification 
    > procedures as provided in that Regulation.

- **Have ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 4

    > 3. Training, validation and testing data sets shall be relevant, sufficiently representative, and 
    > to the best extent possible, free of errors and complete in view of the intended purpose. 
    > They shall have the appropriate statistical properties, including, where applicable, as 
    > regards the persons or groups of persons in relation to whom the high-risk AI system is 
    > intended to be used. Those characteristics of the data sets may be met at the level of 
    > individual data sets or at the level of a combination thereof.

- **Required for conformity assessment activities ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 10

    > 9. Notified bodies shall take out appropriate liability insurance for their conformity 
    > assessment activities, unless liability is assumed by the Member State in which they are 
    > established in accordance with national law or that Member State is itself directly 
    > responsible for the conformity assessment.



---

## Node: physical place
<a name="node-physical-place"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **located in ← publicly accessible space**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 45

    > (44) ‘publicly accessible space’ means any publicly or privately owned physical place 
    > accessible to an undetermined number of natural persons, regardless of whether certain 
    > conditions for access may apply, and regardless of the potential capacity restrictions;



---

## Node: legal basis
<a name="node-legal-basis"></a>

*2 outgoing, 0 incoming*

### Outgoing relationships

- **Establishes → provision of information**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

- **Justification for → request for access**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.

### Incoming relationships

_(none)_



---

## Node: operator
<a name="node-operator"></a>

*1 outgoing, 7 incoming*

### Outgoing relationships

- **Ensures → appropriate corrective action**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 5

    > 4. The operator shall ensure that all appropriate corrective action is taken in respect of all the 
    > AI systems concerned that it has made available on the Union market.

### Incoming relationships

- **can be a type of ← product manufacturer**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **is a type of ← importers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **can be a type of ← distributors**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **can be a type of ← authorised representatives of providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **is a type of ← deployers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **is a type of ← providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **Applicable to infringements of this Regulation by ← union or member states**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.



---

## Node: subliminal techniques beyond a person's consciousness
<a name="node-subliminal-techniques-beyond-a-person-s-consciousness"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **deploys ← ai office**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.



---

## Node: comparable and verifiable documentation
<a name="node-comparable-and-verifiable-documentation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Facilitates compliance with Annex XI (d) and (e) ← commission**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 6

    > 5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and
    > (e) thereof, the Commission shall adopt delegated acts in accordance with Article 97 to 
    > detail measurement and calculation methodologies with a view to allowing for 
    > comparable and verifiable documentation.



---

## Node: entry into force
<a name="node-entry-into-force"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Applicable ← regulation**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.



---

## Node: free and open source licences
<a name="node-free-and-open-source-licences"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Released under ← operators of high-risk ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 14

    > 12. This Regulation applies to AI systems released under free and open source licences, 
    > unless they are placed on the market or put into service as high-risk AI systems or as an 
    > AI system that falls under Article 5 or 50.



---

## Node: informed consent
<a name="node-informed-consent"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Assessment of ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.

- **gives ← subject**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 60

    > (59) ‘informed consent’ means a subject's freely given, specific, unambiguous and voluntary 
    > expression of his or her willingness to participate in a particular testing in real-world 
    > conditions, after having been informed of all aspects of the testing that are relevant to 
    > the subject's decision to participate;

- **requires ← testing in real-world conditions**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.

- **Belongs to ← section 4**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 12

    > 12. National competent authorities shall immediately notify the Commission of any serious 
    > incident, whether or not it they have taken action on it, in accordance with Article 20 of 
    > Regulation (EU) 2019/1020.
    > Section 3
    > Enforcement



---

## Node: documentation of assessment
<a name="node-documentation-of-assessment"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Designated as a general-purpose AI model with systemic risk ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **Complements ← data collection processes**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 5

    > 4. If any of the obligations laid down in this Article is already complied with as a result of 
    > the data protection impact assessment conducted pursuant to Article 35 of Regulation 
    > (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights impact 
    > assessment referred to in paragraph 1 of this Article shall complement that data 
    > protection impact assessment.

- **draws up ← expert**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.



---

## Node: organization implementing the quality management system
<a name="node-organization-implementing-the-quality-management-system"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Implements ← enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 1

    > Article 17
    > Quality management system



---

## Node: detrimental or unfavourable treatment
<a name="node-detrimental-or-unfavourable-treatment"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **leads to ← social score**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.



---

## Node: common specification
<a name="node-common-specification"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **provides means for compliance with requirements established under this Regulation → common specification**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 29

    > (28) ‘common specification’ means a set of technical specifications as defined in Article 2, 
    > point (4) of Regulation (EU) No 1025/2012, providing means to  comply with certain 
    > requirements  established under this Regulation;

### Incoming relationships

- **provides means for compliance with requirements established under this Regulation ← common specification**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 29

    > (28) ‘common specification’ means a set of technical specifications as defined in Article 2, 
    > point (4) of Regulation (EU) No 1025/2012, providing means to  comply with certain 
    > requirements  established under this Regulation;



---

## Node: subcontracting
<a name="node-subcontracting"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **responsible for specific tasks connected with conformity assessment ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 2

    > 1. Where a notified body subcontracts specific tasks connected with the conformity 
    > assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the 
    > subsidiary meets the requirements laid down in Article 31, and shall inform the notifying 
    > authority accordingly.



---

## Node: distributors
<a name="node-distributors"></a>

*5 outgoing, 5 incoming*

### Outgoing relationships

- **can be a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **perform → obligations pursuant to article 16**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Places → placing on the market**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 1

    > Article 24
    > Obligations of distributors

- **Makes → national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 1

    > Article 24
    > Obligations of distributors

- **Informs → provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 3

    > 2. Where a distributor considers or has reason to consider, on the basis of the information in 
    > its possession, a high-risk AI system not to be in conformity with the requirements set out 
    > in Section 2, it shall not make the high-risk AI system available on the market until the 
    > system has been brought into conformity with those requirements. Furthermore, where the 
    > high-risk AI system presents a risk within the meaning of Article 79(1), the distributor 
    > shall inform the provider or the importer of the system, as applicable, to that effect.

### Incoming relationships

- **place on the market ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **Cooperate in any action ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 7

    > 6. Distributors shall cooperate with national competent authorities in any action those 
    > authorities take in relation to a high-risk AI system they made available on the market, 
    > in particular to reduce or mitigate the risk posed by it.

- **Under responsibility ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 4

    > 3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, 
    > where applicable, storage or transport conditions do not jeopardise the compliance of the 
    > system with the requirements set out in Section 2.

- **Applicable to ← providers**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 2

    > 1. Before making a high-risk AI system available on the market, distributors shall verify that 
    > it bears the required CE marking, that it is accompanied by a copy of EU declaration of 
    > conformity and instructions for use, and that the provider and the importer of the system, 
    > as applicable, have complied with their respective obligations as laid down in Article 16, 
    > points (b) and (c) and Article 23(3).

- **Applicable to ← importers**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 2

    > 1. Before making a high-risk AI system available on the market, distributors shall verify that 
    > it bears the required CE marking, that it is accompanied by a copy of EU declaration of 
    > conformity and instructions for use, and that the provider and the importer of the system, 
    > as applicable, have complied with their respective obligations as laid down in Article 16, 
    > points (b) and (c) and Article 23(3).



---

## Node: european parliament or council
<a name="node-european-parliament-or-council"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Notifies simultaneously ← commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 6

    > 5. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the 
    > European Parliament and to the Council.

- **Takes into account positions and findings ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.

- **Precondition for entering into force ← delegated act**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 7

    > 6. Any delegated act adopted pursuant Article 6(6), Article 7(1) and (3), Article 11(3), Article 
    > 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall 
    > enter into force only if no objection has been expressed by either the European Parliament 
    > or the Council within a period of three months of notification of that act to the European 
    > Parliament and the Council or if, before the expiry of that period, the European Parliament 
    > and the Council have both informed the Commission that they will not object. That period 
    > shall be extended by three months at the initiative of the European Parliament or of the 
    > Council.

- **Recipient of ← importers**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.



---

## Node: co-chairs
<a name="node-co-chairs"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **selected from among ← union or member states**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 7

    > 6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from 
    > among its members, in accordance with criteria set out in paragraph 2. The term of 
    > office of the co-chairs shall be two years, renewable once.

- **consists of ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 7

    > 6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from 
    > among its members, in accordance with criteria set out in paragraph 2. The term of 
    > office of the co-chairs shall be two years, renewable once.



---

## Node: other groups of vulnerable persons
<a name="node-other-groups-of-vulnerable-persons"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **may have adverse impact on ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 10

    > 9. When implementing the risk management system as provided for in paragraphs 1 to 7, 
    > providers shall give consideration to whether in view of its intended purpose the high-risk 
    > AI system is likely to have an adverse impact on persons under the age of 18 and, as 
    > appropriate, other groups of vulnerable persons.



---

## Node: regional or local level
<a name="node-regional-or-local-level"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Established at ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 3

    > 2. Additional AI regulatory sandboxes at regional or local level, or established jointly with 
    > the competent authorities of other Member States may also be established.



---

## Node: validation data set
<a name="node-validation-data-set"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **Separate or variable split → validation data set**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 32

    > (31) ‘validation data set’ means a separate data set or part of the training data set, either as a 
    > fixed or variable split;

### Incoming relationships

- **Used for ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 31

    > (30) ‘validation data’ means data used for providing an evaluation of the trained AI system and 
    > for tuning its non-learnable parameters and its learning process in order, inter alia, to 
    > prevent underfitting or overfitting;

- **Separate or variable split ← validation data set**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 32

    > (31) ‘validation data set’ means a separate data set or part of the training data set, either as a 
    > fixed or variable split;



---

## Node: intellectual property rights
<a name="node-intellectual-property-rights"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Compliance with → union and national law**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 6

    > 5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual 
    > property rights, confidential business information and trade secrets in accordance with 
    > Union and national law.

### Incoming relationships

_(none)_



---

## Node: administrative fine
<a name="node-administrative-fine"></a>

*9 outgoing, 3 incoming*

### Outgoing relationships

- **considers → provider of an ai system**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **takes into account → number of affected persons**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **considers → level of damage suffered by them**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **regard is given to → same operator for the same infringement**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **considers → degree of responsibility of the operator**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.

- **Subject to → union institutions, bodies, offices and agencies**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 1

    > Article 100
    > Administrative fines on Union institutions, bodies, offices and agencies

- **may impose → european data protection supervisor**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 2

    > 1. The European Data Protection Supervisor may impose administrative fines on Union 
    > institutions, bodies, offices and agencies falling within the scope of this Regulation. When 
    > deciding whether to impose an administrative fine and when deciding on the amount of the 
    > administrative fine in each individual case, all relevant circumstances of the specific 
    > situation shall be taken into account and due regard shall be given to the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences; taking 
    > into account the purpose of the AI system concerned as well as the number of 
    > affected persons and the level of damage suffered by them, and any relevant 
    > previous infringement;
    > (b) the degree of responsibility of the Union institution, body, office or agency, taking 
    > into account technical and organisational measures implemented by them;
    > (c) any action taken by the Union institution, body, office or agency to mitigate the 
    > damage suffered by affected persons;
    > (d) the degree of cooperation with the European Data Protection Supervisor in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement, including compliance with any of the measures previously ordered by 
    > the European Data Protection Supervisor against the Union institution, body, office 
    > or agency concerned with regard to the same subject matter;
    > (e) any similar previous infringements by the Union institution, body, office or agency;
    > (f) the manner in which the infringement became known to the European Data 
    > Protection Supervisor, in particular whether, and if so to what extent, the Union 
    > institution, body, office or agency notified the infringement;
    > (g) the annual budget of the Union institution, body, office or agency.

- **Applied in Member States with different legal systems → competent national courts or other bodies**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 11

    > 9. Depending on the legal system of the Member States, the rules on administrative fines may 
    > be applied in such a manner that the fines are imposed by competent national courts or by 
    > other bodies, as applicable in those Member States. The application of such rules in those 
    > Member States shall have an equivalent effect.

- **applied by → union or member states**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

### Incoming relationships

- **Subject to ← operators or notified bodies**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 5

    > 4. Non-compliance of an AI system with any of the following provisions related to 
    > operators or notified bodies, other than those laid down in Articles 5 , shall be subject to 
    > administrative fines of up to 15 000 000 EUR or, if the offender is an undertaking, up to

- **Subject to ← national competent authorities**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 7

    > 5. The supply of incorrect, incomplete or misleading information to notified bodies or 
    > national competent authorities in reply to a request shall be subject to administrative fines 
    > of up to 7 500 000 EUR or, if the offender is an undertaking, up to 1 % of its total 
    > worldwide annual turnover for the preceding financial year, whichever is higher.

- **Not affect ← sensitive operational data**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 7

    > 6. Funds collected by imposition of fines in this Article shall contribute to the general 
    > budget of the Union. The fines shall not affect the effective operation of the Union 
    > institution, body, office or agency fined.



---

## Node: users
<a name="node-users"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **transparency obligations pursuant to Article 50 ← providers**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Requires modification of testing in real world conditions ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 4

    > 3. Where a market surveillance authority has been informed by the prospective provider, 
    > the provider or any third party of a serious incident or has other grounds for considering 
    > that the conditions set out in Articles 60 and 61 are not met, it may take either of the 
    > following decisions on its territory, as appropriate:
    > (a) to suspend or terminate the testing in real world conditions;
    > (b) to require the provider or prospective provider and users to modify any aspect of 
    > the testing in real world conditions.



---

## Node: union safeguard procedure
<a name="node-union-safeguard-procedure"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Implements → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 10: Union safeguard procedure
  - Paragraph 1

    > Article 81
    > Union safeguard procedure

### Incoming relationships

_(none)_



---

## Node: personal data
<a name="node-personal-data"></a>

*1 outgoing, 6 incoming*

### Outgoing relationships

- **Defined in → regulation (eu) no 1025/2012**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 51

    > (50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation 
    > (EU) 2016/679;

### Incoming relationships

- **Used for providing an independent evaluation ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 33

    > (32) ‘testing data’ means data used for providing an independent evaluation of the  AI system 
    > in order to confirm the expected performance of that system before its placing on the 
    > market or putting into service;

- **defined in Article ← regulation (eu) no 1025/2012**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 52

    > (51) ‘non-personal data’ means data other than personal data as defined in Article 4, point 
    > (1), of Regulation (EU) 2016/679;

- **results from ← biometric data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 35

    > (34) ‘biometric data’ means personal data resulting from specific technical processing relating 
    > to the physical, physiological or behavioural characteristics of a natural person,  such as 
    > facial images or dactyloscopic data;

- **Further processing for developing AI systems ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 3: Further processing of personal data for developing certain ai systems
  - Paragraph 1

    > Article 59
    > Further processing of personal data for developing certain AI systems
    >  in the public interest in the AI regulatory sandbox

- **Applies to ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **related to ← testing in real-world conditions**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.



---

## Node: operators or notified bodies
<a name="node-operators-or-notified-bodies"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Subject to → administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 5

    > 4. Non-compliance of an AI system with any of the following provisions related to 
    > operators or notified bodies, other than those laid down in Articles 5 , shall be subject to 
    > administrative fines of up to 15 000 000 EUR or, if the offender is an undertaking, up to

### Incoming relationships

_(none)_



---

## Node: identification number of the notified body
<a name="node-identification-number-of-the-notified-body"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Affixed by → authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 5

    > 4. Where applicable, the CE marking shall be followed by the identification number of the 
    > notified body responsible for the conformity assessment procedures set out in Article 43. 
    > The identification number of the notified body shall be affixed by the body itself or, under 
    > its instructions, by the provider or by the provider’s authorised representative. The 
    > identification number shall also be indicated in any promotional material which mentions 
    > that the high-risk AI system fulfils the requirements for CE marking.

- **Indicated in → promotional material**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 5

    > 4. Where applicable, the CE marking shall be followed by the identification number of the 
    > notified body responsible for the conformity assessment procedures set out in Article 43. 
    > The identification number of the notified body shall be affixed by the body itself or, under 
    > its instructions, by the provider or by the provider’s authorised representative. The 
    > identification number shall also be indicated in any promotional material which mentions 
    > that the high-risk AI system fulfils the requirements for CE marking.

### Incoming relationships

- **assumes responsibilities for ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.

- **Applies to ← article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 30: Identification numbers and lists of notified bodies
  - Paragraph 1

    > Article 35
    > Identification numbers and lists of notified bodies



---

## Node: outcome produced involving an ai system
<a name="node-outcome-produced-involving-an-ai-system"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Taking into account → easily corrigible or reversible**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 3

    > 2. When assessing the condition under paragraph 1, point (b),, the Commission shall take into 
    > account the following criteria:
    > (a) the intended purpose of the AI system;
    > (b) the extent to which an AI system has been used or is likely to be used;
    > (c) the nature and amount of the data processed and used by the AI system, in 
    > particular whether special categories of personal data are processed;
    > (d) the extent to which the AI system acts autonomously and the possibility for a 
    > human to override a decision or recommendations that may lead to potential harm;
    > (e) the extent to which the use of an AI system has already caused harm to  health and 
    > safety, has had an adverse impact on  fundamental rights or has given rise to 
    > significant concerns in relation to the likelihood of such harm or adverse impact, as 
    > demonstrated, for example, by reports or documented allegations submitted to 
    > national competent authorities or by other reports, as appropriate;
    > (f) the potential extent of such harm or such adverse impact, in particular in terms of its 
    > intensity and its ability to affect multiple persons or to disproportionately affect a 
    > particular group of persons;
    > (g) the extent to which persons who are potentially harmed or suffer an adverse impact 
    > are dependent on the outcome produced with an AI system, in particular because for 
    > practical or legal reasons it is not reasonably possible to opt-out from that outcome;
    > (h) the extent to which there is an imbalance of power, or the persons who are 
    > potentially harmed or suffer an adverse impact are in a vulnerable position in relation 
    > to the deployer of an AI system, in particular due to status, authority, knowledge, 
    > economic or social circumstances, or age;
    > (i) the extent to which the outcome produced involving an AI system is easily corrigible 
    > or reversible, taking into account the technical solutions available to correct or 
    > reverse it, whereby outcomes having an adverse impact on  health, safety or 
    > fundamental rights, shall not be considered to be easily corrigible or reversible;
    > (j) the magnitude and likelihood of benefit of the deployment of the AI system for 
    > individuals, groups, or society at large, including possible improvements in product 
    > safety;
    > (k) the extent to which existing Union law provides for:
    > (i) effective measures of redress in relation to the risks posed by an AI system, 
    > with the exclusion of claims for damages;
    > (ii) effective measures to prevent or substantially minimise those risks.

### Incoming relationships

_(none)_



---

## Node: single contact point vis-à-vis board
<a name="node-single-contact-point-vis-vis-board"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Designated as a single contact point ← union or member states**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.



---

## Node: implementing acts
<a name="node-implementing-acts"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Establishment ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 2

    > 1. In order to avoid fragmentation across the Union, the Commission shall adopt 
    > implementing acts specifying the detailed arrangements for the establishment, 
    > development, implementation, operation and supervision of the AI regulatory sandboxes. 
    > The implementing acts shall include common principles on the following issues:
    > (a) eligibility and selection criteria for participation in the AI regulatory sandbox;
    > (b) procedures for the application, participation, monitoring, exiting from and 
    > termination of the AI regulatory sandbox, including the sandbox plan and the exit 
    > report;
    > (c) the terms and conditions applicable to the participants.
    > Those implementing acts shall be adopted in accordance with the examination 
    > procedure referred to in Article 98(2).



---

## Node: laws, regulations or administrative provisions
<a name="node-laws-regulations-or-administrative-provisions"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Maintaining or introducing → union or member states**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

### Incoming relationships

- **Does not prevent ← directive (eu) 2022/2557**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.



---

## Node: ai model
<a name="node-ai-model"></a>

*1 outgoing, 4 incoming*

### Outgoing relationships

- **integrates → downstream provider**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 69

    > (68) ‘downstream provider’ means a provider of an AI system, including a general-purpose 
    > AI system, which integrates an AI model, regardless of whether the model is provided by 
    > themselves and vertically integrated or provided by another entity based on contractual 
    > relations.

### Incoming relationships

- **based on ← general-purpose ai models**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 67

    > (66) ‘general-purpose AI system’ means an AI system which is based on a general-purpose 
    > AI model, that has the capability to serve a variety of purposes, both for direct use as 
    > well as for integration in other AI systems;

- **includes ← provider of an ai system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 69

    > (68) ‘downstream provider’ means a provider of an AI system, including a general-purpose 
    > AI system, which integrates an AI model, regardless of whether the model is provided by 
    > themselves and vertically integrated or provided by another entity based on contractual 
    > relations.

- **Restrict making available on the market, withdraw or recall ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

- **unable to access certain information related to ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 4

    > 3. Where a national market surveillance authority is unable to conclude its investigation of 
    > the high-risk AI system because of its inability to access certain information related to 
    > the AI model despite having made all appropriate efforts to obtain that information, it 
    > may submit a reasoned request to the AI Office, by which access to that information 
    > shall be enforced. In that case, the AI Office shall supply to the applicant authority 
    > without delay, and in any event within 30 days, any information that the AI Office 
    > considers to be relevant in order to establish whether a high-risk AI system is non-
    > compliant. National market authorities shall safeguard the confidentiality of the 
    > information they obtain in accordance with Article 78 of this Regulation. The procedure 
    > provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis mutandis.



---

## Node: particular expertise and competence
<a name="node-particular-expertise-and-competence"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Possesses ← expert**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.



---

## Node: controlled environment
<a name="node-controlled-environment"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Provides for ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.



---

## Node: financial resources
<a name="node-financial-resources"></a>

*3 outgoing, 6 incoming*

### Outgoing relationships

- **Include → best practices**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Include → serious incident**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Include → lessons learnt**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

### Incoming relationships

- **Takes into account ← commission**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **requires ← national competent authorities**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **Agrees to access ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **Takes into account ← board**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **Agrees to access ← authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **Prepares ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 11

    > 10. The advisory forum shall prepare an annual report on its activities. That report shall be 
    > made publicly available.



---

## Node: ['process of drawing-up codes of practice']
<a name="node-process-of-drawing-up-codes-of-practice"></a>

*0 outgoing, 6 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Reports regularly on implementation ← ai office**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 6

    > 5. The AI Office shall aim to ensure that participants to the codes of practice report 
    > regularly to the AI Office on the implementation of the commitments and the measures 
    > taken and their outcomes, including as measured against the key performance indicators 
    > as appropriate. Key performance indicators and reporting commitments shall reflect 
    > differences in size and capacity between various participants.

- **may support ← size of the provider’s organisation**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

- **may support ← industry**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

- **may support ← academia**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

- **may support ← downstream provider**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

- **may support ← independent experts**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.



---

## Node: degree of responsibility of the operator
<a name="node-degree-of-responsibility-of-the-operator"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **considers ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.



---

## Node: interface
<a name="node-interface"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Accessible via ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 3

    > 2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it 
    > can easily be accessed via the interface from which that system is accessed or via an 
    > easily accessible machine-readable code or other electronic means.



---

## Node: investment and innovation in ai
<a name="node-investment-and-innovation-in-ai"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Promote ← legal certainty**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 4

    > 3. The participants in the standardisation process shall seek to promote investment and 
    > innovation in AI, including through increasing legal certainty, as well as the 
    > competitiveness and growth of the Union market, and shall contribute to strengthening 
    > global cooperation on standardisation and taking into account existing international 
    > standards in the field of AI that are consistent with Union values, fundamental rights 
    > and interests, and shall enhance multi-stakeholder governance ensuring a balanced 
    > representation of interests and the effective participation of all relevant stakeholders in 
    > accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.

- **Seek to promote ← standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 4

    > 3. The participants in the standardisation process shall seek to promote investment and 
    > innovation in AI, including through increasing legal certainty, as well as the 
    > competitiveness and growth of the Union market, and shall contribute to strengthening 
    > global cooperation on standardisation and taking into account existing international 
    > standards in the field of AI that are consistent with Union values, fundamental rights 
    > and interests, and shall enhance multi-stakeholder governance ensuring a balanced 
    > representation of interests and the effective participation of all relevant stakeholders in 
    > accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.



---

## Node: widespread infringement
<a name="node-widespread-infringement"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **becomes aware of → authorised representatives of providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 4

    > 3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement 
    > or a serious incident as defined in Article 3, point (44) (b), the report referred to in 
    > paragraph 1 of this Article shall be provided immediately, and not later than two days 
    > after the provider or, where applicable, the deployer becomes aware of that incident.

### Incoming relationships

_(none)_



---

## Node: authorised representatives of providers
<a name="node-authorised-representatives-of-providers"></a>

*12 outgoing, 14 incoming*

### Outgoing relationships

- **can be a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **undertake → obligations pursuant to article 16**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Required to undertake → collective agreements**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 1

    > Article 20
    > Corrective actions and duty of information

- **Immediately informs about termination of mandate and reasons (where applicable) → notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 5

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the market surveillance 
    > authority of the Member State in which it is located or established, as well as, where 
    > applicable, the relevant notified body, about the termination of the mandate and the 
    > reasons therefor.

- **appointed in accordance with Article 22(1) → operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).

- **Immediately informs of mandate termination and reasons → ai office**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 6

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the AI Office about the 
    > termination of the mandate and the reasons therefor.

- **Addressable by → national competent authorities**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 5

    > 3. The mandate shall empower the authorised representative to be addressed, in addition to 
    > or instead of the provider, by the AI Office or the national competent authorities, on all 
    > issues related to ensuring compliance with this Regulation.

- **Agrees to access → financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **adoption of immediate measures → automation bias**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **shall suspend the testing in real world conditions → suspension**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **or otherwise terminate it → regulation**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **Notifies → national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 9

    > 8. Providers or prospective providers shall notify the national market surveillance authority 
    > in the Member State where the testing in real world conditions is to be conducted of the 
    > suspension or termination of the testing in real world conditions and of the final 
    > outcomes.

### Incoming relationships

- **Represents in the market ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 1

    > Article 54
    > Authorised representatives of providers of general-purpose AI models

- **Represents ← provider of an ai system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 6

    > (5) ‘authorised representative’ means a natural or legal person located or established in the 
    > Union who has received and accepted a written mandate from a provider of an AI system 
    > or a general-purpose AI model to, respectively, perform and carry out on its behalf the 
    > obligations and procedures established by this Regulation;

- **which are not established in the Union ← providers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **Non-appointing requires non-compliance to end ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Informs of decision/objection ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 5

    > 4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of 
    > this Article, or has issued an objection within the meaning of Article 60(4), point (b), the 
    > decision or the objection shall indicate the grounds therefor and how the provider or 
    > prospective provider can challenge the decision or objection.

- **Inform ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Affixed by ← identification number of the notified body**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 5

    > 4. Where applicable, the CE marking shall be followed by the identification number of the 
    > notified body responsible for the conformity assessment procedures set out in Article 43. 
    > The identification number of the notified body shall be affixed by the body itself or, under 
    > its instructions, by the provider or by the provider’s authorised representative. The 
    > identification number shall also be indicated in any promotional material which mentions 
    > that the high-risk AI system fulfils the requirements for CE marking.

- **Terminates mandate due to non-compliance ← providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 6

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the AI Office about the 
    > termination of the mandate and the reasons therefor.

- **Appoints by written mandate ← public authorities in a third country**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 2

    > 1. Prior to placing a general-purpose AI model on the Union market, providers established 
    > in third countries shall, by written mandate, appoint an authorised representative which 
    > is established in the Union.

- **Inform ← deployers (who are employers)**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 8

    > 7. Before putting into service or using a high-risk AI system at the workplace, deployers 
    > who are employers shall inform workers’ representatives and the affected workers that 
    > they will be subject to the use of the high-risk AI system. This information shall be 
    > provided, where applicable, in accordance with the rules and procedures laid down in 
    > Union and national law and practice on information of workers and their 
    > representatives.

- **Required to provide information ← market surveillance governance and enforcement**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 7

    > 6. In accordance with Article 75, Member States shall confer on their market surveillance 
    > authorities the powers of requiring providers and prospective providers to provide 
    > information, of carrying out unannounced remote or on-site inspections, and of 
    > performing checks on the development of the testing in real world conditions and the 
    > related products. Market surveillance authorities shall use those powers to ensure the 
    > safe development of testing in real world conditions.

- **Conducted by ← testing in real-world conditions**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 3

    > 2. Where testing in real world conditions is conducted for AI systems that are supervised 
    > within an AI regulatory sandbox under Article 59, the market surveillance authorities 
    > shall verify the compliance with the provisions of Article 60 as part of their supervisory 
    > role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the 
    > testing in real world conditions to be conducted by the provider or prospective provider, 
    > in derogation from the conditions set out in Article 60(4), points (f) and (g).

- **becomes aware of ← serious incident**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 4

    > 3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement 
    > or a serious incident as defined in Article 3, point (44) (b), the report referred to in 
    > paragraph 1 of this Article shall be provided immediately, and not later than two days 
    > after the provider or, where applicable, the deployer becomes aware of that incident.

- **becomes aware of ← widespread infringement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 4

    > 3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement 
    > or a serious incident as defined in Article 3, point (44) (b), the report referred to in 
    > paragraph 1 of this Article shall be provided immediately, and not later than two days 
    > after the provider or, where applicable, the deployer becomes aware of that incident.



---

## Node: automation bias
<a name="node-automation-bias"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Require ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

- **creates tendency for ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **adoption of immediate measures ← authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.



---

## Node: confidentiality of information and data obtained
<a name="node-confidentiality-of-information-and-data-obtained"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Complies with ← national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 6

    > 5. When performing their tasks, the national competent authorities shall act in compliance 
    > with the confidentiality obligations set out in Article 78.

- **ensures ← certificate**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.



---

## Node: real number
<a name="node-real-number"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **subset of ← floating-point operation**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 68

    > (67) ‘floating-point operation’ or ‘FLOP’ means any mathematical operation or assignment 
    > involving floating-point numbers, which are a subset of the real numbers typically 
    > represented on computers by an integer of fixed precision scaled by an integer exponent 
    > of a fixed base;



---

## Node: single point of contact
<a name="node-single-point-of-contact"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Designated by ← national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 1

    > Article 70
    > Designation of national competent authorities and single point of contact

- **Designate ← union or member states**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 3

    > 2. Member States shall communicate to the Commission the identity of the notifying 
    > authorities and the market surveillance authorities and the tasks of those authorities, as 
    > well as any subsequent changes thereto. Member States shall make publicly available 
    > information on how competent authorities and single points of contact can be contacted, 
    > through electronic communication means by… [12 months from the date of entry into 
    > force of this Regulation]. Member States shall designate a market surveillance authority 
    > to act as the single point of contact for this Regulation, and shall notify the Commission 
    > of the identity of the single point of contact. The Commission shall make a list of the 
    > single points of contact publicly available.



---

## Node: member states or ai office
<a name="node-member-states-or-ai-office"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Requests guidelines updates ← commission**
  - Chapter 10: Codes of conduct and guidelines
  - Article 2: Guidelines from the commission on the implementation of this regulation
  - Paragraph 3

    > 2. Upon request of the Member States or the AI Office, or on its own initiative, the 
    > Commission shall update guidelines previously adopted when deemed necessary.



---

## Node: national accreditation body
<a name="node-national-accreditation-body"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Accredited by ← conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 3

    > 2. The application for notification shall be accompanied by a description of the conformity 
    > assessment activities, the conformity assessment module or modules and the types of AI 
    > systems for which the conformity assessment body claims to be competent, as well as by 
    > an accreditation certificate, where one exists, issued by a national accreditation body 
    > attesting that the conformity assessment body fulfils the requirements laid down in 
    > Article 31. 
    > Any valid document related to existing designations of the applicant notified body under 
    > any other Union harmonisation legislation shall be added.

- **Within the meaning of, and in accordance with ← regulation (eu) no 1025/2012**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 3

    > 2. Member States may decide that the assessment and monitoring referred to in 
    > paragraph 1 shall be carried out by a national accreditation body within the meaning of, 
    > and in accordance with, Regulation (EC) No 765/2008  .



---

## Node: public authorities in a third country
<a name="node-public-authorities-in-a-third-country"></a>

*1 outgoing, 6 incoming*

### Outgoing relationships

- **Appoints by written mandate → authorised representatives of providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 2

    > 1. Prior to placing a general-purpose AI model on the Union market, providers established 
    > in third countries shall, by written mandate, appoint an authorised representative which 
    > is established in the Union.

### Incoming relationships

- **Applicability exemption ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 6

    > 4. This Regulation applies neither to public authorities in a third country nor to international 
    > organisations falling within the scope of this Regulation pursuant to paragraph 1, where 
    > those authorities or organisations use AI systems in the framework of international 
    > cooperation or agreements for law enforcement and judicial cooperation with the Union or 
    > with one or more Member States, provided that such a third country or international 
    > organisation provides adequate safeguards with respect to the protection of fundamental 
    > rights and freedoms of individuals.

- **Exchange confidential information with ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 7

    > 5. The Commission and Member States may exchange, where necessary and in accordance 
    > with relevant provisions of international and trade agreements, confidential information 
    > with regulatory authorities of third countries with which they have concluded bilateral or 
    > multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.

- **Identify ← union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 3

    > 2. By … [three months after the entry into force of this Regulation], each Member State shall 
    > identify the public authorities or bodies referred to in paragraph 1 and make a list of them 
    > publicly available  . Member States shall notify the list to the Commission and to the 
    > other Member States, and shall keep the list up to date.

- **Reasoned request for testing through technical means ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 6: Powers of authorities protecting fundamental rights
  - Paragraph 4

    > 3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an 
    > infringement of obligations under Union law protecting fundamental rights has occurred, 
    > the public authority or body referred to in paragraph 1 may make a reasoned request to the 
    > market surveillance authority, to organise testing of the high-risk AI system through 
    > technical means. The market surveillance authority shall organise the testing with the close 
    > involvement of the requesting public authority or body within a reasonable time following 
    > the request.

- **acts on behalf of ← deployers**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 4

    > 3. The data listed in Section C of Annex VIII shall be entered into the EU database by the 
    > deployer who is, or who acts on behalf of, a public authority, agency or body, in 
    > accordance with Articles 49(2) and (3).

- **Notifies ← serious incident**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 8

    > 8. Upon receiving a notification related to a serious incident referred to in Article 3, point 
    > (44)(c), the relevant market surveillance authority shall inform the national public 
    > authorities or bodies referred to in Article 77(1). The Commission shall develop dedicated 
    > guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. 
    > That guidance shall be issued by … [12 months after the entry into force of this 
    > Regulation], and shall be assessed regularly.



---

## Node: effective judicial remedies and due process
<a name="node-effective-judicial-remedies-and-due-process"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Subject to ← market surveillance governance and enforcement**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.



---

## Node: fundamental rights
<a name="node-fundamental-rights"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Impacts ← ai systems**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Pose an adverse impact on ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.



---

## Node: affected workers
<a name="node-affected-workers"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Inform ← deployers (who are employers)**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 8

    > 7. Before putting into service or using a high-risk AI system at the workplace, deployers 
    > who are employers shall inform workers’ representatives and the affected workers that 
    > they will be subject to the use of the high-risk AI system. This information shall be 
    > provided, where applicable, in accordance with the rules and procedures laid down in 
    > Union and national law and practice on information of workers and their 
    > representatives.



---

## Node: standardisation requests
<a name="node-standardisation-requests"></a>

*2 outgoing, 4 incoming*

### Outgoing relationships

- **Directed to → standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 3

    > 2. The Commission shall issue standardisation requests covering all requirements set out 
    > in Section 2 of this Chapter and, as applicable, obligations set out in Chapter IV of this 
    > Regulation, in accordance with Article 10 of Regulation (EU) No 1025/2012, without 
    > undue delay. The standardisation request shall also ask for deliverables on reporting 
    > and documentation processes to improve AI systems’ resource performance, such as 
    > reducing the high-risk AI system’s consumption of energy and other resources 
    > consumption during its lifecycle, and on the energy-efficient development of general-
    > purpose AI models. When preparing a standardisation request, the Commission shall 
    > consult the Board and relevant stakeholders, including the advisory forum.
    > When issuing a standardisation request to European standardisation organisations, the 
    > Commission shall specify that standards have to be clear, consistent, including with the 
    > standards developed in the various sectors for products covered by the existing Union 
    > harmonisation legislation listed in Annex I, and aiming to ensure that AI systems or AI 
    > models placed on the market or put into service in the Union meet the relevant 
    > requirements laid down in this Regulation.
    > The Commission shall request the European standardisation organisations to provide 
    > evidence of their best efforts to fulfil the objectives referred to in the first and the second 
    > subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 
    > 1025/2012.

- **Seek to promote → investment and innovation in ai**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 4

    > 3. The participants in the standardisation process shall seek to promote investment and 
    > innovation in AI, including through increasing legal certainty, as well as the 
    > competitiveness and growth of the Union market, and shall contribute to strengthening 
    > global cooperation on standardisation and taking into account existing international 
    > standards in the field of AI that are consistent with Union values, fundamental rights 
    > and interests, and shall enhance multi-stakeholder governance ensuring a balanced 
    > representation of interests and the effective participation of all relevant stakeholders in 
    > accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012.

### Incoming relationships

- **Assesses harmonised standard ← commission**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 5

    > 4. Where a harmonised standard is adopted by a European standardisation organisation 
    > and proposed to the Commission for the publication of its reference in the Official 
    > Journal of the European Union, the Commission shall assess the harmonised standard 
    > in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised 
    > standard is published in the Official Journal of the European Union, the Commission 
    > shall repeal the implementing acts referred to in paragraph 1, or parts thereof which 
    > cover the same requirements set out in Section 2 of this Chapter.

- **Participate in coordination activities ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 13

    > 12. Notified bodies shall participate in coordination activities as referred to in Article 38. They 
    > shall also take part directly, or be represented in, European standardisation organisations, 
    > or ensure that they are aware and up to date in respect of relevant standards.

- **Directed to ← standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 3

    > 2. The Commission shall issue standardisation requests covering all requirements set out 
    > in Section 2 of this Chapter and, as applicable, obligations set out in Chapter IV of this 
    > Regulation, in accordance with Article 10 of Regulation (EU) No 1025/2012, without 
    > undue delay. The standardisation request shall also ask for deliverables on reporting 
    > and documentation processes to improve AI systems’ resource performance, such as 
    > reducing the high-risk AI system’s consumption of energy and other resources 
    > consumption during its lifecycle, and on the energy-efficient development of general-
    > purpose AI models. When preparing a standardisation request, the Commission shall 
    > consult the Board and relevant stakeholders, including the advisory forum.
    > When issuing a standardisation request to European standardisation organisations, the 
    > Commission shall specify that standards have to be clear, consistent, including with the 
    > standards developed in the various sectors for products covered by the existing Union 
    > harmonisation legislation listed in Annex I, and aiming to ensure that AI systems or AI 
    > models placed on the market or put into service in the Union meet the relevant 
    > requirements laid down in this Regulation.
    > The Commission shall request the European standardisation organisations to provide 
    > evidence of their best efforts to fulfil the objectives referred to in the first and the second 
    > subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 
    > 1025/2012.

- **Addressing request ← harmonised standard**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 2

    > 1. The Commission is empowered to adopt, implementing acts establishing common 
    > specifications for the requirements set out in Section 2 of this Chapter or, as applicable, 
    > for the obligations set out in Chapter IV where the following conditions have been 
    > fulfilled:
    > (a) the Commission has requested, pursuant to Article 10(1) of Regulation (EU) 
    > No 1025/2012, one or more European standardisation organisations to draft a 
    > harmonised standard for the requirements set out in Section 2 of this Chapter, 
    > and:
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > 
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > (i) the request has not been accepted by any of the European standardisation 
    > organisations; or
    > (ii) the harmonised standards addressing that request are not delivered within 
    > the deadline set in accordance with Article 10(1) of Regulation (EU) 
    > No 1025/2012; or
    > (iii) the relevant harmonised standards insufficiently address fundamental rights 
    > concerns; or
    > (iv) the harmonised standards do not comply with the request; and
    > (b) no reference to harmonised standards covering the requirements referred to in 
    > Section 2 of this Title has been published in the Official Journal of the European 
    > Union in accordance with Regulation (EU) No 1025/2012, and no such reference 
    > is expected to be published within a reasonable period.
    > The implementing acts referred to in the first subparagraph of this paragraph shall be 
    > adopted in accordance with the examination procedure referred to in Article 98(2), 
    > after consulting the advisory forum referred to in Article 67.



---

## Node: criminal proceedings
<a name="node-criminal-proceedings"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **related to → sensitive operational data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 39

    > (38) ‘sensitive operational data’ means operational data related to activities of prevention, 
    > detection, investigation or prosecution of criminal offences, the disclosure of which 
    > could jeopardise the integrity of criminal proceedings;

### Incoming relationships

_(none)_



---

## Node: real-time remote biometric identification system
<a name="node-real-time-remote-biometric-identification-system"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Subclass of → real-time remote biometric identification system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 44

    > (43) ‘post remote biometric identification system’ means a remote biometric identification 
    > system other than a real-time remote biometric identification system;

### Incoming relationships

- **Subclass of ← real-time remote biometric identification system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 44

    > (43) ‘post remote biometric identification system’ means a remote biometric identification 
    > system other than a real-time remote biometric identification system;



---

## Node: ai regulatory sandbox
<a name="node-ai-regulatory-sandbox"></a>

*16 outgoing, 5 incoming*

### Outgoing relationships

- **Controlled and responsible by → national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 3: Further processing of personal data for developing certain ai systems
  - Paragraph 3

    > 2. For the purposes of the prevention, investigation, detection or prosecution of criminal 
    > offences or the execution of criminal penalties, including safeguarding against and 
    > preventing prevention threats to public security, under the control and responsibility of 
    > law enforcement authorities, the processing of personal data in AI regulatory sandboxes 
    > shall be based on a specific or Union or national law and subject to the same cumulative 
    > conditions as referred to in paragraph 1.

- **Providing equivalent level of national coverage → union or member states**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 2

    > 1. Member States shall ensure that their competent authorities establish at least one AI 
    > regulatory sandbox at national level, which shall be operational by … [24 months from 
    > the date of entry into force of this Regulation]. That sandbox may also be established 
    > jointly with the competent authorities of one or more other Member States. The 
    > Commission may provide technical support, advice and tools for the establishment and 
    > operation of AI regulatory sandboxes.
    > The obligation under the first subparagraph may also be fulfilled by participating in an 
    > existing sandbox in so far as that participation provides an equivalent level of national 
    > coverage for the participating Member States.

- **Established at → regional or local level**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 3

    > 2. Additional AI regulatory sandboxes at regional or local level, or established jointly with 
    > the competent authorities of other Member States may also be established.

- **Established under this Article → appropriate procedural safeguards**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 5

    > 4. Where national competent authorities consider authorising testing in real world 
    > conditions supervised within the framework of an AI regulatory sandbox to be 
    > established under this Article, they shall specifically agree with the participants on the 
    > terms and conditions of such testing and in particular on the appropriate safeguards 
    > with a view to protecting fundamental rights, health and safety. Where appropriate, they 
    > shall cooperate with other national competent authorities with a view to ensuring 
    > consistent practices across the Union.

- **Further processing for developing AI systems → personal data**
  - Chapter 6: Measures in support of innovation
  - Article 3: Further processing of personal data for developing certain ai systems
  - Paragraph 1

    > Article 59
    > Further processing of personal data for developing certain AI systems
    >  in the public interest in the AI regulatory sandbox

- **Provides for → controlled environment**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.

- **For a limited time → limited time**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.

- **Testing in real world conditions supervised in the sandbox. → testing in real-world conditions**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.

- **Contribute to → legal certainty**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.

- **Support through cooperation → best practices**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.

- **Foster → innovation and competitiveness**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.

- **Contribute to → evidence-based regulatory learning**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.

- **Applies to → article 6(1)**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 1

    > Article 58
    > Detailed arrangements for and functioning of AI regulatory sandboxes

- **Establishment → implementing acts**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 2

    > 1. In order to avoid fragmentation across the Union, the Commission shall adopt 
    > implementing acts specifying the detailed arrangements for the establishment, 
    > development, implementation, operation and supervision of the AI regulatory sandboxes. 
    > The implementing acts shall include common principles on the following issues:
    > (a) eligibility and selection criteria for participation in the AI regulatory sandbox;
    > (b) procedures for the application, participation, monitoring, exiting from and 
    > termination of the AI regulatory sandbox, including the sandbox plan and the exit 
    > report;
    > (c) the terms and conditions applicable to the participants.
    > Those implementing acts shall be adopted in accordance with the examination 
    > procedure referred to in Article 98(2).

- **open to any applying → provider of an ai system**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 3

    > 2. The implementing acts referred to in paragraph 1 shall ensure that:
    > (a) AI regulatory sandboxes are open to any applying prospective provider of an AI 
    > system who fulfils eligibility and selection criteria, which shall be transparent and 
    > fair and national competent authorities inform applicants of their decision within 
    > three months of the application;
    > (b) AI regulatory sandboxes allow broad and equal access and keep up with demand 
    > for participation; prospective providers may also submit applications in 
    > partnerships with users and other relevant third parties;
    > (c) the detailed arrangements for and conditions concerning AI regulatory sandboxes 
    > to the best extent possible support flexibility for national competent authorities to 
    > establish and operate their AI regulatory sandboxes;
    > (d) access to the AI regulatory sandboxes is free of charge for SMEs, including start-
    > ups, without prejudice to exceptional costs that national competent authorities may 
    > recover in a fair and proportionate manner;
    > (e) they facilitate prospective providers, by means of the learning outcomes of the AI 
    > regulatory sandboxes, in complying with conformity assessment obligations under 
    > this Regulation and the voluntary application of the codes of conduct referred to in 
    > Article 95;
    > (f) AI regulatory sandboxes facilitate the involvement of other relevant actors within 
    > the AI ecosystem, such as notified bodies and standardisation organisations, 
    > SMEs, start-ups, enterprises, innovators, testing and experimentation facilities, 
    > research and experimentation labs and European Digital Innovation Hubs, centres 
    > of excellence, individual researchers, in order to allow and facilitate cooperation 
    > with the public and private sectors;
    > (g) procedures, processes and administrative requirements for application, selection, 
    > participation and exiting the AI regulatory sandbox are simple, easily intelligible, 
    > and clearly communicated in order to facilitate the participation of SMEs, 
    > including start-ups, with limited legal and administrative capacities and are 
    > streamlined across the Union, in order to avoid fragmentation and that 
    > participation in an AI regulatory sandbox established by a Member State, or by the 
    > European Data Protection Supervisor is mutually and uniformly recognised and 
    > carries the same legal effects across the Union;
    > (h) participation in the AI regulatory sandbox is limited to a period that is appropriate 
    > to the complexity and scale of the project, which may be extended by the national 
    > competent authority;
    > (i) AI regulatory sandboxes facilitate the development of tools and infrastructure for 
    > testing, benchmarking, assessing and explaining dimensions of AI systems 
    > relevant for regulatory learning, such as accuracy, robustness and cybersecurity, 
    > as well as measures to mitigate risks to fundamental rights and society at large.

- **free of charge → smes including start-ups**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 3

    > 2. The implementing acts referred to in paragraph 1 shall ensure that:
    > (a) AI regulatory sandboxes are open to any applying prospective provider of an AI 
    > system who fulfils eligibility and selection criteria, which shall be transparent and 
    > fair and national competent authorities inform applicants of their decision within 
    > three months of the application;
    > (b) AI regulatory sandboxes allow broad and equal access and keep up with demand 
    > for participation; prospective providers may also submit applications in 
    > partnerships with users and other relevant third parties;
    > (c) the detailed arrangements for and conditions concerning AI regulatory sandboxes 
    > to the best extent possible support flexibility for national competent authorities to 
    > establish and operate their AI regulatory sandboxes;
    > (d) access to the AI regulatory sandboxes is free of charge for SMEs, including start-
    > ups, without prejudice to exceptional costs that national competent authorities may 
    > recover in a fair and proportionate manner;
    > (e) they facilitate prospective providers, by means of the learning outcomes of the AI 
    > regulatory sandboxes, in complying with conformity assessment obligations under 
    > this Regulation and the voluntary application of the codes of conduct referred to in 
    > Article 95;
    > (f) AI regulatory sandboxes facilitate the involvement of other relevant actors within 
    > the AI ecosystem, such as notified bodies and standardisation organisations, 
    > SMEs, start-ups, enterprises, innovators, testing and experimentation facilities, 
    > research and experimentation labs and European Digital Innovation Hubs, centres 
    > of excellence, individual researchers, in order to allow and facilitate cooperation 
    > with the public and private sectors;
    > (g) procedures, processes and administrative requirements for application, selection, 
    > participation and exiting the AI regulatory sandbox are simple, easily intelligible, 
    > and clearly communicated in order to facilitate the participation of SMEs, 
    > including start-ups, with limited legal and administrative capacities and are 
    > streamlined across the Union, in order to avoid fragmentation and that 
    > participation in an AI regulatory sandbox established by a Member State, or by the 
    > European Data Protection Supervisor is mutually and uniformly recognised and 
    > carries the same legal effects across the Union;
    > (h) participation in the AI regulatory sandbox is limited to a period that is appropriate 
    > to the complexity and scale of the project, which may be extended by the national 
    > competent authority;
    > (i) AI regulatory sandboxes facilitate the development of tools and infrastructure for 
    > testing, benchmarking, assessing and explaining dimensions of AI systems 
    > relevant for regulatory learning, such as accuracy, robustness and cybersecurity, 
    > as well as measures to mitigate risks to fundamental rights and society at large.

### Incoming relationships

- **Set up by ← notifying authority**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 56

    > (55) ‘AI regulatory sandbox’ means a controlled framework set up by a competent authority 
    > which offers providers or prospective providers of AI systems the possibility to develop, 
    > train, validate and test, where appropriate in real-world conditions, an innovative AI 
    > system, pursuant to a sandbox plan for a limited time under regulatory supervision;

- **Supervisory role ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 3

    > 2. Where testing in real world conditions is conducted for AI systems that are supervised 
    > within an AI regulatory sandbox under Article 59, the market surveillance authorities 
    > shall verify the compliance with the provisions of Article 60 as part of their supervisory 
    > role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the 
    > testing in real world conditions to be conducted by the provider or prospective provider, 
    > in derogation from the conditions set out in Article 60(4), points (f) and (g).

- **Facilitates cross-border cooperation ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 14

    > 13. The AI regulatory sandboxes shall be designed and implemented in such a way that, 
    > where relevant, they facilitate cross-border cooperation between national competent 
    > authorities.

- **Applicable to ← article 6(1)**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 1

    > Article 57
    > AI regulatory sandboxes

- **Accesses ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 1

    > Article 79
    > Procedure at national level for dealing with AI systems presenting a risk



---

## Node: cost-effectiveness
<a name="node-cost-effectiveness"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **taking into account ← fines**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 3

    > 2. The Member States may be required to pay fees for the advice and support provided by 
    > the experts. The structure and the level of fees as well as the scale and structure of 
    > recoverable costs shall be set out in the implementing act referred to in Article 68(1), 
    > taking into account the objectives of the adequate implementation of this Regulation, 
    > cost-effectiveness and the necessity of ensuring effective access to experts for all 
    > Member States.



---

## Node: union ai testing support structures
<a name="node-union-ai-testing-support-structures"></a>

*3 outgoing, 2 incoming*

### Outgoing relationships

- **Implements Union AI testing support structures → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 1

    > Article 84
    > Union AI testing support structures

- **Provides Union AI conformity assessment for testing purposes → conformity assessment body**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 1

    > Article 84
    > Union AI testing support structures

- **Seeks independent technical or scientific advice → board**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies

### Incoming relationships

- **Seeks independent technical or scientific advice ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies

- **Provides independent technical or scientific advice upon request ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies



---

## Node: artificial intelligence act
<a name="node-artificial-intelligence-act"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Takes into account → regulation (eu) no 1025/2012**
  - Chapter 13: Final provisions 
  - Article 6: Amendment to regulation (eu) 2018/858
  - Paragraph 2

    > 2 of that Regulation shall be taken into account.
    > ________________
    > * Regulation (EU) 2024/… of the European Parliament and of the Council of … laying down 
    > harmonised rules on artificial intelligence (Artificial intelligence act) and amending certain 
    > Union legislative acts (OJ L, …, ELI: …).’
    > + OJ: Please insert in the text the number of this Regulation (2021/0106(COD)) and complete 
    > the corresponding footnote.

- **Violation of → ai office**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 1

    > Article 5
    > Prohibited AI Practices

### Incoming relationships

- **Consults ← commission**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 6

    > 5. The Commission shall, after consulting the European Artificial Intelligence Board (the 
    > ‘Board’), and no later than … [18 months from the date of entry into force of this 
    > Regulation], provide guidelines specifying the practical implementation of this Article in 
    > line with Article 96 together with a comprehensive list of practical examples of use cases 
    > of AI systems that are high-risk and not high-risk.

- **Establishment of ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 2

    > 1. A European Artificial Intelligence Board (the ‘Board’) is hereby established.



---

## Node: related litigation or judicial proceedings
<a name="node-related-litigation-or-judicial-proceedings"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Is informed about ← commission**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 13

    > 11. Member States shall, on an annual basis, report to the Commission about the 
    > administrative fines they have issued during that year, in accordance with this Article, 
    > and about any related litigation or judicial proceedings.



---

## Node: relevant data gaps or shortcomings
<a name="node-relevant-data-gaps-or-shortcomings"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Identify ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.



---

## Node: harmonised standards and common specifications
<a name="node-harmonised-standards-and-common-specifications"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **adopted → regulation**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

### Incoming relationships

_(none)_



---

## Node: emotion recognition system
<a name="node-emotion-recognition-system"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **assigns to specific categories based on biometric data → natural person**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 41

    > (40) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural 
    > persons to specific categories on the basis of their biometric data, unless it is ancillary to 
    > another commercial service and strictly necessary for objective technical reasons;

### Incoming relationships

_(none)_



---

## Node: european economic and social committee
<a name="node-european-economic-and-social-committee"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Recipient of ← importers**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.



---

## Node: advisory forum
<a name="node-advisory-forum"></a>

*7 outgoing, 3 incoming*

### Outgoing relationships

- **Provides technical expertise and advises → board and the commission**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 2

    > 1. An advisory forum shall be established to provide technical expertise and advise the 
    > Board and the Commission, and to contribute to their tasks under this Regulation.

- **represents a balanced selection of → ['industry', 'start-ups', 'smes', 'civil society', 'academia']**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.

- **consists of → co-chairs**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 7

    > 6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from 
    > among its members, in accordance with criteria set out in paragraph 2. The term of 
    > office of the co-chairs shall be two years, renewable once.

- **Invites to meetings → experts and other stakeholders**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 8

    > 7. The advisory forum shall hold meetings at least twice a year. The advisory forum may 
    > invite experts and other stakeholders to its meetings.

- **Prepares → financial resources**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 11

    > 10. The advisory forum shall prepare an annual report on its activities. That report shall be 
    > made publicly available.

- **Makes → publicly accessible space**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 11

    > 10. The advisory forum shall prepare an annual report on its activities. That report shall be 
    > made publicly available.

- **has duration → ai office**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 5

    > 4. The term of office of the members of the advisory forum shall be two years, which may 
    > be extended by up to no more than four years.

### Incoming relationships

- **Appoints ← commission**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 4

    > 3. The Commission shall appoint the members of the advisory forum, in accordance with 
    > the criteria set out in paragraph 2, from amongst stakeholders with recognised expertise 
    > in the field of AI.

- **Requests opinions, recommendations and written contributions from ← board**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 9

    > 8. The advisory forum may prepare opinions, recommendations and written contributions 
    > upon request of the Board or the Commission.

- **Applicable to ← article 6(1)**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 1

    > Article 67
    > Advisory forum



---

## Node: impartiality and objectivity
<a name="node-impartiality-and-objectivity"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **upholds ← certificate**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.



---

## Node: objectivity
<a name="node-objectivity"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 8

    > 7. The Board shall be organised and operated so as to safeguard the objectivity and 
    > impartiality of its activities.



---

## Node: instructions for use accompanying the systems
<a name="node-instructions-for-use-accompanying-the-systems"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Use in accordance with ← provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 2

    > 1. Deployers of high-risk AI systems shall take appropriate technical and organisational 
    > measures to ensure they use such systems in accordance with the instructions for use 
    > accompanying the systems, pursuant to paragraphs 3 and 6.

- **does not seek or take ← certificate**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.



---

## Node: compliance with those requirements
<a name="node-compliance-with-those-requirements"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← appropriate type and degree of transparency**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.

- **ensures ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.



---

## Node: deployers
<a name="node-deployers"></a>

*16 outgoing, 5 incoming*

### Outgoing relationships

- **Involved in → ai literacy**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 57

    > (56) ‘AI literacy’ means skills, knowledge and understanding that allows providers, deployers 
    > and affected persons, taking into account their respective rights and obligations in the 
    > context of this Regulation, to make an informed deployment of AI systems, as well as to 
    > gain awareness about the opportunities and risks of AI and possible harm it can cause;

- **Not subject to regulation (exemption) → natural person**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 12

    > 10. This Regulation does not apply to obligations of deployers who are natural persons 
    > using AI systems in the course of a purely personal non-professional activity.

- **uses → ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 5

    > (4) ‘deployer’ means a natural or legal person, public authority, agency or other body using an 
    > AI system under its authority  except where the AI system is used in the course of a 
    > personal non-professional activity;

- **is a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **provides instructions for use to → providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 16

    > (15) ‘instructions for use’ means the information provided by the provider to inform the 
    > deployer of in particular an AI system’s intended purpose and proper use ;

- **are subject to → obligations pursuant to article 16**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Ensures → transparency**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 1

    > Article 13
    > Transparency and provision of information to deployers

- **Provides → provision of information**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 1

    > Article 13
    > Transparency and provision of information to deployers

- **Ensures → biometric data**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 5

    > 4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over 
    > the input data, that deployer shall ensure that input data is relevant and sufficiently 
    > representative in view of the intended purpose of the high-risk AI system.

- **Ensures → sufficiently representative input data**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 5

    > 4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over 
    > the input data, that deployer shall ensure that input data is relevant and sufficiently 
    > representative in view of the intended purpose of the high-risk AI system.

- **Suspend Use and Inform → market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 6

    > 5. Deployers shall monitor the operation of the high-risk AI system on the basis of the 
    > instructions for use and, where relevant, inform providers in accordance with Article 72. 
    > Where deployers have reason to consider that the use of the high-risk AI system in 
    > accordance with the instructions may present a risk within the meaning of Article 79(1), 
    > they shall, without undue delay, inform the provider or distributor and the relevant market 
    > surveillance authority, and shall suspend the use of that system. Where deployers have 
    > identified a serious incident, they shall also immediately inform first the provider, and 
    > then the importer or distributor and the relevant market surveillance authorities of that 
    > incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis 
    > mutandis. This obligation shall not cover sensitive operational data of deployers of AI 
    > systems which are law enforcement authorities.
    > For deployers that are financial institutions subject to requirements regarding their 
    > internal governance, arrangements or processes under Union financial services law, the 
    > monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by 
    > complying with the rules on internal governance arrangements, processes and mechanisms 
    > pursuant to the relevant financial service law.

- **Allows → interpret a system's output**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.

- **Assign human oversight to → natural person**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 3

    > 2. Deployers shall assign human oversight to natural persons who have the necessary 
    > competence, training and authority, as well as the necessary support.
    > .

- **Cooperate with → national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 14

    > 12. Deployers shall cooperate with the relevant national competent authorities in any action 
    > those authorities take in relation to the high-risk AI system in order to implement this 
    > Regulation.

- **Facilitates compliance with simplified manner through → obligations laid down in this regulation**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies

- **acts on behalf of → public authorities in a third country**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 4

    > 3. The data listed in Section C of Annex VIII shall be entered into the EU database by the 
    > deployer who is, or who acts on behalf of, a public authority, agency or body, in 
    > accordance with Articles 49(2) and (3).

### Incoming relationships

- **have their place of establishment or are located ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **Inform ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **provided ← testing**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

- **Monitor Operation ← providers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 6

    > 5. Deployers shall monitor the operation of the high-risk AI system on the basis of the 
    > instructions for use and, where relevant, inform providers in accordance with Article 72. 
    > Where deployers have reason to consider that the use of the high-risk AI system in 
    > accordance with the instructions may present a risk within the meaning of Article 79(1), 
    > they shall, without undue delay, inform the provider or distributor and the relevant market 
    > surveillance authority, and shall suspend the use of that system. Where deployers have 
    > identified a serious incident, they shall also immediately inform first the provider, and 
    > then the importer or distributor and the relevant market surveillance authorities of that 
    > incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis 
    > mutandis. This obligation shall not cover sensitive operational data of deployers of AI 
    > systems which are law enforcement authorities.
    > For deployers that are financial institutions subject to requirements regarding their 
    > internal governance, arrangements or processes under Union financial services law, the 
    > monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by 
    > complying with the rules on internal governance arrangements, processes and mechanisms 
    > pursuant to the relevant financial service law.

- **Provides relevant data on performance throughout lifetime ← market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.



---

## Node: level of autonomy
<a name="node-level-of-autonomy"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Related to ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.



---

## Node: requisite competence in the specific field
<a name="node-requisite-competence-in-the-specific-field"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Possesses ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 11

    > 10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with 
    > the highest degree of professional integrity and the requisite competence in the specific 
    > field, whether those tasks are carried out by notified bodies themselves or on their behalf 
    > and under their responsibility.



---

## Node: guidance
<a name="node-guidance"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Develops ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 8

    > 8. Upon receiving a notification related to a serious incident referred to in Article 3, point 
    > (44)(c), the relevant market surveillance authority shall inform the national public 
    > authorities or bodies referred to in Article 77(1). The Commission shall develop dedicated 
    > guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. 
    > That guidance shall be issued by … [12 months after the entry into force of this 
    > Regulation], and shall be assessed regularly.



---

## Node: board and the commission
<a name="node-board-and-the-commission"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Provides technical expertise and advises ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 2

    > 1. An advisory forum shall be established to provide technical expertise and advise the 
    > Board and the Commission, and to contribute to their tasks under this Regulation.



---

## Node: criteria outlined in article 68(2)
<a name="node-criteria-outlined-in-article-68-2"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Must meet ← independent experts appointed for this task**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 3

    > 2. The Commission may decide to appoint independent experts to carry out evaluations on 
    > its behalf, including from the scientific panel established pursuant to Article 68. 
    > Independent experts appointed for this task shall meet the criteria outlined in Article 
    > 68(2).



---

## Node: request for access
<a name="node-request-for-access"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Intention behind ← purpose of request**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.

- **Justification for ← legal basis**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.



---

## Node: risk
<a name="node-risk"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **combination with → occurrence of harm**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 3

    > (2) ‘risk’ means the combination of the probability of an occurrence of harm and the 
    > severity of that harm;

### Incoming relationships

- **Associated with ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.

- **Presents ← innovative ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 1

    > Article 82
    > Compliant AI systems which present a risk



---

## Node: innovation and competitiveness
<a name="node-innovation-and-competitiveness"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Foster ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.



---

## Node: eu declaration of conformity
<a name="node-eu-declaration-of-conformity"></a>

*2 outgoing, 3 incoming*

### Outgoing relationships

- **Concerned → types of ai systems concerned**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Meets → requirements set out in this section**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

### Incoming relationships

- **Incorrect drawing up requires non-compliance to end ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Translated into a language that can be easily understood by ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Draws up ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.



---

## Node: areas listed in annex iii
<a name="node-areas-listed-in-annex-iii"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Intended to be used in ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.



---

## Node: ai office
<a name="node-ai-office"></a>

*19 outgoing, 9 incoming*

### Outgoing relationships

- **Carried out by → commission**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 48

    > (47) ‘AI Office’ means the Commission’s function of contributing to the implementation, 
    > monitoring and supervision of AI systems and AI governance carried out by the 
    > European Artificial Intelligence Office established by Commission Decision of 
    > 24.1.2024; references in this Regulation to the AI Office shall be construed as references 
    > to the Commission;

- **Facilitate → union or member states**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Encourages and facilitates the drawing up of codes of conduct → smes including start-ups**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 5

    > 4. The AI Office and the Member States shall take into account the specific interests and 
    > needs of SMEs, including start-ups, when encouraging and facilitating the drawing up of 
    > codes of conduct.

- **develops → methodology for evaluation of risk levels**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 13

    > 11. To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the 
    > AI Office shall undertake to develop an objective and participative methodology for the 
    > evaluation of risk levels based on the criteria outlined in the relevant Articles and the 
    > inclusion of new systems in:
    > (a) the list in Annex III, including the extension of existing area headings or the 
    > addition of new area headings in that Annex;
    > (b) the list of prohibited practices laid down in Article 5; and,
    > (c) the list of AI systems requiring additional transparency measures pursuant to 
    > Article 50.

- **Develops → questionnaire template (automated tool)**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies

- **Encourages and facilitates drawing up → systemic risks at union level**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 2

    > 1. The AI Office shall encourage and facilitate the drawing up of codes of practice at 
    > Union level in order to contribute to the proper application of this Regulation, taking 
    > into account international approaches.

- **Reports regularly on implementation → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 6

    > 5. The AI Office shall aim to ensure that participants to the codes of practice report 
    > regularly to the AI Office on the implementation of the commitments and the measures 
    > taken and their outcomes, including as measured against the key performance indicators 
    > as appropriate. Key performance indicators and reporting commitments shall reflect 
    > differences in size and capacity between various participants.

- **Ensures reflection in reporting commitments → key performance indicators**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 6

    > 5. The AI Office shall aim to ensure that participants to the codes of practice report 
    > regularly to the AI Office on the implementation of the commitments and the measures 
    > taken and their outcomes, including as measured against the key performance indicators 
    > as appropriate. Key performance indicators and reporting commitments shall reflect 
    > differences in size and capacity between various participants.

- **Attend meetings without taking part in votes → board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 3

    > 2. The Board shall be composed of one representative per Member State. The European 
    > Data Protection Supervisor shall participate as observer. The AI Office shall also attend 
    > the Board’s meetings, without taking part in the votes. Other national and Union 
    > authorities, bodies or experts may be invited to the meetings by the Board on a case by 
    > case basis, where the issues discussed are of relevance for them.

- **Assessed → codes of conduct**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 10

    > 9. Codes of practice shall be ready at the latest by … [nine months from the date of entry into 
    > force of this Regulation]. The AI Office shall take the necessary steps, including inviting 
    > providers pursuant to paragraph 7. 
    > If, by ... [12 months from the date of entry into force], a code of practice cannot be 
    > finalised, or if the AI Office deems it is not adequate following its assessment under 
    > paragraph 6 of this Article, the Commission may provide, by means of implementing 
    > acts, common rules for the implementation of the obligations provided for in Articles 53 
    > and 55, including the issues set out in paragraph 2 of this Article. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).

- **Follows the specifications of → board in its reasoned request**
  - Chapter 6: Measures in support of innovation
  - Article 6: Measures for  providers and deployers, in particular smes, including start-ups
  - Paragraph 4

    > 3. The AI Office shall undertake the following actions:
    > (a) provide standardised templates for areas covered by this Regulation, as specified by 
    > the Board in its reasoned request;
    > (b) develop and maintain a single information platform providing easy to use 
    > information in relation to this Regulation for all operators across the Union;
    > (c) organise appropriate communication campaigns to raise awareness about the 
    > obligations arising from this Regulation;
    > (d) evaluate and promote the convergence of best practices in public procurement 
    > procedures in relation to AI systems.

- **Facilitates tasks entrusted → union or member states**
  - Chapter 7: Governance
  - Article 1: Ai office
  - Paragraph 3

    > 2. Member States shall facilitate the tasks entrusted to the AI Office, as reflected in this 
    > Regulation.

- **qualified alert → certificate**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 2

    > 1. The scientific panel may provide a qualified alert to the AI Office where it has reason to 
    > suspect that:
    > (a) a general-purpose AI model poses concrete identifiable risk at Union level; or,
    > (b) a general-purpose AI model meets the requirements referred to in Article 51 .

- **supports cross-border market surveillance activities → market surveillance governance and enforcement**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 4

    > 3. The scientific panel shall advise and support the AI Office, in particular with regard to 
    > the following tasks:
    > (a) supporting the implementation and enforcement of this Regulation as regards 
    > general-purpose AI models and systems, in particular by:
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > 
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (b) supporting the work of market surveillance authorities, at their request;
    > (c) supporting cross-border market surveillance activities as referred to in 
    > Article 74(11), without prejudice to the powers of market surveillance authorities;
    > (d) supporting the AI Office in carrying out its duties in the context of the safeguard 
    > clause pursuant to Article 81.

- **Initiates structured dialogue → general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 3

    > 2. Before a measure is requested, the AI Office may initiate a structured dialogue with the 
    > provider of the general-purpose AI model.

- **national market authorities shall safeguard the confidentiality of → information technologies**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 4

    > 3. Where a national market surveillance authority is unable to conclude its investigation of 
    > the high-risk AI system because of its inability to access certain information related to 
    > the AI model despite having made all appropriate efforts to obtain that information, it 
    > may submit a reasoned request to the AI Office, by which access to that information 
    > shall be enforced. In that case, the AI Office shall supply to the applicant authority 
    > without delay, and in any event within 30 days, any information that the AI Office 
    > considers to be relevant in order to establish whether a high-risk AI system is non-
    > compliant. National market authorities shall safeguard the confidentiality of the 
    > information they obtain in accordance with Article 78 of this Regulation. The procedure 
    > provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis mutandis.

- **deploys → subliminal techniques beyond a person's consciousness**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.

- **exploits → natural persons or groups of persons**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.

- **Infringes → other union law**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 9

    > 8. This Article shall not affect the prohibitions that apply where an AI practice infringes 
    > other Union law.

### Incoming relationships

- **Invites ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.

- **Supervises and enforces Chapter V ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 17: Enforcement of the obligations of providers of general-purpose ai models
  - Paragraph 2

    > 1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking 
    > into account the procedural guarantees under Article 94. The Commission shall entrust 
    > the implementation of these tasks to the AI Office, without prejudice to the powers of 
    > organisation of the Commission and the division of competences between Member States 
    > and the Union based on the Treaties.

- **Encourage and facilitate ← codes of conduct**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 2

    > 1. The AI Office and the Member States shall encourage and facilitate the drawing up of 
    > codes of conduct, including related governance mechanisms, intended to foster the 
    > voluntary application to AI systems, other than high-risk AI systems, of some or all of the 
    > requirements set out in Chapter III, Section 2 taking into account the available technical 
    > solutions and industry best practices allowing for the application of such requirements.

- **cooperate to carry out compliance evaluations ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 3

    > 2. Where the relevant market surveillance authorities have sufficient reason to consider 
    > general-purpose AI systems that can be used directly by deployers for at least one 
    > purpose that is classified as high-risk pursuant to this Regulation to be non-compliant 
    > with the requirements laid down in this Regulation, they shall cooperate with the AI 
    > Office to carry out compliance evaluations, and shall inform the Board and other market 
    > surveillance authorities accordingly.

- **Submits annual reports ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Violation of ← artificial intelligence act**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 1

    > Article 5
    > Prohibited AI Practices

- **Immediately informs of mandate termination and reasons ← authorised representatives of providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 6

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the AI Office about the 
    > termination of the mandate and the reasons therefor.

- **Responsible for overseeing ← notifying authority**
  - Chapter 7: Governance
  - Article 1: Ai office
  - Paragraph 1

    > Article 64
    > AI Office

- **has duration ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 5

    > 4. The term of office of the members of the advisory forum shall be two years, which may 
    > be extended by up to no more than four years.



---

## Node: codes of conduct
<a name="node-codes-of-conduct"></a>

*5 outgoing, 3 incoming*

### Outgoing relationships

- **Related to → voluntary application of specific requirements**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 1

    > Article 95
    > Codes of conduct for voluntary application of specific requirements

- **Encourage and facilitate → ai office**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 2

    > 1. The AI Office and the Member States shall encourage and facilitate the drawing up of 
    > codes of conduct, including related governance mechanisms, intended to foster the 
    > voluntary application to AI systems, other than high-risk AI systems, of some or all of the 
    > requirements set out in Chapter III, Section 2 taking into account the available technical 
    > solutions and industry best practices allowing for the application of such requirements.

- **Encourage and facilitate → union or member states**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 2

    > 1. The AI Office and the Member States shall encourage and facilitate the drawing up of 
    > codes of conduct, including related governance mechanisms, intended to foster the 
    > voluntary application to AI systems, other than high-risk AI systems, of some or all of the 
    > requirements set out in Chapter III, Section 2 taking into account the available technical 
    > solutions and industry best practices allowing for the application of such requirements.

- **Voluntary application → ai systems**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Undergoes → impact and effectiveness evaluation**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 8

    > 7. By … [four years from the date of entry into force of this Regulation] and every three 
    > years thereafter, the Commission shall evaluate the impact and effectiveness of voluntary 
    > codes of conduct to foster the application of the requirements set out in Chapter II, Section

### Incoming relationships

- **Adherence ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.

- **Assessed ← ai office**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 10

    > 9. Codes of practice shall be ready at the latest by … [nine months from the date of entry into 
    > force of this Regulation]. The AI Office shall take the necessary steps, including inviting 
    > providers pursuant to paragraph 7. 
    > If, by ... [12 months from the date of entry into force], a code of practice cannot be 
    > finalised, or if the AI Office deems it is not adequate following its assessment under 
    > paragraph 6 of this Article, the Commission may provide, by means of implementing 
    > acts, common rules for the implementation of the obligations provided for in Articles 53 
    > and 55, including the issues set out in paragraph 2 of this Article. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).

- **Applicable to ← article 6(1)**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 1

    > Article 56
    > Codes of practice



---

## Node: highest degree of professional integrity
<a name="node-highest-degree-of-professional-integrity"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Upholds ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 11

    > 10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with 
    > the highest degree of professional integrity and the requisite competence in the specific 
    > field, whether those tasks are carried out by notified bodies themselves or on their behalf 
    > and under their responsibility.



---

## Node: authorities informed accordingly
<a name="node-authorities-informed-accordingly"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **informs ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 2

    > 1. Where a notified body subcontracts specific tasks connected with the conformity 
    > assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the 
    > subsidiary meets the requirements laid down in Article 31, and shall inform the notifying 
    > authority accordingly.



---

## Node: expert
<a name="node-expert"></a>

*3 outgoing, 4 incoming*

### Outgoing relationships

- **Possesses → particular expertise and competence**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **draws up → documentation of assessment**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.

- **Description of → relevant provider**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.

### Incoming relationships

- **Selects ← commission**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **requirement for payment of fees ← union or member states**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 3

    > 2. The Member States may be required to pay fees for the advice and support provided by 
    > the experts. The structure and the level of fees as well as the scale and structure of 
    > recoverable costs shall be set out in the implementing act referred to in Article 68(1), 
    > taking into account the objectives of the adequate implementation of this Regulation, 
    > cost-effectiveness and the necessity of ensuring effective access to experts for all 
    > Member States.

- **Reason by ← certificate**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.

- **Responsibility for incomplete, incorrect or misleading information ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 6

    > 5. The provider of the general-purpose AI model concerned, or its representative shall 
    > supply the information requested. In the case of legal persons, companies or firms, or 
    > where the provider has no legal personality, the persons authorised to represent them by 
    > law or by their statutes, shall supply the information requested on behalf of the provider 
    > of the general-purpose AI model concerned. Lawyers duly authorised to act may supply 
    > information on behalf of their clients. The clients shall nevertheless remain fully 
    > responsible if the information supplied is incomplete, incorrect or misleading.



---

## Node: national level registration
<a name="node-national-level-registration"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Evaluates ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 5

    > 4. The Commission shall without undue delay enter into consultation with the Member State 
    > or member States concerned and the relevant operators, and shall evaluate the national 
    > measures taken. On the basis of the results of that evaluation, the Commission shall decide 
    > whether the measure is justified and, where necessary, propose other appropriate measures.

- **Resulted from ← union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Referred to in point 2 of Annex III for ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 44: Registration
  - Paragraph 7

    > 5. High-risk AI systems referred to in point 2 of Annex III shall be registered at national 
    > level.



---

## Node: limited period authorization
<a name="node-limited-period-authorization"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Grants for exceptional reasons ← market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 2

    > 1. By way of derogation from Article 43 and upon a duly justified request, any market 
    > surveillance authority may authorise the placing on the market or the putting into service of 
    > specific high-risk AI systems within the territory of the Member State concerned, for 
    > exceptional reasons of public security or the protection of life and health of persons, 
    > environmental protection or the protection of key industrial and infrastructural assets. That 
    > authorisation shall be for a limited period  while the necessary conformity assessment 
    > procedures are being carried out, taking into account the exceptional reasons justifying 
    > the derogation. The completion of those procedures shall be undertaken without undue 
    > delay.



---

## Node: overall residual risk
<a name="node-overall-residual-risk"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **represents ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.



---

## Node: sensitive operational data
<a name="node-sensitive-operational-data"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Not affect → administrative fine**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 7

    > 6. Funds collected by imposition of fines in this Article shall contribute to the general 
    > budget of the Union. The fines shall not affect the effective operation of the Union 
    > institution, body, office or agency fined.

### Incoming relationships

- **related to ← criminal proceedings**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 39

    > (38) ‘sensitive operational data’ means operational data related to activities of prevention, 
    > detection, investigation or prosecution of criminal offences, the disclosure of which 
    > could jeopardise the integrity of criminal proceedings;



---

## Node: union and national law
<a name="node-union-and-national-law"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Claims competence for ← conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 3

    > 2. The application for notification shall be accompanied by a description of the conformity 
    > assessment activities, the conformity assessment module or modules and the types of AI 
    > systems for which the conformity assessment body claims to be competent, as well as by 
    > an accreditation certificate, where one exists, issued by a national accreditation body 
    > attesting that the conformity assessment body fulfils the requirements laid down in 
    > Article 31. 
    > Any valid document related to existing designations of the applicant notified body under 
    > any other Union harmonisation legislation shall be added.

- **Subject to ← market surveillance governance and enforcement**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 12

    > 10. The exercise by the market surveillance authority of its powers under this Article shall 
    > be subject to appropriate procedural safeguards in accordance with Union and national 
    > law, including effective judicial remedies and due process.

- **Compliance with ← intellectual property rights**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 6

    > 5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual 
    > property rights, confidential business information and trade secrets in accordance with 
    > Union and national law.

- **Unless provided otherwise in applicable ← provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 7

    > 6. Deployers of high-risk AI systems shall keep the logs automatically generated by that 
    > high-risk AI system  to the extent such logs are under their control,  for a period  
    > appropriate to the intended purpose of the high-risk AI system, of at least six months, 
    > unless provided otherwise in applicable Union or national law, in particular in Union law 
    > on the protection of personal data.
    > Deployers that are financial institutions subject to requirements regarding their internal 
    > governance, arrangements or processes under Union financial services law shall 
    > maintain the logs as part of the documentation kept pursuant to the relevant Union 
    > financial service law.



---

## Node: environmental sustainability
<a name="node-environmental-sustainability"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **(b) → environmental sustainability**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

### Incoming relationships

- **(b) ← environmental sustainability**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.



---

## Node: general-purpose ai models
<a name="node-general-purpose-ai-models"></a>

*23 outgoing, 13 incoming*

### Outgoing relationships

- **Involved in → general-purpose ai models**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **place on the market → providers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **based on → ai model**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 67

    > (66) ‘general-purpose AI system’ means an AI system which is based on a general-purpose 
    > AI model, that has the capability to serve a variety of purposes, both for direct use as 
    > well as for integration in other AI systems;

- **Involved in → systemic risk**
  - Chapter 5: General-purpose ai models
  - Article 5: Obligations for providers of general-purpose ai models with systemic risk
  - Paragraph 1

    > Article 55
    > Obligations for providers of general-purpose AI models with systemic risk

- **Designated as → commission**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **presumed to have → high-impact capabilities**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 3

    > 2. A general-purpose AI model shall be presumed to have high impact capabilities 
    > pursuant to paragraph 1, point (a), when the cumulative amount of computation used 
    > for its training measured in FLOPs is greater than 10^25.

- **Designated as a general-purpose AI model with systemic risk → documentation of assessment**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **Adheres to → regulation**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 1

    > Article 53
    > Obligations for providers of general-purpose AI models

- **Cooperates with → national competent authorities**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 4

    > 3. Providers of general-purpose AI models shall cooperate as necessary with the 
    > Commission and the national competent authorities in the exercise of their competences 
    > and powers pursuant to this Regulation.

- **Provide information and documentation → provider of an ai system**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 2

    > 1. Providers of general-purpose AI models shall:
    > (a) draw up and keep up-to-date the technical documentation of the model, including 
    > its training and testing process and the results of its evaluation, which shall 
    > contain, at a minimum, the elements set out in Annex XI for the purpose of 
    > providing it, upon request, to the AI Office and the national competent authorities;
    > (b) draw up, keep up-to-date and make available information and documentation to 
    > providers of AI systems who intend to integrate the general-purpose AI model into 
    > their AI systems. Without prejudice to the need to respect and protect intellectual 
    > property rights and confidential business information or trade secrets in 
    > accordance with Union and national law, the information and documentation 
    > shall:
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > 
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (c) put in place a policy to comply with Union copyright law, and in particular to 
    > identify and comply with, including through state of the art technologies, a 
    > reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;
    > (d) draw up and make publicly available a sufficiently detailed summary about the 
    > content used for training of the general-purpose AI model, according to a template 
    > provided by the AI Office.

- **Invites → ai office**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.

- **Enable good understanding of capabilities and limitations → ai systems**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 2

    > 1. Providers of general-purpose AI models shall:
    > (a) draw up and keep up-to-date the technical documentation of the model, including 
    > its training and testing process and the results of its evaluation, which shall 
    > contain, at a minimum, the elements set out in Annex XI for the purpose of 
    > providing it, upon request, to the AI Office and the national competent authorities;
    > (b) draw up, keep up-to-date and make available information and documentation to 
    > providers of AI systems who intend to integrate the general-purpose AI model into 
    > their AI systems. Without prejudice to the need to respect and protect intellectual 
    > property rights and confidential business information or trade secrets in 
    > accordance with Union and national law, the information and documentation 
    > shall:
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > 
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (i) enable providers of AI systems to have a good understanding of the 
    > capabilities and limitations of the general-purpose AI model and to comply 
    > with their obligations pursuant to this Regulation; and
    > (ii) contain, at a minimum, the elements set out in Annex XII;
    > (c) put in place a policy to comply with Union copyright law, and in particular to 
    > identify and comply with, including through state of the art technologies, a 
    > reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;
    > (d) draw up and make publicly available a sufficiently detailed summary about the 
    > content used for training of the general-purpose AI model, according to a template 
    > provided by the AI Office.

- **Represents in the market → authorised representatives of providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 1

    > Article 54
    > Authorised representatives of providers of general-purpose AI models

- **Adherence → codes of conduct**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.

- **Declares explicitly → interest to join the full code**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 8

    > 7. The AI Office may invite all providers of general-purpose AI models to adhere to the 
    > codes of practice. For providers of general-purpose AI models not presenting systemic 
    > risks this adherence may be limited to the obligations provided for in Article 53, unless 
    > they declare explicitly their interest to join the full code.

- **Imposes → fines**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 1

    > Article 101
    > Fines for providers of general-purpose AI models

- **Compliance with → obligations laid down in this regulation**
  - Chapter 13: Final provisions 
  - Article 10: Ai systems already placed on the market or put into service
  - Paragraph 4

    > 3. Providers of general-purpose AI models that have been placed on the market before … 
    > [12 months from the date of entry into force of this Regulation] shall take the necessary 
    > steps in order to comply with the obligations laid down in this Regulation by … [36 
    > months from the date of entry into force of this Regulation].

- **poses concrete identifiable risk at Union level → certificate**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 2

    > 1. The scientific panel may provide a qualified alert to the AI Office where it has reason to 
    > suspect that:
    > (a) a general-purpose AI model poses concrete identifiable risk at Union level; or,
    > (b) a general-purpose AI model meets the requirements referred to in Article 51 .

- **In respect of → inspections, investigations or audits**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 16: Reporting of infringements and protection of reporting persons
  - Paragraph 1

    > Article 87
    > Reporting of infringements and protection of reporting persons
    > Directive (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation and the 
    > protection of persons reporting such infringements.
    > Section 5
    > Supervision, investigation, enforcement and monitoring in respect of 
    > providers of general-purpose AI models

- **Enforce → regulation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 17: Enforcement of the obligations of providers of general-purpose ai models
  - Paragraph 1

    > Article 88
    > Enforcement of the obligations of providers of general-purpose AI models

- **Point of contact → types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.

- **Legal representation through authorisation → lawyers duly authorised to act**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 6

    > 5. The provider of the general-purpose AI model concerned, or its representative shall 
    > supply the information requested. In the case of legal persons, companies or firms, or 
    > where the provider has no legal personality, the persons authorised to represent them by 
    > law or by their statutes, shall supply the information requested on behalf of the provider 
    > of the general-purpose AI model concerned. Lawyers duly authorised to act may supply 
    > information on behalf of their clients. The clients shall nevertheless remain fully 
    > responsible if the information supplied is incomplete, incorrect or misleading.

- **Responsibility for incomplete, incorrect or misleading information → expert**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 6

    > 5. The provider of the general-purpose AI model concerned, or its representative shall 
    > supply the information requested. In the case of legal persons, companies or firms, or 
    > where the provider has no legal personality, the persons authorised to represent them by 
    > law or by their statutes, shall supply the information requested on behalf of the provider 
    > of the general-purpose AI model concerned. Lawyers duly authorised to act may supply 
    > information on behalf of their clients. The clients shall nevertheless remain fully 
    > responsible if the information supplied is incomplete, incorrect or misleading.

### Incoming relationships

- **Involved in ← general-purpose ai models**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Complaint against ← downstream provider**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 18: Monitoring actions
  - Paragraph 3

    > 2. Downstream providers shall have the right to lodge a complaint alleging an 
    > infringement of this Regulation. A complaint shall be duly reasoned and indicate at 
    > least:
    > (a) the point of contact of the provider of the general-purpose AI model concerned;
    > (b) a description of the relevant facts, the provisions of this Regulation concerned, and 
    > the reason why the downstream provider considers that the provider of the general-
    > purpose AI model concerned infringed this Regulation;
    > (c) any other information that the downstream provider that sent the request considers 
    > relevant, including, where appropriate, information gathered on its own initiative.

- **Imposes fines on ← commission**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 2

    > 1. The Commission may impose on providers of general purpose AI models fines not 
    > exceeding 3 % of their total worldwide turnover in the preceding financial year or 15 
    > million EUR, whichever is higher., when the Commission finds that the provider 
    > intentionally or negligently:
    > (a) infringed the relevant provisions of this Regulation;
    > (b) failed to comply with a request for a document or for information pursuant to 
    > Article 91, or supplied incorrect, incomplete or misleading information;
    > (c) failed to comply with a measure requested under Article 93;
    > (d) failed to make available to the Commission access to the general-purpose AI model 
    > or general-purpose AI model with systemic risk with a view to conducting an 
    > evaluation pursuant to Article 92.
    > In fixing the amount of the fine or periodic penalty payment, regard shall be had to the 
    > nature, gravity and duration of the infringement, taking due account of the principles of 
    > proportionality and appropriateness. The Commission shall also into account 
    > commitments made in accordance with Article 93(3) or made in relevant codes of 
    > practice in accordance with Article 56.

- **Cooperates with ← commission**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 4

    > 3. Providers of general-purpose AI models shall cooperate as necessary with the 
    > Commission and the national competent authorities in the exercise of their competences 
    > and powers pursuant to this Regulation.

- **Requests information ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 4

    > 3. Upon a duly substantiated request from the scientific panel, the Commission may issue a 
    > request for information to a provider of a general-purpose AI model, where the access to 
    > information is necessary and proportionate for the fulfilment of the tasks of the 
    > scientific panel under Article 68(2).

- **equivalent to or exceed the capabilities of ← high-impact capabilities**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 65

    > (64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities 
    > recorded in the most advanced general-purpose AI models;

- **supply of ← placing on the market**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 11

    > (10) ‘making available on the market’ means the supply of an AI system or a general-purpose 
    > AI model for distribution or use on the Union market in the course of a commercial 
    > activity, whether in return for payment or free of charge;

- **Initiates structured dialogue ← ai office**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 3

    > 2. Before a measure is requested, the AI Office may initiate a structured dialogue with the 
    > provider of the general-purpose AI model.

- **Developed by the same provider ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 2

    > 1. Where an AI system is based on a general-purpose AI model, and the model and the 
    > system are developed by the same provider, the AI Office shall have powers to monitor 
    > and supervise compliance of that AI system with obligations under this Regulation. To 
    > carry out its monitoring and supervision tasks, the AI Office shall have all the powers of 
    > a market surveillance authority within the meaning of Regulation (EU) 2019/1020.

- **Subject to ← operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 1

    > Article 75
    > Mutual assistance, market surveillance and control of general-purpose AI systems

- **Does not sufficiently substantiate ← arguments submitted pursuant to paragraph 2**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 4

    > 3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 
    > are not sufficiently substantiated and the relevant provider was not able to demonstrate 
    > that the general-purpose AI model does not present, due to its specific characteristics, 
    > systemic risks, it shall reject those arguments, and the general-purpose AI model shall be 
    > considered to be a general-purpose AI model with systemic risk.

- **Was not able to demonstrate ← relevant provider**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 4

    > 3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 
    > are not sufficiently substantiated and the relevant provider was not able to demonstrate 
    > that the general-purpose AI model does not present, due to its specific characteristics, 
    > systemic risks, it shall reject those arguments, and the general-purpose AI model shall be 
    > considered to be a general-purpose AI model with systemic risk.

- **contributes to the development of tools and templates ← certificate**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 4

    > 3. The scientific panel shall advise and support the AI Office, in particular with regard to 
    > the following tasks:
    > (a) supporting the implementation and enforcement of this Regulation as regards 
    > general-purpose AI models and systems, in particular by:
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > 
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (b) supporting the work of market surveillance authorities, at their request;
    > (c) supporting cross-border market surveillance activities as referred to in 
    > Article 74(11), without prejudice to the powers of market surveillance authorities;
    > (d) supporting the AI Office in carrying out its duties in the context of the safeguard 
    > clause pursuant to Article 81.



---

## Node: number of affected persons
<a name="node-number-of-affected-persons"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **takes into account ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.



---

## Node: guidelines issued by
<a name="node-guidelines-issued-by"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Pursuant to Article 96. ← commission**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.



---

## Node: communication with authorities
<a name="node-communication-with-authorities"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Provides information to ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall, upon a reasoned request by a  competent 
    > authority, provide that authority  all the information and documentation necessary to 
    > demonstrate the conformity of the high-risk AI system with the requirements set out in 
    > Section 2, in a language which can be easily understood by the authority in one of the 
    > official languages of the institutions of the Union as indicated by the Member State 
    > concerned.



---

## Node: storage conditions
<a name="node-storage-conditions"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Do not jeopardise compliance ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 4

    > 3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, 
    > where applicable, storage or transport conditions do not jeopardise the compliance of the 
    > system with the requirements set out in Section 2.



---

## Node: legally designated representative
<a name="node-legally-designated-representative"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **related to ← testing in real-world conditions**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.



---

## Node: relevant capacities and limitations
<a name="node-relevant-capacities-and-limitations"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **allows for understanding ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.



---

## Node: data necessary for the identification
<a name="node-data-necessary-for-the-identification"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Involved in ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.



---

## Node: obligations laid down in this regulation
<a name="node-obligations-laid-down-in-this-regulation"></a>

*0 outgoing, 6 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Shall ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Compliance with ← general-purpose ai models**
  - Chapter 13: Final provisions 
  - Article 10: Ai systems already placed on the market or put into service
  - Paragraph 4

    > 3. Providers of general-purpose AI models that have been placed on the market before … 
    > [12 months from the date of entry into force of this Regulation] shall take the necessary 
    > steps in order to comply with the obligations laid down in this Regulation by … [36 
    > months from the date of entry into force of this Regulation].

- **May be accompanied by ← importers**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 4

    > 3. By … [four years from the date of entry into force of this Regulation] and every four years 
    > thereafter, the Commission shall submit a report on the evaluation and review of this 
    > Regulation to the European Parliament and to the Council. The report shall include an 
    > assessment with regard to the structure of enforcement and the possible need for a 
    > Union agency to resolve any identified shortcomings. On the basis of the findings, that 
    > report shall, where appropriate, be accompanied by a proposal for amendment of this 
    > Regulation. The reports shall be made public.

- **Facilitates compliance with simplified manner through ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies

- **Considers that there has been an infringement of the ← natural person**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.

- **Facilitates ← directive (eu) 2022/2557**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.



---

## Node: social score
<a name="node-social-score"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **leads to → detrimental or unfavourable treatment**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.

### Incoming relationships

_(none)_



---

## Node: relevant provider
<a name="node-relevant-provider"></a>

*1 outgoing, 4 incoming*

### Outgoing relationships

- **Was not able to demonstrate → general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 4

    > 3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 
    > are not sufficiently substantiated and the relevant provider was not able to demonstrate 
    > that the general-purpose AI model does not present, due to its specific characteristics, 
    > systemic risks, it shall reject those arguments, and the general-purpose AI model shall be 
    > considered to be a general-purpose AI model with systemic risk.

### Incoming relationships

- **Consults with ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 5

    > 4. The Commission shall without undue delay enter into consultation with the Member State 
    > or member States concerned and the relevant operators, and shall evaluate the national 
    > measures taken. On the basis of the results of that evaluation, the Commission shall decide 
    > whether the measure is justified and, where necessary, propose other appropriate measures.

- **prescribes appropriate corrective action period ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 3

    > 2. Where, in the course of that evaluation, the market surveillance authority finds that the 
    > AI system concerned is high-risk, it shall without undue delay require the relevant 
    > provider to take all necessary actions to bring the AI system into compliance with the 
    > requirements and obligations laid down in this Regulation, as well as take appropriate 
    > corrective action within a period the market surveillance authority may prescribe.

- **Exercise supervisory powers within limits ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 12

    > 11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the 
    > competent authorities supervising the sandboxes, including at regional or local level. Any 
    > significant risks to health and safety and fundamental rights identified during the 
    > development and testing of such AI systems shall result in an adequate mitigation. 
    > National competent authorities shall have the power to temporarily or permanently 
    > suspend the testing process, or the participation in the sandbox if no effective mitigation 
    > is possible, and shall inform the AI Office of such decision. National competent 
    > authorities shall exercise their supervisory powers within the limits of the relevant law, 
    > using their discretionary powers when implementing legal provisions in respect of a 
    > specific AI sandbox project, with the objective of supporting innovation in AI in the 
    > Union.

- **Description of ← expert**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.



---

## Node: detailed arrangements and conditions of evaluations
<a name="node-detailed-arrangements-and-conditions-of-evaluations"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Sets out ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 7

    > 6. The Commission shall adopt implementing acts setting out the detailed arrangements 
    > and the conditions of the evaluations, including the detailed arrangements for involving 
    > independent experts, and the procedure for the selection thereof. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).



---

## Node: operators of high-risk ai systems
<a name="node-operators-of-high-risk-ai-systems"></a>

*59 outgoing, 12 incoming*

### Outgoing relationships

- **Subject to → operators of high-risk ai systems**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Classified as → article 6(1)**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 3

    > 2. For  AI systems classified as high-risk AI systems in accordance with Article 6(1) and

- **Released under → free and open source licences**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 14

    > 12. This Regulation applies to AI systems released under free and open source licences, 
    > unless they are placed on the market or put into service as high-risk AI systems or as an 
    > AI system that falls under Article 5 or 50.

- **Represents → operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 1

    > Article 22
    > Authorised representatives of providers of high-risk AI systems

- **Add or modify use-cases → annex iii**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.

- **Intended to be used in → areas listed in annex iii**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.

- **Pose a risk of harm to → health and safety**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.

- **Pose an adverse impact on → fundamental rights**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.

- **Ensures compliance with → enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

- **affects → residual risk associated with each hazard**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

- **represents → overall residual risk**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

- **provided with → obligations pursuant to article 16**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

- **Intended purpose of → training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **Allows for → automatic recording of events (logs)**
  - Chapter 3: High-risk ai systems
  - Article 7: Record-keeping
  - Paragraph 2

    > 1. High-risk AI systems shall technically allow for the automatic recording of events (‘logs’) 
    > over their lifetime.

- **Provides → high-impact capabilities**
  - Chapter 3: High-risk ai systems
  - Article 7: Record-keeping
  - Paragraph 4

    > 3. For high-risk AI systems referred to in point 1 (a) of Annex III, the logging capabilities 
    > shall provide, at a minimum:
    > (a) recording of the period of each use of the system (start date and time and end date 
    > and time of each use);
    > (b) the reference database against which input data has been checked by the system;
    > (c) the input data for which the search has led to a match;
    > (d) the identification of the natural persons involved in the verification of the results, as 
    > referred to in Article 14(5).

- **Ensures → sufficiently transparent operation**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 2

    > 1. High-risk AI systems shall be designed and developed in such a way as to ensure that their 
    > operation is sufficiently transparent to enable deployers to interpret a system’s output and 
    > use it appropriately. An appropriate type and degree of transparency shall be ensured  
    > with a view to achieving compliance with the relevant obligations of the provider and 
    > deployer set out in Section 3.

- **required for conformity with this Regulation → instructions for use**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).

- **Always considered high-risk → natural person**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 4

    > 3. By derogation from paragraph 2, an AI system shall not be considered to be high-risk if 
    > it does not pose a significant risk of harm to the health, safety or fundamental rights of 
    > natural persons, including by not materially influencing the outcome of decision 
    > making. This shall be the case where one or more of the following conditions are 
    > fulfilled:
    > (a) the AI system is intended to perform a narrow procedural task;
    > (b) the AI system is intended to improve the result of a previously completed human 
    > activity;
    > (c) the AI system is intended to detect decision-making patterns or deviations from 
    > prior decision-making patterns and is not meant to replace or influence the 
    > previously completed human assessment, without proper human review; or
    > (d) the AI system is intended to perform a preparatory task to an assessment relevant 
    > for the purposes of the use cases listed in Annex III.
    > Notwithstanding the first subparagraph, an AI system referred to in Annex III shall 
    > always be considered to be high-risk where the AI system performs profiling of natural 
    > persons.

- **Related to → level of autonomy**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.

- **Becomes aware of risk → providers**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 3

    > 2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and 
    > the provider becomes aware of that risk, it shall immediately investigate the causes, in 
    > collaboration with the reporting deployer, where applicable, and inform the market 
    > surveillance authorities of the Member State or Member States in which they made the 
    > high-risk AI system available on the market and, where applicable, the notified body that 
    > issued a certificate for that high-risk AI system in accordance with Article 44, in 
    > particular, of the nature of the non-compliance and of any relevant corrective action 
    > taken.

- **Considered as → product manufacturer**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 4

    > 3. In the case of high-risk AI systems that are safety components of products covered by the 
    > Union harmonisation legislation listed in Section A of Annex I, the product manufacturer 
    > shall be considered to be the provider of the high-risk AI system, and shall be subject to 
    > the obligations under Article 16 under either of the following circumstances:
    > (a) the high-risk AI system is placed on the market together with the product under the 
    > name or trademark of the product manufacturer;
    > (b) the high-risk AI system is put into service under the name or trademark of the 
    > product manufacturer after the product has been placed on the market.

- **Verifies conformity → notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 2

    > 1. Notified bodies shall verify the conformity of high-risk AI systems in accordance with 
    > the conformity assessment procedures set out in Article 43.

- **Conformity with → harmonised standard**
  - Chapter 3: High-risk ai systems
  - Article 35: Harmonised standards and standardisation deliverables
  - Paragraph 2

    > 1. High-risk AI systems which are in conformity with harmonised standards or parts thereof 
    > the references of which have been published in the Official Journal of the European Union 
    > in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity 
    > with the requirements set out in Section 2 of this Chapter or, as applicable, with the 
    > obligations set out in Chapter IV of this Regulation, to the extent that those standards 
    > cover those requirements or obligations.

- **Meets → requirements set out in this section**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Undergoes in event of substantial modification → conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 5

    > 4. High-risk AI systems that have already been subject to a conformity assessment 
    > procedure shall undergo a new conformity assessment procedure in the event of a 
    > substantial modification, regardless of whether the modified system is intended to be 
    > further distributed or continues to be used by the current deployer.
    > For high-risk AI systems that continue to learn after being placed on the market or put into 
    > service, changes to the high-risk AI system and its performance that have been pre-
    > determined by the provider at the moment of the initial conformity assessment and are part 
    > of the information contained in the technical documentation referred to in point 2(f) of 
    > Annex IV, shall not constitute a substantial modification.

- **drawn up in accordance with Article 11 and Annex IV → partial application**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).

- **Authorises → market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 4

    > 3. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance 
    > authority concludes that the high-risk AI system complies with the requirements of Section

- **Draws up → eu declaration of conformity**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.

- **Affixes → ce marking**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.

- **Keep → logs referred to in article 12(1)**
  - Chapter 3: High-risk ai systems
  - Article 14: Automatically generated logs
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), 
    > automatically generated by their high-risk AI systems, to the extent such logs are under 
    > their control. Without prejudice to applicable Union or national law, the logs shall be 
    > kept for a period  appropriate to the intended purpose of the high-risk AI system, of at 
    > least six months, unless provided otherwise in the applicable Union or national law, in 
    > particular in Union law on the protection of personal data.

- **Referred to in point 2 of Annex III for → national level registration**
  - Chapter 3: High-risk ai systems
  - Article 44: Registration
  - Paragraph 7

    > 5. High-risk AI systems referred to in point 2 of Annex III shall be registered at national 
    > level.

- **Intended to be used as → safety component**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 2

    > 1. Irrespective of whether an AI system is placed on the market or put into service 
    > independently from the products referred to in points (a) and (b), that AI system shall be 
    > considered to be high-risk where both of the following conditions are fulfilled:
    > (a) the AI system is intended to be used as a safety component of a product, or the AI 
    > system is itself a product, covered by the Union harmonisation legislation listed in 
    > Annex I;
    > (b) the product whose safety component pursuant to point (a) is the AI system, or the 
    > AI system itself as a product, is required to undergo a third-party conformity 
    > assessment, with a view to the placing on the market or the putting into service of 
    > that product pursuant to the Union harmonisation legislation listed in Annex I.

- **Performs → narrow procedural task**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 4

    > 3. By derogation from paragraph 2, an AI system shall not be considered to be high-risk if 
    > it does not pose a significant risk of harm to the health, safety or fundamental rights of 
    > natural persons, including by not materially influencing the outcome of decision 
    > making. This shall be the case where one or more of the following conditions are 
    > fulfilled:
    > (a) the AI system is intended to perform a narrow procedural task;
    > (b) the AI system is intended to improve the result of a previously completed human 
    > activity;
    > (c) the AI system is intended to detect decision-making patterns or deviations from 
    > prior decision-making patterns and is not meant to replace or influence the 
    > previously completed human assessment, without proper human review; or
    > (d) the AI system is intended to perform a preparatory task to an assessment relevant 
    > for the purposes of the use cases listed in Annex III.
    > Notwithstanding the first subparagraph, an AI system referred to in Annex III shall 
    > always be considered to be high-risk where the AI system performs profiling of natural 
    > persons.

- **Associated with → risk**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.

- **Can mitigate or eliminate → development or design**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 4

    > 3. The risks referred to in this Article shall concern only those which may be reasonably 
    > mitigated or eliminated through the development or design of the high-risk AI system, or 
    > the provision of adequate technical information.

- **may have adverse impact on → persons under the age of 18**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 10

    > 9. When implementing the risk management system as provided for in paragraphs 1 to 7, 
    > providers shall give consideration to whether in view of its intended purpose the high-risk 
    > AI system is likely to have an adverse impact on persons under the age of 18 and, as 
    > appropriate, other groups of vulnerable persons.

- **may have adverse impact on → other groups of vulnerable persons**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 10

    > 9. When implementing the risk management system as provided for in paragraphs 1 to 7, 
    > providers shall give consideration to whether in view of its intended purpose the high-risk 
    > AI system is likely to have an adverse impact on persons under the age of 18 and, as 
    > appropriate, other groups of vulnerable persons.

- **Dependent on → context of use**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.

- **allows for understanding → relevant capacities and limitations**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **facilitates detection of → anomalies, dysfunctions and unexpected performance**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **creates tendency for → automation bias**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **allows for decision whether to use or disregard output → regulation**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **interrupts through a stop button or similar procedure → union or member states**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **Ensures keeping under their control → operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.

- **Inform → deployers**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Under responsibility → distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 4

    > 3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, 
    > where applicable, storage or transport conditions do not jeopardise the compliance of the 
    > system with the requirements set out in Section 2.

- **Do not jeopardise compliance → storage conditions**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 4

    > 3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, 
    > where applicable, storage or transport conditions do not jeopardise the compliance of the 
    > system with the requirements set out in Section 2.

- **provides information to → national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 6

    > 5. Upon a reasoned request from a national competent authority, distributors of a high-risk 
    > AI system shall provide that authority with all the information and documentation 
    > regarding its actions pursuant to paragraphs 1 to 4 necessary to demonstrate the 
    > conformity of that system with the requirements set out in Section 2.

- **putting into service → law-enforcement authorities or civil protection authorities**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 3

    > 2. In a duly justified situation of urgency for exceptional reasons of public security or in 
    > the case of specific, substantial and imminent threat to the life or physical safety of 
    > natural persons, law-enforcement authorities or civil protection authorities may put a 
    > specific high-risk AI system into service without the authorisation referred to in 
    > paragraph 1, provided that such authorisation is requested during or after the use 
    > without undue delay. If the authorisation referred to in paragraph 1 is refused, the use 
    > of the high-risk AI system shall be stopped with immediate effect and all the results and 
    > outputs of such use shall be immediately discarded.

- **Exceptionally processed → special categories of personal data**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 6

    > 5. To the extent that it is strictly necessary for the purpose of ensuring bias  detection and 
    > correction in relation to the high-risk AI systems in accordance with paragraph (2), points
    > (f) and (g) of this Article, the providers of such systems may exceptionally process special 
    > categories of personal data, subject to appropriate safeguards for the fundamental rights 
    > and freedoms of natural persons. In addition to the provisions set out in Regulation (EU) 
    > 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following 
    > conditions shall apply in order for such processing to occur:
    > (a) the bias detection and correction cannot be effectively fulfilled by processing other 
    > data, including synthetic or anonymised data;
    > (b) the special categories of personal data are subject to technical limitations on the 
    > re-use of the personal data, and state of the art security and privacy-preserving 
    > measures, including pseudonymisation;
    > (c) the special categories of personal data are subject to measures to ensure that the 
    > personal data processed are secured, protected, subject to suitable safeguards, 
    > including strict controls and documentation of the access, to avoid misuse and 
    > ensure that only authorised persons with appropriate confidentiality obligations 
    > have access to those personal data;
    > (d) the personal data in the special categories of personal data are not to be 
    > transmitted, transferred or otherwise accessed by other parties;
    > (e) the personal data in the special categories of personal data are deleted once the 
    > bias has been corrected or the personal data has reached the end of its retention 
    > period, whichever comes first;
    > (f) the records of processing activities pursuant to Regulations (EU) 2016/679 and 
    > (EU) 2018/1725 and Directive (EU) 2016/680include the reasons why the 
    > processing of special categories of personal data was strictly necessary to detect 
    > and correct biases, and why that objective could not be achieved by processing 
    > other data.

- **Immediately take → collective agreements**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Inform → authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Inform → importers**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Provides information to → communication with authorities**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall, upon a reasoned request by a  competent 
    > authority, provide that authority  all the information and documentation necessary to 
    > demonstrate the conformity of the high-risk AI system with the requirements set out in 
    > Section 2, in a language which can be easily understood by the authority in one of the 
    > official languages of the institutions of the Union as indicated by the Member State 
    > concerned.

- **Requires → digital ce marking**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 3

    > 2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it 
    > can easily be accessed via the interface from which that system is accessed or via an 
    > easily accessible machine-readable code or other electronic means.

- **Accessible via → interface**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 3

    > 2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it 
    > can easily be accessed via the interface from which that system is accessed or via an 
    > easily accessible machine-readable code or other electronic means.

- **Requires post-market monitoring to ensure compliance with requirements → providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Ensures recall or withdrawal from the market → market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 3

    > 2. Where the non-compliance referred to in paragraph 1 persists, the market surveillance 
    > authority of the Member State concerned shall take appropriate and proportionate 
    > measures to restrict or prohibit the high-risk AI system being made available on the market 
    > or to ensure that it is recalled or withdrawn from the market without delay.

- **Subject to → general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 1

    > Article 75
    > Mutual assistance, market surveillance and control of general-purpose AI systems

### Incoming relationships

- **Subject to ← operators of high-risk ai systems**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Used in ← ai systems**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Related to ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Informs ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 1

    > Article 75
    > Mutual assistance, market surveillance and control of general-purpose AI systems

- **Misclassification ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 2

    > 1. Where a market surveillance authority has sufficient reason to consider that an AI 
    > system classified by the provider is not high-risk pursuant to Article 6(3)I is indeed high-
    > risk, the market surveillance authority shall carry out an evaluation of the AI system 
    > concerned in respect of its classification as a high-risk AI system based on the conditions 
    > set out in Article 6(3) and the Commission guidelines.

- **Represents ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 1

    > Article 22
    > Authorised representatives of providers of high-risk AI systems

- **Ensures keeping under their control ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.

- **Independent of ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 5

    > 4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to 
    > which they perform conformity assessment activities. Notified bodies shall also be 
    > independent of any other operator having an economic interest in high-risk AI systems 
    > assessed, as well as of any competitors of the provider. This shall not preclude the use of 
    > assessed high-risk AI systems that are necessary for the operations of the conformity 
    > assessment body, or the use of such high-risk AI systems for personal purposes.

- **Collects and analyzes data throughout lifetime for compliance ← market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Appropriate to ← provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 7

    > 6. Deployers of high-risk AI systems shall keep the logs automatically generated by that 
    > high-risk AI system  to the extent such logs are under their control,  for a period  
    > appropriate to the intended purpose of the high-risk AI system, of at least six months, 
    > unless provided otherwise in applicable Union or national law, in particular in Union law 
    > on the protection of personal data.
    > Deployers that are financial institutions subject to requirements regarding their internal 
    > governance, arrangements or processes under Union financial services law shall 
    > maintain the logs as part of the documentation kept pursuant to the relevant Union 
    > financial service law.

- **appointed in accordance with Article 22(1) ← authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 2

    > 1. Before placing a high-risk AI system on the market, importers shall ensure that the system 
    > is in conformity with this Regulation by verifying that:
    > (a) the relevant conformity assessment procedure referred to in Article 43 has been 
    > carried out by the provider of the high-risk AI system;
    > (b) the provider has drawn up the technical documentation in accordance with Article 11 
    > and Annex IV;
    > (c) the system bears the required CE marking and is accompanied by the EU declaration 
    > of conformity and instructions for use;
    > (d) the provider has appointed an authorised representative in accordance with 
    > Article 22(1).

- **Causal relationship ← serious incident**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 5

    > 5. Notwithstanding paragraph 2, in the event of the death of a person, the report shall be 
    > provided immediately after the provider or the deployer has established, or as soon as it 
    > suspects, a causal relationship between the high-risk AI system and the serious incident, 
    > but not later than 10 days after the date on which the provider or, where applicable, the 
    > deployer becomes aware of the serious incident.



---

## Node: purpose of request
<a name="node-purpose-of-request"></a>

*2 outgoing, 0 incoming*

### Outgoing relationships

- **Defines → provision of information**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

- **Intention behind → request for access**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.

### Incoming relationships

_(none)_



---

## Node: (a) strategy for regulatory compliance
<a name="node-a-strategy-for-regulatory-compliance"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Includes ← enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.



---

## Node: testing
<a name="node-testing"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **provided → deployers**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

### Incoming relationships

- **performs ← conformity assessment body**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 22

    > (21) ‘conformity assessment body’ means a body that performs third-party conformity 
    > assessment activities, including testing, certification and inspection;



---

## Node: registration in the eu database
<a name="node-registration-in-the-eu-database"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Non-carrying out requires non-compliance to end ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.



---

## Node: obligations pursuant to article 16
<a name="node-obligations-pursuant-to-article-16"></a>

*0 outgoing, 7 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **comply with ← importers**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Takes account of ← commission**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.

- **perform ← distributors**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **are subject to ← deployers**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **undergoes ← providers**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **undertake ← authorised representatives of providers**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **provided with ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.



---

## Node: fair gender and geographical representation
<a name="node-fair-gender-and-geographical-representation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← commission**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.



---

## Node: easily corrigible or reversible
<a name="node-easily-corrigible-or-reversible"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Taking into account ← outcome produced involving an ai system**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 3

    > 2. When assessing the condition under paragraph 1, point (b),, the Commission shall take into 
    > account the following criteria:
    > (a) the intended purpose of the AI system;
    > (b) the extent to which an AI system has been used or is likely to be used;
    > (c) the nature and amount of the data processed and used by the AI system, in 
    > particular whether special categories of personal data are processed;
    > (d) the extent to which the AI system acts autonomously and the possibility for a 
    > human to override a decision or recommendations that may lead to potential harm;
    > (e) the extent to which the use of an AI system has already caused harm to  health and 
    > safety, has had an adverse impact on  fundamental rights or has given rise to 
    > significant concerns in relation to the likelihood of such harm or adverse impact, as 
    > demonstrated, for example, by reports or documented allegations submitted to 
    > national competent authorities or by other reports, as appropriate;
    > (f) the potential extent of such harm or such adverse impact, in particular in terms of its 
    > intensity and its ability to affect multiple persons or to disproportionately affect a 
    > particular group of persons;
    > (g) the extent to which persons who are potentially harmed or suffer an adverse impact 
    > are dependent on the outcome produced with an AI system, in particular because for 
    > practical or legal reasons it is not reasonably possible to opt-out from that outcome;
    > (h) the extent to which there is an imbalance of power, or the persons who are 
    > potentially harmed or suffer an adverse impact are in a vulnerable position in relation 
    > to the deployer of an AI system, in particular due to status, authority, knowledge, 
    > economic or social circumstances, or age;
    > (i) the extent to which the outcome produced involving an AI system is easily corrigible 
    > or reversible, taking into account the technical solutions available to correct or 
    > reverse it, whereby outcomes having an adverse impact on  health, safety or 
    > fundamental rights, shall not be considered to be easily corrigible or reversible;
    > (j) the magnitude and likelihood of benefit of the deployment of the AI system for 
    > individuals, groups, or society at large, including possible improvements in product 
    > safety;
    > (k) the extent to which existing Union law provides for:
    > (i) effective measures of redress in relation to the risks posed by an AI system, 
    > with the exclusion of claims for damages;
    > (ii) effective measures to prevent or substantially minimise those risks.



---

## Node: serious incident
<a name="node-serious-incident"></a>

*5 outgoing, 2 incoming*

### Outgoing relationships

- **reporting obligation → national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **Occurrence → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 12

    > 12. National competent authorities shall immediately notify the Commission of any serious 
    > incident, whether or not it they have taken action on it, in accordance with Article 20 of 
    > Regulation (EU) 2019/1020.
    > Section 3
    > Enforcement

- **Causal relationship → operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 5

    > 5. Notwithstanding paragraph 2, in the event of the death of a person, the report shall be 
    > provided immediately after the provider or the deployer has established, or as soon as it 
    > suspects, a causal relationship between the high-risk AI system and the serious incident, 
    > but not later than 10 days after the date on which the provider or, where applicable, the 
    > deployer becomes aware of the serious incident.

- **becomes aware of → authorised representatives of providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 4

    > 3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement 
    > or a serious incident as defined in Article 3, point (44) (b), the report referred to in 
    > paragraph 1 of this Article shall be provided immediately, and not later than two days 
    > after the provider or, where applicable, the deployer becomes aware of that incident.

- **Notifies → public authorities in a third country**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 8

    > 8. Upon receiving a notification related to a serious incident referred to in Article 3, point 
    > (44)(c), the relevant market surveillance authority shall inform the national public 
    > authorities or bodies referred to in Article 77(1). The Commission shall develop dedicated 
    > guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. 
    > That guidance shall be issued by … [12 months after the entry into force of this 
    > Regulation], and shall be assessed regularly.

### Incoming relationships

- **Causal link between ← ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 3

    > 2. The report referred to in paragraph 1 shall be made immediately after the provider has 
    > established a causal link between the AI system and the serious incident or  the 
    > reasonable likelihood of such a link, and, in any event, not later than 15 days after the 
    > provider or, where applicable, the deployer, becomes aware of the serious incident.
    > The period for the reporting referred to in the first subparagraph shall take account of 
    > the severity of the serious incident  .

- **Include ← financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.



---

## Node: context of use
<a name="node-context-of-use"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Dependent on ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 4

    > 3. The oversight measures shall be commensurate to the risks, level of autonomy and 
    > context of use of the high-risk AI system, and shall be ensured through either one or both 
    > of the following types of measures:
    > (a) measures identified and built, when technically feasible, into the high-risk AI system 
    > by the provider before it is placed on the market or put into service;
    > (b) measures identified by the provider before placing the high-risk AI system on the 
    > market or putting it into service and that are appropriate to be implemented by the 
    > deployer.



---

## Node: providers of intermediary services
<a name="node-providers-of-intermediary-services"></a>

*3 outgoing, 2 incoming*

### Outgoing relationships

- **Affected by → liability**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 7

    > 5. This Regulation shall not affect the application of the provisions on the liability of 
    > providers of intermediary services as set out in Chapter II of Regulation (EU) 2022/2065.

- **Fails to take → appropriate corrective action**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 7

    > 6. Where the provider of the AI system concerned does not take adequate corrective action 
    > within the period referred to in paragraph 2 of this Article, Article 79(5) to (9) shall 
    > apply.

- **Makes available on the Union market → types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 3

    > 2. The provider or other relevant operator shall ensure that corrective action is taken in 
    > respect of all the AI systems concerned that it has made available on the Union market 
    > within the timeline prescribed by the market surveillance authority of the Member State 
    > referred to in paragraph 1.

### Incoming relationships

- **Recalls from national market ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.

- **Related to ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.



---

## Node: individual decision-making
<a name="node-individual-decision-making"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Performs → right to explanation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 15: Right to explanation of individual decision-making
  - Paragraph 1

    > Article 86
    > Right to explanation of individual decision-making

### Incoming relationships

_(none)_



---

## Node: national competent authorities
<a name="node-national-competent-authorities"></a>

*26 outgoing, 14 incoming*

### Outgoing relationships

- **Subject to → administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 7

    > 5. The supply of incorrect, incomplete or misleading information to notified bodies or 
    > national competent authorities in reply to a request shall be subject to administrative fines 
    > of up to 7 500 000 EUR or, if the offender is an undertaking, up to 1 % of its total 
    > worldwide annual turnover for the preceding financial year, whichever is higher.

- **requires → financial resources**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **focuses on → importers**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **Determines conditions for documentation availability → union or member states**
  - Chapter 3: High-risk ai systems
  - Article 13: Documentation keeping
  - Paragraph 3

    > 2. Each Member State shall determine conditions under which the documentation referred 
    > to in paragraph 1 remains at the disposal of the national competent authorities for the 
    > period indicated in that paragraph for the cases when a provider or its authorised 
    > representative established on its territory goes bankrupt or ceases its activity prior to the 
    > end of that period.

- **Cooperate in action taken → importers**
  - Chapter 3: High-risk ai systems
  - Article 18: Obligations of importers
  - Paragraph 8

    > 7. Importers shall cooperate with national competent authorities in any action those 
    > authorities take in relation to a high-risk AI system the importers placed on the market, 
    > in particular to reduce and mitigate the risks posed by it.

- **Cooperate in any action → distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 7

    > 6. Distributors shall cooperate with national competent authorities in any action those 
    > authorities take in relation to a high-risk AI system they made available on the market, 
    > in particular to reduce or mitigate the risk posed by it.

- **Translated into a language that can be easily understood by → eu declaration of conformity**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Performs → proportional**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 1

    > Article 21
    > Cooperation with competent authorities

- **provided with contact details and documentation by authorised representative → market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 4

    > 3. The authorised representative shall perform the tasks specified in the mandate received 
    > from the provider. It shall provide a copy of the mandate to the market surveillance 
    > authorities upon request, in one of the official languages of the institutions of the 
    > Union, as indicated by the national competent authority. For the purposes of this 
    > Regulation, the mandate shall empower the authorised representative to carry out the 
    > following tasks:
    > (a) verify that the EU declaration of conformity and the technical documentation 
    > referred to in Article 11 have been drawn up and that an appropriate conformity 
    > assessment procedure has been carried out by the provider;
    > (b) keep at the disposal of the national competent authorities and national authorities 
    > or bodies referred to in Article 74(10), for a period of 10 years after the high-risk 
    > AI system has been placed on the market or put into service, the contact details of 
    > the provider that appointed the authorised representative, a copy of the EU 
    > declaration of conformity, the technical documentation and, if applicable, the 
    > certificate issued by the notified body;
    > (c) provide a national competent authority, upon a reasoned request, with all the 
    > information and documentation, including that referred to in point (b) of this 
    > subparagraph, necessary to demonstrate the conformity of a high-risk AI system 
    > with the requirements set out in Section 2, including access to the logs, as referred to 
    > in Article 12(1), automatically generated by the high-risk AI system, to the extent 
    > such logs are under the control of the provider ;
    > (d) cooperate with competent  authorities, upon a reasoned request, in any action the 
    > latter take in relation to the high-risk AI system, in particular to reduce and mitigate 
    > the risks posed by the high-risk AI system;
    > (e) where applicable, comply with the registration obligations referred in Article 49(1), 
    > or, if the registration is carried out by the provider itself, ensure that the 
    > information referred to in Section A of Annex VIII is correct.
    > The mandate shall empower the authorised representative to be addressed, in addition to 
    > or instead of the provider, by the competent authorities, on all issues related to ensuring 
    > compliance with this Regulation.

- **Confirmed that there is no → types of ai systems concerned**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 10

    > 9. With the exception of certificates unduly issued, and where a designation has been 
    > withdrawn, the certificates shall remain valid for a period of nine months under the 
    > following circumstances:
    > (a) the national competent authority of the Member State in which the provider of the 
    > AI system covered by the certificate has its registered place of business has 
    > confirmed that there is no risk to health, safety or fundamental rights associated 
    > with the high-risk AI systems concerned; and
    > (b) another notified body has confirmed in writing that it will assume immediate 
    > responsibilities for assessing those AI systems and completes its assessment within

- **Compliance monitoring → conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 1

    > Article 43
    > Conformity assessment

- **Maintained for five years after subcontracting activity termination date → relevant documents concerning assessment of qualifications**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 5

    > 4. The relevant documents concerning the assessment of the qualifications of the 
    > subcontractor or the subsidiary and the work carried out by them under this Regulation 
    > shall be kept at the disposal of the notifying authority for a period of five years from the 
    > termination date of the subcontracting activity.

- **Cooperate with → national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 5

    > 4. Member States shall ensure that the competent authorities referred to in paragraphs 1 
    > and 2 allocate sufficient resources to comply with this Article effectively and in a timely 
    > manner. Where appropriate, national competent authorities shall cooperate with other 
    > relevant authorities, and may allow for the involvement of other actors within the AI 
    > ecosystem. This Article shall not affect other regulatory sandboxes established under 
    > Union or national law. Member States shall ensure an appropriate level of cooperation 
    > between the authorities supervising those other sandboxes and the national competent 
    > authorities.

- **Associated with the operation of → european data protection supervisor**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 11

    > 10. National competent authorities shall ensure that, to the extent the innovative AI systems 
    > involve the processing of personal data or otherwise fall under the supervisory remit of 
    > other national authorities or competent authorities providing or supporting access to data, 
    > the national data protection authorities and those other national or competent authorities 
    > are associated with the operation of the AI regulatory sandbox and involved in the 
    > supervision of those aspects to the extent of their respective tasks and powers.

- **Exercise supervisory powers within limits → relevant provider**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 12

    > 11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the 
    > competent authorities supervising the sandboxes, including at regional or local level. Any 
    > significant risks to health and safety and fundamental rights identified during the 
    > development and testing of such AI systems shall result in an adequate mitigation. 
    > National competent authorities shall have the power to temporarily or permanently 
    > suspend the testing process, or the participation in the sandbox if no effective mitigation 
    > is possible, and shall inform the AI Office of such decision. National competent 
    > authorities shall exercise their supervisory powers within the limits of the relevant law, 
    > using their discretionary powers when implementing legal provisions in respect of a 
    > specific AI sandbox project, with the objective of supporting innovation in AI in the 
    > Union.

- **Implement legal provisions with discretionary powers → specific ai sandbox project**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 12

    > 11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the 
    > competent authorities supervising the sandboxes, including at regional or local level. Any 
    > significant risks to health and safety and fundamental rights identified during the 
    > development and testing of such AI systems shall result in an adequate mitigation. 
    > National competent authorities shall have the power to temporarily or permanently 
    > suspend the testing process, or the participation in the sandbox if no effective mitigation 
    > is possible, and shall inform the AI Office of such decision. National competent 
    > authorities shall exercise their supervisory powers within the limits of the relevant law, 
    > using their discretionary powers when implementing legal provisions in respect of a 
    > specific AI sandbox project, with the objective of supporting innovation in AI in the 
    > Union.

- **Facilitates cross-border cooperation → ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 14

    > 13. The AI regulatory sandboxes shall be designed and implemented in such a way that, 
    > where relevant, they facilitate cross-border cooperation between national competent 
    > authorities.

- **Submits annual reports → board**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Submits annual reports → ai office**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Makes annual reports or abstracts thereof available → public**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Cooperate with other national competent authorities → consistent practices across the union**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 5

    > 4. Where national competent authorities consider authorising testing in real world 
    > conditions supervised within the framework of an AI regulatory sandbox to be 
    > established under this Article, they shall specifically agree with the participants on the 
    > terms and conditions of such testing and in particular on the appropriate safeguards 
    > with a view to protecting fundamental rights, health and safety. Where appropriate, they 
    > shall cooperate with other national competent authorities with a view to ensuring 
    > consistent practices across the Union.

- **Agrees to access → financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **Takes → adequate level of cybersecurity measures**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 5

    > 4. National competent authorities shall take an adequate level of cybersecurity measures.

- **Complies with → confidentiality of information and data obtained**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 6

    > 5. When performing their tasks, the national competent authorities shall act in compliance 
    > with the confidentiality obligations set out in Article 78.

- **Provide guidance and advice to → smes including start-ups**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 9

    > 8. National competent authorities may provide guidance and advice on the implementation of 
    > this Regulation, in particular to SMEs including start-ups, taking into account the 
    > guidance and advice of the Board and the Commission, as appropriate. Whenever 
    > national competent authorities intend to provide guidance and advice with regard to an AI 
    > system in areas covered by other Union law, the competent national authorities under that 
    > Union law shall be consulted, as appropriate.

- **Designated by → single point of contact**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 1

    > Article 70
    > Designation of national competent authorities and single point of contact

### Incoming relationships

- **Cooperates with ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 4

    > 3. Providers of general-purpose AI models shall cooperate as necessary with the 
    > Commission and the national competent authorities in the exercise of their competences 
    > and powers pursuant to this Regulation.

- **Facilitates exchange of experience ← commission**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 8

    > 7. The Commission shall facilitate the exchange of experience between national competent 
    > authorities.

- **Controlled and responsible by ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 3: Further processing of personal data for developing certain ai systems
  - Paragraph 3

    > 2. For the purposes of the prevention, investigation, detection or prosecution of criminal 
    > offences or the execution of criminal penalties, including safeguarding against and 
    > preventing prevention threats to public security, under the control and responsibility of 
    > law enforcement authorities, the processing of personal data in AI regulatory sandboxes 
    > shall be based on a specific or Union or national law and subject to the same cumulative 
    > conditions as referred to in paragraph 1.

- **Ensure ← union or member states**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 2

    > 1. Member States shall ensure that their competent authorities establish at least one AI 
    > regulatory sandbox at national level, which shall be operational by … [24 months from 
    > the date of entry into force of this Regulation]. That sandbox may also be established 
    > jointly with the competent authorities of one or more other Member States. The 
    > Commission may provide technical support, advice and tools for the establishment and 
    > operation of AI regulatory sandboxes.
    > The obligation under the first subparagraph may also be fulfilled by participating in an 
    > existing sandbox in so far as that participation provides an equivalent level of national 
    > coverage for the participating Member States.

- **report annually to ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 3

    > 2. As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020, 
    > the market surveillance authorities shall report annually to the Commission and relevant 
    > national competition authorities any information identified in the course of market 
    > surveillance activities that may be of potential interest for the application of Union law on 
    > competition rules. They shall also annually report to the Commission about the use of 
    > prohibited practices that occurred during that year and about the measures taken.

- **Cooperate with ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 5

    > 4. Member States shall ensure that the competent authorities referred to in paragraphs 1 
    > and 2 allocate sufficient resources to comply with this Article effectively and in a timely 
    > manner. Where appropriate, national competent authorities shall cooperate with other 
    > relevant authorities, and may allow for the involvement of other actors within the AI 
    > ecosystem. This Article shall not affect other regulatory sandboxes established under 
    > Union or national law. Member States shall ensure an appropriate level of cooperation 
    > between the authorities supervising those other sandboxes and the national competent 
    > authorities.

- **provides information to ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 6

    > 5. Upon a reasoned request from a national competent authority, distributors of a high-risk 
    > AI system shall provide that authority with all the information and documentation 
    > regarding its actions pursuant to paragraphs 1 to 4 necessary to demonstrate the 
    > conformity of that system with the requirements set out in Section 2.

- **establishes or designates ← union or member states**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 2

    > 1. Each Member State shall establish or designate as national competent authorities at least 
    > one notifying authority and at least one market surveillance authority for the purposes of 
    > this Regulation. Those national competent authorities shall exercise their powers 
    > independently, impartially and without bias so as to safeguard the objectivity of their 
    > activities and tasks, and to ensure the application and implementation of this Regulation. 
    > The members of those authorities shall refrain from any action incompatible with their 
    > duties. Provided that those principles are observed, such activities and tasks may be 
    > performed by one or more designated authorities, in accordance with the organisational 
    > needs of the Member State.

- **Makes ← distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 1

    > Article 24
    > Obligations of distributors

- **Gives access to automatically generated logs ← providers**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 3

    > 2. Upon a reasoned request by a national competent authority, providers shall also give the 
    > requesting national competent authority, as applicable, access to the automatically 
    > generated logs of the high-risk AI system referred to in Article 12(1), to the extent such 
    > logs are under their control.

- **Cooperate with ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 14

    > 12. Deployers shall cooperate with the relevant national competent authorities in any action 
    > those authorities take in relation to the high-risk AI system in order to implement this 
    > Regulation.

- **Addressable by ← authorised representatives of providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 5

    > 3. The mandate shall empower the authorised representative to be addressed, in addition to 
    > or instead of the provider, by the AI Office or the national competent authorities, on all 
    > issues related to ensuring compliance with this Regulation.

- **reporting obligation ← serious incident**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **Notifies ← authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 9

    > 8. Providers or prospective providers shall notify the national market surveillance authority 
    > in the Member State where the testing in real world conditions is to be conducted of the 
    > suspension or termination of the testing in real world conditions and of the final 
    > outcomes.



---

## Node: occurrence of harm
<a name="node-occurrence-of-harm"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **combination with ← risk**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 3

    > (2) ‘risk’ means the combination of the probability of an occurrence of harm and the 
    > severity of that harm;



---

## Node: specific requirements
<a name="node-specific-requirements"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **On the basis of → ai systems**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Permitted under certain conditions → regulation**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 1

    > Article 63
    > Derogations for specific operators

### Incoming relationships

- **Implements ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 1

    > Article 31
    > Requirements relating to notified bodies

- **Satisfies ← article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 37: Presumption of conformity with certain requirements
  - Paragraph 1

    > Article 42
    > Presumption of conformity with certain requirements



---

## Node: lessons learnt
<a name="node-lessons-learnt"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Include ← financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.



---

## Node: transparency
<a name="node-transparency"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 1

    > Article 13
    > Transparency and provision of information to deployers



---

## Node: information technologies
<a name="node-information-technologies"></a>

*1 outgoing, 3 incoming*

### Outgoing relationships

- **Specifies → provision of information**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

### Incoming relationships

- **national market authorities shall safeguard the confidentiality of ← ai office**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 4: Mutual assistance, market surveillance and control of general-purpose ai systems
  - Paragraph 4

    > 3. Where a national market surveillance authority is unable to conclude its investigation of 
    > the high-risk AI system because of its inability to access certain information related to 
    > the AI model despite having made all appropriate efforts to obtain that information, it 
    > may submit a reasoned request to the AI Office, by which access to that information 
    > shall be enforced. In that case, the AI Office shall supply to the applicant authority 
    > without delay, and in any event within 30 days, any information that the AI Office 
    > considers to be relevant in order to establish whether a high-risk AI system is non-
    > compliant. National market authorities shall safeguard the confidentiality of the 
    > information they obtain in accordance with Article 78 of this Regulation. The procedure 
    > provided for in Chapter VI of Regulation (EU) 2019/1020 shall apply mutatis mutandis.

- **Proportional to ← market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Expertise in fields such as ← competent personnel**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.



---

## Node: collective agreements
<a name="node-collective-agreements"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **Encouraging or allowing → union or member states**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

### Incoming relationships

- **Immediately take ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 2

    > 1. Providers of high-risk AI systems which consider or have reason to consider that a high-
    > risk AI system that they have placed on the market or put into service is not in conformity 
    > with this Regulation shall immediately take the necessary corrective actions to bring that 
    > system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They 
    > shall inform the distributors of the high-risk AI system concerned and, where applicable, 
    > the deployers, the authorised representative and importers accordingly.

- **Required to undertake ← authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 1

    > Article 20
    > Corrective actions and duty of information



---

## Node: requirements set out in this section
<a name="node-requirements-set-out-in-this-section"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Facilitate consistency and coordination between national competent authorities ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **Meets ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Meets ← eu declaration of conformity**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Supports ← certificate**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 2

    > 1. Member States may call upon experts of the scientific panel to support their enforcement 
    > activities under this Regulation.



---

## Node: ai systems or ai models
<a name="node-ai-systems-or-ai-models"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Sole purpose → scientific research and development**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 8

    > 6. This Regulation does not apply to AI systems or AI models, including their output, 
    > specifically developed and put into service for the sole purpose of scientific research and 
    > development.

- **Valid for period indicated → certificate**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 3

    > 2. Certificates shall be valid for the period they indicate, which shall not exceed five years for 
    > AI systems covered by Annex I, and four years for AI systems covered by Annex III. On 
    > the application of the provider, the validity of a certificate may be extended for further 
    > periods, each not exceeding five years for AI systems covered by Annex I, and four years 
    > for AI systems covered by Annex III, based on a re-assessment in accordance with the 
    > applicable conformity assessment procedures. Any supplement to a certificate shall 
    > remain valid, provided that the certificate which it supplements is valid.

### Incoming relationships

- **More favourable to ← importers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **Protecting ← workers' rights**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.



---

## Node: quality management system approvals
<a name="node-quality-management-system-approvals"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **refuses, suspends or withdraws ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.



---

## Node: testing in real-world conditions
<a name="node-testing-in-real-world-conditions"></a>

*4 outgoing, 5 incoming*

### Outgoing relationships

- **related to → legally designated representative**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.

- **requires → informed consent**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.

- **related to → personal data**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 6

    > 5. Any subjects of the testing in real world conditions, or their legally designated 
    > representative, as appropriate, may, without any resulting detriment and without having 
    > to provide any justification, withdraw from the testing at any time by revoking their 
    > informed consent and may request the immediate and permanent deletion of their 
    > personal data. The withdrawal of the informed consent shall not affect the lawfulness or 
    > validity of activities already carried out.

- **Conducted by → authorised representatives of providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 3

    > 2. Where testing in real world conditions is conducted for AI systems that are supervised 
    > within an AI regulatory sandbox under Article 59, the market surveillance authorities 
    > shall verify the compliance with the provisions of Article 60 as part of their supervisory 
    > role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the 
    > testing in real world conditions to be conducted by the provider or prospective provider, 
    > in derogation from the conditions set out in Article 60(4), points (f) and (g).

### Incoming relationships

- **Tested in ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 58

    > (57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its 
    > intended purpose in real-world conditions outside a laboratory or otherwise simulated 
    > environment, with a view to gathering reliable and robust data and to assessing and 
    > verifying the conformity of the AI system with the requirements of this Regulation and it 
    > is not considered to be placing the AI system on the market or putting it into service 
    > within the meaning of this Regulation, provided that all the conditions laid down in 
    > Article 57 or 60 are fulfilled;

- **participates in ← subject**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 60

    > (59) ‘informed consent’ means a subject's freely given, specific, unambiguous and voluntary 
    > expression of his or her willingness to participate in a particular testing in real-world 
    > conditions, after having been informed of all aspects of the testing that are relevant to 
    > the subject's decision to participate;

- **Testing in real world conditions supervised in the sandbox. ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 6

    > 5. AI regulatory sandboxes established under paragraph (1) shall provide for a controlled 
    > environment that fosters innovation and facilitates the development, training, testing and 
    > validation of innovative AI systems for a limited time before their being placed on the 
    > market or put into service pursuant to a specific sandbox plan agreed between the 
    > prospective providers and the competent authority. Such regulatory sandboxes may 
    > include testing in real world conditions supervised in the sandbox.

- **Ensures that ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 5: Supervision of testing in real world conditions by market surveillance authorities
  - Paragraph 2

    > 1. Market surveillance authorities shall have competences and powers to ensure that 
    > testing in real world conditions is in accordance with this Regulation.

- **Tested in ← innovative ai systems**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 4

    > 3. The testing of high-risk AI systems in real world conditions under this Article shall be 
    > without prejudice to any ethical review that is required by Union or national law.



---

## Node: downstream provider
<a name="node-downstream-provider"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Complaint against → general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 18: Monitoring actions
  - Paragraph 3

    > 2. Downstream providers shall have the right to lodge a complaint alleging an 
    > infringement of this Regulation. A complaint shall be duly reasoned and indicate at 
    > least:
    > (a) the point of contact of the provider of the general-purpose AI model concerned;
    > (b) a description of the relevant facts, the provisions of this Regulation concerned, and 
    > the reason why the downstream provider considers that the provider of the general-
    > purpose AI model concerned infringed this Regulation;
    > (c) any other information that the downstream provider that sent the request considers 
    > relevant, including, where appropriate, information gathered on its own initiative.

- **may support → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

### Incoming relationships

- **integrates ← ai model**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 69

    > (68) ‘downstream provider’ means a provider of an AI system, including a general-purpose 
    > AI system, which integrates an AI model, regardless of whether the model is provided by 
    > themselves and vertically integrated or provided by another entity based on contractual 
    > relations.

- **Takes due account of ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 3

    > 2. Notified bodies shall avoid unnecessary burdens for providers when performing their 
    > activities, and take due account of the size of the provider, the sector in which it 
    > operates, its structure and the degree of complexity of the high-risk AI system 
    > concerned, in particular in view of minimising administrative burdens and compliance 
    > costs for micro- and small enterprises within the meaning of Recommendation 
    > 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the 
    > level of protection required for the compliance of the high-risk AI system with the 
    > requirements of this Regulation. .



---

## Node: ['industry', 'start-ups', 'smes', 'civil society', 'academia']
<a name="node-industry-start-ups-smes-civil-society-academia"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **balanced with regard to commercial and non-commercial interests ← union or member states**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.

- **represents a balanced selection of ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.



---

## Node: physical or virtual environments
<a name="node-physical-or-virtual-environments"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **influences ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 2

    > (1) ‘AI system’ means a machine-based system designed to operate with varying levels of 
    > autonomy, that may exhibit adaptiveness after deployment and that, for explicit or 
    > implicit objectives, infers, from the input it receives, how to generate outputs such as 
    > predictions, content, recommendations, or decisions that can influence physical or virtual 
    > environments;



---

## Node: confidentiality
<a name="node-confidentiality"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← commission**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Obligated to safeguard ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 7

    > 6. Notifying authorities shall safeguard the confidentiality of the information they obtain, in 
    > accordance with Article 78.



---

## Node: ai value chain
<a name="node-ai-value-chain"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Comprises → responsibilities**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 1

    > Article 25
    > Responsibilities along the AI value chain

### Incoming relationships

_(none)_



---

## Node: confidence in their performance
<a name="node-confidence-in-their-performance"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **ensure ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 4

    > 3. The organisational structure, allocation of responsibilities, reporting lines and operation of 
    > notified bodies shall ensure confidence in their performance, and in the results of the 
    > conformity assessment activities that the notified bodies conduct.



---

## Node: data collection processes
<a name="node-data-collection-processes"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Complements → documentation of assessment**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 5

    > 4. If any of the obligations laid down in this Article is already complied with as a result of 
    > the data protection impact assessment conducted pursuant to Article 35 of Regulation 
    > (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamental rights impact 
    > assessment referred to in paragraph 1 of this Article shall complement that data 
    > protection impact assessment.

### Incoming relationships

- **Origin of ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.



---

## Node: enforcement measures
<a name="node-enforcement-measures"></a>

*5 outgoing, 5 incoming*

### Outgoing relationships

- **consideration of → partial application**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 5

    > 4. The risk management measures referred to in paragraph 2, point (d), shall give due 
    > consideration to the effects and possible interaction resulting from the combined 
    > application of the requirements set out in this Section, with a view to minimising risks 
    > more effectively while achieving an appropriate balance in implementing the measures 
    > to fulfil those requirements.

- **give due consideration to → effects and possible interaction**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 5

    > 4. The risk management measures referred to in paragraph 2, point (d), shall give due 
    > consideration to the effects and possible interaction resulting from the combined 
    > application of the requirements set out in this Section, with a view to minimising risks 
    > more effectively while achieving an appropriate balance in implementing the measures 
    > to fulfil those requirements.

- **Implements → organization implementing the quality management system**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 1

    > Article 17
    > Quality management system

- **Includes → (a) strategy for regulatory compliance**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

- **Referred to in Article 9 → enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

### Incoming relationships

- **Lay down the rules on ← union or member states**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 2

    > 1. In compliance with the terms and conditions laid down in this Regulation, Member States 
    > shall lay down the rules on penalties and other enforcement measures, which may also 
    > include warnings and non-monetary measures, applicable to infringements of this 
    > Regulation by operators, and shall take all measures necessary to ensure that they are 
    > properly and effectively implemented and taking into account the guidelines issued by 
    > the Commission pursuant to Article 96. The penalties provided for shall be effective, 
    > proportionate and dissuasive. They shall take into  account the interests of SMEs, 
    > including start-ups, and their economic viability.

- **Ensures compliance with ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

- **Fulfil tasks ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 3

    > 2. Notified bodies shall satisfy the organisational, quality management, resources and process 
    > requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity 
    > requirements.

- **Referred to in Article 9 ← enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

- **Applies to ← article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 1

    > Article 9
    > Risk management system



---

## Node: real-world testing plan
<a name="node-real-world-testing-plan"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **participates in → subject**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 59

    > (58) ‘subject’, for the purpose of real-world testing, means a natural person who participates 
    > in testing in real-world conditions;

### Incoming relationships

- **Involves ← committee procedure**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 8

    > 7. Testing procedures may include testing in real-world conditions in accordance with 
    > Article 60.



---

## Node: elements necessary for technical progress
<a name="node-elements-necessary-for-technical-progress"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Introduces in delegated acts ← commission**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 6

    > 5. The Commission shall adopt delegated acts in accordance with Article 97 for the purpose 
    > of updating the content of the EU declaration of conformity set out in Annex V, in order to 
    > introduce elements that become necessary in light of technical progress.



---

## Node: union agency
<a name="node-union-agency"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Need for establishment ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.



---

## Node: mitigation and control measures
<a name="node-mitigation-and-control-measures"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **address → risks that cannot be eliminated**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.

### Incoming relationships

_(none)_



---

## Node: risk involved
<a name="node-risk-involved"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Associated with ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.



---

## Node: sufficiently representative input data
<a name="node-sufficiently-representative-input-data"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensures ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 5

    > 4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over 
    > the input data, that deployer shall ensure that input data is relevant and sufficiently 
    > representative in view of the intended purpose of the high-risk AI system.



---

## Node: notifying authority
<a name="node-notifying-authority"></a>

*46 outgoing, 15 incoming*

### Outgoing relationships

- **equivalent to → market surveillance governance and enforcement**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 49

    > (48) ‘national competent authority’ means a notifying authority or a market surveillance 
    > authority;;

- **agrees on → sandbox plan**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 55

    > (54) ‘sandbox plan’ means a document agreed between the participating provider and the 
    > competent authority describing the objectives, conditions, timeframe, methodology and 
    > requirements for the activities carried out within the sandbox;

- **Set up by → ai regulatory sandbox**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 56

    > (55) ‘AI regulatory sandbox’ means a controlled framework set up by a competent authority 
    > which offers providers or prospective providers of AI systems the possibility to develop, 
    > train, validate and test, where appropriate in real-world conditions, an innovative AI 
    > system, pursuant to a sandbox plan for a limited time under regulatory supervision;

- **ensures → compliance with those requirements**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **Fulfil tasks → enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 3

    > 2. Notified bodies shall satisfy the organisational, quality management, resources and process 
    > requirements that are necessary to fulfil their tasks, as well as suitable cybersecurity 
    > requirements.

- **Implements → specific requirements**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 1

    > Article 31
    > Requirements relating to notified bodies

- **ensure → confidence in their performance**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 4

    > 3. The organisational structure, allocation of responsibilities, reporting lines and operation of 
    > notified bodies shall ensure confidence in their performance, and in the results of the 
    > conformity assessment activities that the notified bodies conduct.

- **Independent of → operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 5

    > 4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to 
    > which they perform conformity assessment activities. Notified bodies shall also be 
    > independent of any other operator having an economic interest in high-risk AI systems 
    > assessed, as well as of any competitors of the provider. This shall not preclude the use of 
    > assessed high-risk AI systems that are necessary for the operations of the conformity 
    > assessment body, or the use of such high-risk AI systems for personal purposes.

- **Takes due account of → downstream provider**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 3

    > 2. Notified bodies shall avoid unnecessary burdens for providers when performing their 
    > activities, and take due account of the size of the provider, the sector in which it 
    > operates, its structure and the degree of complexity of the high-risk AI system 
    > concerned, in particular in view of minimising administrative burdens and compliance 
    > costs for micro- and small enterprises within the meaning of Recommendation 
    > 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the 
    > level of protection required for the compliance of the high-risk AI system with the 
    > requirements of this Regulation. .

- **Operates to safeguard → liability**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.

- **Documents and implements → committee procedure**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.

- **Relates to → assessment activities**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.

- **Required for conformity assessment activities → appropriate procedural safeguards**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 10

    > 9. Notified bodies shall take out appropriate liability insurance for their conformity 
    > assessment activities, unless liability is assumed by the Member State in which they are 
    > established in accordance with national law or that Member State is itself directly 
    > responsible for the conformity assessment.

- **Upholds → highest degree of professional integrity**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 11

    > 10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with 
    > the highest degree of professional integrity and the requisite competence in the specific 
    > field, whether those tasks are carried out by notified bodies themselves or on their behalf 
    > and under their responsibility.

- **Possesses → requisite competence in the specific field**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 11

    > 10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with 
    > the highest degree of professional integrity and the requisite competence in the specific 
    > field, whether those tasks are carried out by notified bodies themselves or on their behalf 
    > and under their responsibility.

- **Participate in coordination activities → standardisation requests**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 13

    > 12. Notified bodies shall participate in coordination activities as referred to in Article 38. They 
    > shall also take part directly, or be represented in, European standardisation organisations, 
    > or ensure that they are aware and up to date in respect of relevant standards.

- **Ensure awareness of → up to date with relevant standards**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 13

    > 12. Notified bodies shall participate in coordination activities as referred to in Article 38. They 
    > shall also take part directly, or be represented in, European standardisation organisations, 
    > or ensure that they are aware and up to date in respect of relevant standards.

- **Takes due account of → sector in which it operates**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 3

    > 2. Notified bodies shall avoid unnecessary burdens for providers when performing their 
    > activities, and take due account of the size of the provider, the sector in which it 
    > operates, its structure and the degree of complexity of the high-risk AI system 
    > concerned, in particular in view of minimising administrative burdens and compliance 
    > costs for micro- and small enterprises within the meaning of Recommendation 
    > 2003/361/EC. The notified body shall, nevertheless, respect the degree of rigour and the 
    > level of protection required for the compliance of the high-risk AI system with the 
    > requirements of this Regulation. .

- **issues (with request) → union technical documentation assessment certificates**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.

- **make available to → market surveillance governance and enforcement**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 7

    > 6. In the event of the restriction, suspension or withdrawal of a designation, the notifying 
    > authority shall take appropriate steps to ensure that the files of the notified body 
    > concerned are kept, and to make them available to notifying authorities in other Member 
    > States and to market surveillance authorities at their request.

- **Related to → section 4**
  - Chapter 3: High-risk ai systems
  - Article 22: Fundamental rights impact assessment for high-risk ai systems
  - Paragraph 6

    > 5. The AI Office shall develop a template for a questionnaire, including through an 
    > automated tool, to facilitate deployers in complying with their obligations under this 
    > Article in a simplified manner.
    > Section 4
    > Notifying authorities and notified bodies

- **Applies to → article 6(1)**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 1

    > Article 28
    > Notifying authorities

- **Provides → conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 4

    > 3. The notification referred to in paragraph 2 of this Article shall include full details of the 
    > conformity assessment activities, the conformity assessment module or modules, the types 
    > of AI systems concerned, and the relevant attestation of competence. Where a 
    > notification is not based on an accreditation certificate as referred to in Article 29(2), the 
    > notifying authority shall provide the Commission and the other Member States with 
    > documentary evidence which attests to the competence of the conformity assessment 
    > body and to the arrangements in place to ensure that that body will be monitored 
    > regularly and will continue to satisfy the requirements laid down in Article 31.

- **Challenges notified bodies' competence → competent personnel**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 1

    > Article 37
    > Challenge to the competence of notified bodies

- **Offer nor provide → commercial or competitive consultancy services**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 6

    > 5. Notifying authorities shall offer or provide neither any activities that conformity 
    > assessment bodies perform, nor any consultancy services on a commercial or competitive 
    > basis.

- **Obligated to safeguard → confidentiality**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 7

    > 6. Notifying authorities shall safeguard the confidentiality of the information they obtain, in 
    > accordance with Article 78.

- **Informs → union or member states**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 2

    > 1. The notifying authority shall notify the Commission and the other Member States of any 
    > relevant changes to the notification of a notified body via the electronic notification tool 
    > referred to in Article 30(2).

- **ownership relationship → subsidiaries**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 1

    > Article 33
    > Subsidiaries of notified bodies and subcontracting

- **responsible for specific tasks connected with conformity assessment → subcontracting**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 2

    > 1. Where a notified body subcontracts specific tasks connected with the conformity 
    > assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the 
    > subsidiary meets the requirements laid down in Article 31, and shall inform the notifying 
    > authority accordingly.

- **informs → authorities informed accordingly**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 2

    > 1. Where a notified body subcontracts specific tasks connected with the conformity 
    > assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the 
    > subsidiary meets the requirements laid down in Article 31, and shall inform the notifying 
    > authority accordingly.

- **Performs → partial application**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 1

    > Article 34
    > Operational obligations of notified bodies

- **informs → types of ai systems concerned**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 6

    > 5. Where its designation has been suspended, restricted, or fully or partially withdrawn, the 
    > notified body shall inform the providers concerned at the latest within 10 days.

- **ceased activities → five-year period**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.

- **informs → notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.

- **suspends or withdraws → certificate**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **imposes → regulation**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **gives reasons for its decision → provider of an ai system**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **takes account of → principle of proportionality**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **refuses, suspends or withdraws → quality management system approvals**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.

- **Requires → ce marking**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 1

    > Article 30
    > Notification procedure

- **withdraws → design choices**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.

- **assumes responsibilities for → identification number of the notified body**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 4

    > 3. Where a notified body decides to cease its conformity assessment activities, it shall 
    > inform the notifying authority and the providers concerned as soon as possible and, in 
    > the case of a planned cessation, at least one year before ceasing its activities. The 
    > certificates of the notified body may remain valid for a temporary period of nine months 
    > after cessation of the notified body’s activities, on condition that another notified body 
    > has confirmed in writing that it will assume responsibilities for the high risk AI systems 
    > covered by those certificates. The latter notified body shall complete a full assessment of 
    > the AI systems affected by the end of that nine-month-period before issuing new 
    > certificates for those systems. Where the notified body has ceased its activity, the 
    > notifying authority shall withdraw the designation.

- **Responsible for overseeing → ai office**
  - Chapter 7: Governance
  - Article 1: Ai office
  - Paragraph 1

    > Article 64
    > AI Office

- **during investigations → types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.

- **Exercises powers for evaluation → regulation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 1

    > Article 92
    > Power to conduct evaluations

- **Requests measures → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 1

    > Article 93
    > Power to request measures

### Incoming relationships

- **Exchange of information ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.

- **confidential basis ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 5

    > 3. Without prejudice to paragraphs 1 and 2, information exchanged on a confidential basis 
    > between the national competent authorities or between national competent authorities and 
    > the Commission shall not be disclosed without prior consultation of the originating 
    > national competent authority and the deployer when high-risk AI systems referred to in 
    > point 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or 
    > asylum authorities and when such disclosure would jeopardise public and national security 
    > interests. This exchange of information shall not cover sensitive operational data in 
    > relation to the activities of law enforcement, border control, immigration or asylum 
    > authorities.
    > When the law enforcement, immigration or asylum authorities are providers of high-risk 
    > AI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred 
    > to in Annex IV shall remain within the premises of those authorities. Those authorities 
    > shall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as 
    > applicable, can, upon request, immediately access the documentation or obtain a copy 
    > thereof. Only staff of the market surveillance authority holding the appropriate level of 
    > security clearance shall be allowed to access that documentation or any copy thereof.

- **listed in Section B of Annex I; ← conformity assessment body**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 23

    > (22) ‘notified body’ means a conformity assessment body notified in accordance with this 
    > Regulation and other relevant Union harmonisation legislation as listed in Section B of 
    > Annex I;

- **Can perform activities of ← conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 5

    > 4. The conformity assessment body concerned may perform the activities of a notified body 
    > only where no objections are raised by the Commission or the other Member States within 
    > two weeks of a notification by a notifying authority where it includes an accreditation 
    > certificate referred to in Article 29(2), or within two months of a notification by the 
    > notifying authority where it includes documentary evidence referred to in Article 29(3).

- **cooperates with ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.

- **informs accordingly ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 3

    > 2. Where the market surveillance authority of a Member State has sufficient reason to 
    > consider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall 
    > carry out an evaluation of the AI system concerned in respect of its compliance with all the 
    > requirements and obligations laid down in this Regulation. Particular attention shall be 
    > given to AI systems presenting a risk to groups of vulnerable persons referred to in 
    > Article 5. Where risks to fundamental rights of persons are identified, the market 
    > surveillance authority shall also inform and fully cooperate with the relevant national 
    > public authorities or bodies referred to in Article 77(1). The relevant operators shall 
    > cooperate as necessary with the market surveillance authority and with the other national 
    > public authorities or bodies referred to in Article 77(1).
    > Where, in the course of that evaluation, the market surveillance authority or, where 
    > applicable the market surveillance authority in cooperation with the national public 
    > authority referred to in Article 77(1), finds that the AI system does not comply with the 
    > requirements and obligations laid down in this Regulation, it shall without undue delay 
    > require the relevant operator to take all appropriate corrective actions to bring the AI 
    > system into compliance, to withdraw the AI system from the market, or to recall it within a 
    > period the market surveillance authority may prescribe, and in any event within the 
    > shorter of 15 working days, or as provided for in the relevant Union harmonisation 
    > legislation.
    > The market surveillance authority shall inform the relevant notified body accordingly. 
    > Article 18 of Regulation (EU) 2019/1020 shall apply to the measures referred to in the 
    > second subparagraph of this paragraph.

- **Verifies conformity ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 29: Operational obligations of notified bodies
  - Paragraph 2

    > 1. Notified bodies shall verify the conformity of high-risk AI systems in accordance with 
    > the conformity assessment procedures set out in Article 43.

- **Established under national law ← union or member states**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 2

    > 1. A notified body shall be established under the national law of a Member State and shall 
    > have legal personality.

- **informs ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 40: Information obligations of notified bodies
  - Paragraph 3

    > 2. Each notified body shall inform the other notified bodies of:
    > (a) quality management system approvals which it has refused, suspended or withdrawn, 
    > and, upon request, of quality system approvals which it has issued;
    > (b) Union technical documentation assessment certificates or any supplements thereto 
    > which it has refused, withdrawn, suspended or otherwise restricted, and, upon 
    > request, of the certificates and/or supplements thereto which it has issued.

- **Immediately informs about termination of mandate and reasons (where applicable) ← authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 17: Authorised representatives of providers of high-risk ai systems
  - Paragraph 5

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the market surveillance 
    > authority of the Member State in which it is located or established, as well as, where 
    > applicable, the relevant notified body, about the termination of the mandate and the 
    > reasons therefor.

- **Occurrence ← serious incident**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 12

    > 12. National competent authorities shall immediately notify the Commission of any serious 
    > incident, whether or not it they have taken action on it, in accordance with Article 20 of 
    > Regulation (EU) 2019/1020.
    > Section 3
    > Enforcement

- **Implements ← union safeguard procedure**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 10: Union safeguard procedure
  - Paragraph 1

    > Article 81
    > Union safeguard procedure

- **Exchange of information ← member states and their relevant authorities**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.

- **Notifies ← types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 1

    > Article 79
    > Procedure at national level for dealing with AI systems presenting a risk

- **Implements Union AI testing support structures ← union ai testing support structures**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 1

    > Article 84
    > Union AI testing support structures



---

## Node: chapters i and ii
<a name="node-chapters-i-and-ii"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Applicable from → partial application**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

### Incoming relationships

- **Amends ← commission**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97(2) to amend 
    > Annexes XI and XII in the light of evolving technological developments.



---

## Node: inspections, investigations or audits
<a name="node-inspections-investigations-or-audits"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **In respect of ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 16: Reporting of infringements and protection of reporting persons
  - Paragraph 1

    > Article 87
    > Reporting of infringements and protection of reporting persons
    > Directive (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation and the 
    > protection of persons reporting such infringements.
    > Section 5
    > Supervision, investigation, enforcement and monitoring in respect of 
    > providers of general-purpose AI models

- **Applies to ← directive (eu) 2022/2557**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.



---

## Node: product
<a name="node-product"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Withdraws from national market ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 6

    > 5. Where the operator of an AI system does not take adequate corrective action within the 
    > period referred to in paragraph 2, the market surveillance authority shall take all 
    > appropriate provisional measures to prohibit or restrict the AI system's being made 
    > available on its national market or put into service, to withdraw the product or the 
    > standalone AI system from that market or to recall it. That authority shall without undue 
    > delay notify the Commission and the other Member States  of those measures.



---

## Node: deployers (who are employers)
<a name="node-deployers-who-are-employers"></a>

*2 outgoing, 0 incoming*

### Outgoing relationships

- **Inform → authorised representatives of providers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 8

    > 7. Before putting into service or using a high-risk AI system at the workplace, deployers 
    > who are employers shall inform workers’ representatives and the affected workers that 
    > they will be subject to the use of the high-risk AI system. This information shall be 
    > provided, where applicable, in accordance with the rules and procedures laid down in 
    > Union and national law and practice on information of workers and their 
    > representatives.

- **Inform → affected workers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 8

    > 7. Before putting into service or using a high-risk AI system at the workplace, deployers 
    > who are employers shall inform workers’ representatives and the affected workers that 
    > they will be subject to the use of the high-risk AI system. This information shall be 
    > provided, where applicable, in accordance with the rules and procedures laid down in 
    > Union and national law and practice on information of workers and their 
    > representatives.

### Incoming relationships

_(none)_



---

## Node: experts designated by each member state
<a name="node-experts-designated-by-each-member-state"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Consults ← commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 5

    > 4. Before adopting a delegated act, the Commission shall consult experts designated by each 
    > Member State in accordance with the principles laid down in the Interinstitutional 
    > Agreement of 13 April 2016 on Better Law-Making.



---

## Node: shortcomings
<a name="node-shortcomings"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Identification of ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.



---

## Node: evidence-based regulatory learning
<a name="node-evidence-based-regulatory-learning"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Contribute to ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 10

    > 9. The establishment of AI regulatory sandboxes shall aim to contribute to the following 
    > objectives:
    > (a) improving legal certainty to achieve regulatory compliance with this Regulation or, 
    > where relevant, other applicable Union and national law;
    > (b) supporting the sharing of best practices through cooperation with the authorities 
    > involved in the AI regulatory sandbox;
    > (c) fostering innovation and competitiveness and facilitating the development of an AI 
    > ecosystem;
    > (d) contributing to evidence-based regulatory learning;
    > (e) facilitating and accelerating access to the Union market for AI systems, in 
    > particular when provided by SMEs, including start-ups.



---

## Node: measures to support innovation
<a name="node-measures-to-support-innovation"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **With a particular focus on SMEs, including start-
ups. → measures to support innovation**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

### Incoming relationships

- **With a particular focus on SMEs, including start-
ups. ← measures to support innovation**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.



---

## Node: manipulative or deceptive techniques
<a name="node-manipulative-or-deceptive-techniques"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **exploits ← ai systems**
  - Chapter 2: Prohibited artificial intelligence practices
  - Article 1: Prohibited ai practices
  - Paragraph 2

    > 1. The following AI practices shall be prohibited:
    > (a) the placing on the market, the putting into service or the use of an AI system that 
    > deploys subliminal techniques beyond a person’s consciousness or purposefully 
    > manipulative or deceptive techniques, with the objective, or the effect of, materially 
    > distorting the behaviour of a person or a group of persons by appreciably impairing 
    > their ability to make an informed decision, thereby causing a person to take a 
    > decision that that person would not have otherwise taken in a manner that causes or 
    > is likely to cause that person, another person or group of persons significant harm;
    > (b) the placing on the market, the putting into service or the use of an AI system that 
    > exploits any of the vulnerabilities of a person or a specific group of persons due to 
    > their age, disability or a specific social or economic situation, with the objective, or 
    > the effect, of materially distorting the behaviour of that person or a person 
    > belonging to that group in a manner that causes or is reasonably likely to cause that 
    > person or another person significant harm;
    > (c) the placing on the market, the putting into service or the use of AI systems  for the 
    > purpose of the evaluation or classification of natural persons or groups of persons 
    > over a certain period of time based on their social behaviour or known, inferred or 
    > predicted personal or personality characteristics, with the social score leading to 
    > either or both of the following:
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > 
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (i) detrimental or unfavourable treatment of certain natural persons or whole 
    > groups of persons in social contexts that are unrelated to the contexts in which 
    > the data was originally generated or collected;
    > (ii) detrimental or unfavourable treatment of certain natural persons or  groups of 
    > persons that is unjustified or disproportionate to their social behaviour or its 
    > gravity;
    > (d) the placing on the market, the putting into service for this specific purpose, or the 
    > use of an AI system for making risk assessments of natural persons in order to 
    > assess or predict the likelihood of a natural person committing a criminal offence, 
    > based solely on the profiling of a natural person or on assessing their personality 
    > traits and characteristics; this prohibition shall not apply to AI systems used to 
    > support the human assessment of the involvement of a person in a criminal 
    > activity, which is already based on objective and verifiable facts directly linked to a 
    > criminal activity;
    > (e) the placing on the market, the putting into service for this specific purpose, or use 
    > of AI systems that create or expand facial recognition databases through the 
    > untargeted scraping of facial images from the internet or CCTV footage;
    > (f) the placing on the market, the putting into service for this specific purpose, or the 
    > use of AI systems to infer emotions of a natural person in the areas of workplace 
    > and education institutions, except where the use of the AI system is intended to be 
    > put in place or into the market for medical or safety reasons.
    > (g) the placing on the market, the putting into service for this specific purpose, or the 
    > use of biometric categorisation systems that categorise individually natural persons 
    > based on their biometric data to deduce or infer their race, political opinions, trade 
    > union membership, religious or philosophical beliefs, sex life or sexual 
    > orientation; this prohibition does not cover any labelling or filtering of lawfully 
    > acquired biometric datasets, such as images, based on biometric data or 
    > categorizing of biometric data in the area of law enforcement;
    > (h) the use of ‘real-time’ remote biometric identification systems in publicly accessible 
    > spaces for the purposes of law enforcement,  unless and in so far as such use is 
    > strictly necessary for one of the following objectives:
    > (i) the targeted search for specific  victims of abduction, trafficking in human 
    > beings or sexual exploitation of human beings, as well as searching for 
    > missing persons;
    > (ii) the prevention of a specific, substantial and imminent threat to the life or 
    > physical safety of natural persons or a genuine and present or genuine and 
    > foreseeable threat of a terrorist attack;
    > (iii) the  localisation or identification of a person suspected of having committed 
    > a criminal offence, for the purpose of conducting a criminal investigation, 
    > prosecution or executing a criminal penalty for offences referred to in Annex 
    > II and punishable in the Member State concerned by a custodial sentence or a 
    > detention order for a maximum period of at least four years;
    >  
    > Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 
    > 2016/679 for the processing of biometric data for purposes other than law enforcement.



---

## Node: data governance and management practices
<a name="node-data-governance-and-management-practices"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Subject to ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.



---

## Node: reasoned complaints
<a name="node-reasoned-complaints"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Taken into account for the purpose of conducting ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 14: Right to lodge a complaint with a market surveillance authority
  - Paragraph 1

    > Article 85
    > Right to lodge a complaint with a market surveillance authority
    > Without prejudice to other administrative or judicial remedies, any natural or legal person 
    > having grounds to consider that there has been an infringement of the provisions of this 
    > Regulation may submit reasoned complaints to the relevant market surveillance 
    > authority.
    > In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into 
    > account for the purpose of conducting market surveillance activities, and shall be 
    > handled in line with the dedicated procedures established therefor by the market 
    > surveillance authorities.



---

## Node: direct applicability
<a name="node-direct-applicability"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Bound by ← union or member states**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Entails ← regulation**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.



---

## Node: benchmarks and measurement methodologies
<a name="node-benchmarks-and-measurement-methodologies"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Encourages the development of ← commission**
  - Chapter 3: High-risk ai systems
  - Article 10: Accuracy, robustness and cybersecurity
  - Paragraph 3

    > 2. To address the technical aspects of how to measure the appropriate levels of accuracy 
    > and robustness set out in paragraph 1 and any other relevant performance metrics, the 
    > Commission shall, in cooperation with relevant stakeholder and organisations such as 
    > metrology and benchmarking authorities, encourage, as appropriate, the development of 
    > benchmarks and measurement methodologies.



---

## Node: independent experts
<a name="node-independent-experts"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **may support → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

### Incoming relationships

- **Involves ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 7

    > 6. The Commission shall adopt implementing acts setting out the detailed arrangements 
    > and the conditions of the evaluations, including the detailed arrangements for involving 
    > independent experts, and the procedure for the selection thereof. Those implementing 
    > acts shall be adopted in accordance with the examination procedure referred to in 
    > Article 98(2).



---

## Node: ['smes', 'other undertakings']
<a name="node-smes-other-undertakings"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **balanced with regard to commercial interests ← union or member states**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 3

    > 2. The membership of the advisory forum shall represent a balanced selection of 
    > stakeholders, including industry, start-ups, SMEs, civil society and academia. The 
    > membership of the advisory forum shall be balanced with regard to commercial and 
    > non-commercial interests and, within the category of commercial interests, with regard 
    > to SMEs and other undertakings.



---

## Node: principle of proportionality
<a name="node-principle-of-proportionality"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **takes account of ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.



---

## Node: market and technological developments
<a name="node-market-and-technological-developments"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Considers or takes into account ← commission**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 7

    > 6. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > conditions laid down in paragraph 3, first subparagraph, of this Article.
    > The Commission may adopt delegated acts in accordance with Article 97 in order to add 
    > new conditions to those laid down in paragraph 3, first subparagraph, or to modify them, 
    > only where there is concrete and reliable evidence of the existence of AI systems that fall 
    > under the scope of Annex III but do not pose a significant risk of harm to the health, 
    > safety or fundamental rights of natural persons.
    > The Commission shall adopt delegated acts in accordance with Article 97 in order to 
    > delete any of the conditions laid down in the paragraph 3, first subparagraph, where 
    > there is concrete and reliable evidence that this is necessary for the purpose of 
    > maintaining the level of protection of health, safety and fundamental rights in the 
    > Union.
    > Any amendment to the conditions laid down in paragraph 3, first subparagraph, shall 
    > not decrease the overall level of protection of health, safety and fundamental rights in 
    > the Union.
    > When adopting the delegated acts, the Commission shall ensure consistency with the 
    > delegated acts adopted pursuant to Article 7(1), and shall take account of market and 
    > technological developments.

- **Correlates with ← key performance indicators**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.



---

## Node: military, defence or national security purposes
<a name="node-military-defence-or-national-security-purposes"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **exclusively for ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 5

    > 3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in 
    > any event, affect the competences of the Member States concerning national security, 
    > regardless of the type of entity entrusted by the Member States with carrying out tasks in 
    > relation to those competences.
    > This Regulation does not apply to AI systems where and in so far they are placed on the 
    > market, put into service, or used with or without modification exclusively for military, 
    > defence or national security purposes, regardless of the type of entity carrying out those 
    > activities.
    > This Regulation does not apply to AI systems which are not placed on the market or put 
    > into service in the Union, where the output is used in the Union exclusively for military, 
    > defence or national security purposes, regardless of the type of entity carrying out those 
    > activities.



---

## Node: logs referred to in article 12(1)
<a name="node-logs-referred-to-in-article-12-1"></a>

*3 outgoing, 2 incoming*

### Outgoing relationships

- **Not otherwise provided for → other union law**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 15: Right to explanation of individual decision-making
  - Paragraph 4

    > 3. This Article shall apply only to the extent that the right referred to in paragraph 1 is not 
    > otherwise provided for under Union law.

- **Associated with → supplying incorrect, incomplete or misleading information**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

- **Penalty for not providing → failure to provide access**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.

### Incoming relationships

- **Registered in ← ai systems**
  - Chapter 3: High-risk ai systems
  - Article 44: Registration
  - Paragraph 3

    > 2. Before placing on the market or putting into service an AI system for which the provider 
    > has concluded that it is not high-risk according to Article 6(3), that provider or, where 
    > applicable, the authorised representative shall register themselves and that system in the 
    > EU database referred to in Article 71.

- **Keep ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 14: Automatically generated logs
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), 
    > automatically generated by their high-risk AI systems, to the extent such logs are under 
    > their control. Without prejudice to applicable Union or national law, the logs shall be 
    > kept for a period  appropriate to the intended purpose of the high-risk AI system, of at 
    > least six months, unless provided otherwise in the applicable Union or national law, in 
    > particular in Union law on the protection of personal data.



---

## Node: persons under the age of 18
<a name="node-persons-under-the-age-of-18"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **may have adverse impact on ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 10

    > 9. When implementing the risk management system as provided for in paragraphs 1 to 7, 
    > providers shall give consideration to whether in view of its intended purpose the high-risk 
    > AI system is likely to have an adverse impact on persons under the age of 18 and, as 
    > appropriate, other groups of vulnerable persons.



---

## Node: adequate level of cybersecurity measures
<a name="node-adequate-level-of-cybersecurity-measures"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Takes ← national competent authorities**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 5

    > 4. National competent authorities shall take an adequate level of cybersecurity measures.



---

## Node: board
<a name="node-board"></a>

*8 outgoing, 5 incoming*

### Outgoing relationships

- **Takes into account → financial resources**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 9

    > 8. Subject to the confidentiality provisions in Article 78, and with the agreement of the 
    > provider or prospective provider, the Commission and the Board shall be authorised to 
    > access the exit reports and shall take them into account, as appropriate, when exercising 
    > their tasks under this Regulation. If both the provider or prospective provider and the 
    > national competent authority explicitly agree, the exit report may be made publicly 
    > available through the single information platform referred to in this Article.

- **Establishment of → artificial intelligence act**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 2

    > 1. A European Artificial Intelligence Board (the ‘Board’) is hereby established.

- **Observer → european data protection supervisor**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 3

    > 2. The Board shall be composed of one representative per Member State. The European 
    > Data Protection Supervisor shall participate as observer. The AI Office shall also attend 
    > the Board’s meetings, without taking part in the votes. Other national and Union 
    > authorities, bodies or experts may be invited to the meetings by the Board on a case by 
    > case basis, where the issues discussed are of relevance for them.

- **Contribute actively to the achievement of tasks → union or member states**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **Facilitate consistency and coordination between national competent authorities → requirements set out in this section**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 5

    > 4. Member States shall ensure that their representatives on the Board:
    > (a) have the relevant competences and powers in their Member State so as to 
    > contribute actively to the achievement of the Board’s tasks referred to in 
    > Article 66;
    > (b) are designated as a single contact point vis-à-vis the Board and, where appropriate, 
    > taking into account Member States’ needs, as a single contact point for 
    > stakeholders;
    > (c) are empowered to facilitate consistency and coordination between national 
    > competent authorities in their Member State as regards the implementation of this 
    > Regulation, including through the collection of relevant data and information for 
    > the purpose of fulfilling their tasks on the Board.

- **Ensures → objectivity**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 8

    > 7. The Board shall be organised and operated so as to safeguard the objectivity and 
    > impartiality of its activities.

- **Maintains → liability**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 8

    > 7. The Board shall be organised and operated so as to safeguard the objectivity and 
    > impartiality of its activities.

- **Requests opinions, recommendations and written contributions from → advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 9

    > 8. The advisory forum may prepare opinions, recommendations and written contributions 
    > upon request of the Board or the Commission.

### Incoming relationships

- **Transmits information to ← commission**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 7

    > 6. By …, [one year from the date of entry into force of this Regulation] and once every two 
    > years thereafter, Member States shall report to the Commission  on the status of the 
    > financial and human resources of the national competent authorities, with an assessment of 
    > their adequacy. The Commission shall transmit that information to the Board for 
    > discussion and possible recommendations.

- **Attend meetings without taking part in votes ← ai office**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 3

    > 2. The Board shall be composed of one representative per Member State. The European 
    > Data Protection Supervisor shall participate as observer. The AI Office shall also attend 
    > the Board’s meetings, without taking part in the votes. Other national and Union 
    > authorities, bodies or experts may be invited to the meetings by the Board on a case by 
    > case basis, where the issues discussed are of relevance for them.

- **Submits annual reports ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Communicated to ← fines imposed under this article**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 5

    > 4. Information on fines imposed under this Article shall also be communicated to the 
    > Board as appropriate.

- **Seeks independent technical or scientific advice ← union ai testing support structures**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 3

    > 2. Without prejudice to the tasks referred to in paragraph 1, Union AI testing support 
    > structures shall also provide independent technical or scientific advice at the request of 
    > the Board, the Commission, or of market surveillance authorities.
    > Section 4
    > Remedies



---

## Node: clear objectives
<a name="node-clear-objectives"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Concerning → ai systems**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

### Incoming relationships

- **Precondition for entering into force ← delegated act**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 7

    > 6. Any delegated act adopted pursuant Article 6(6), Article 7(1) and (3), Article 11(3), Article 
    > 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall 
    > enter into force only if no objection has been expressed by either the European Parliament 
    > or the Council within a period of three months of notification of that act to the European 
    > Parliament and the Council or if, before the expiry of that period, the European Parliament 
    > and the Council have both informed the Commission that they will not object. That period 
    > shall be extended by three months at the initiative of the European Parliament or of the 
    > Council.



---

## Node: market monitoring
<a name="node-market-monitoring"></a>

*6 outgoing, 4 incoming*

### Outgoing relationships

- **Implementation of → market monitoring**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Based on → market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 4

    > 3. The post-market monitoring system shall be based on a post-market monitoring plan. The 
    > post-market monitoring plan shall be part of the technical documentation referred to in 
    > Annex IV. The Commission shall adopt an implementing act laying down detailed 
    > provisions establishing a template for the post-market monitoring plan and the list of 
    > elements to be included in the plan by … [six months before the entry into application of 
    > this Regulation]. That implementing act shall be adopted in accordance with the 
    > examination procedure referred to in Article 98(2).

- **Proportional to → information technologies**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Collects and analyzes data throughout lifetime for compliance → operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Evaluates continuous compliance with requirements in Chapter III, Section 2 → providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Provides relevant data on performance throughout lifetime → deployers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

### Incoming relationships

- **Collects and reviews experience gained from use ← provider of an ai system**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 26

    > (25) ‘post-market monitoring system’ means all activities carried out by providers of AI 
    > systems to  collect and review experience gained from the use of AI systems they place 
    > on the market or put into service for the purpose of identifying any need to immediately 
    > apply any necessary corrective or preventive actions;

- **Implementation of ← market monitoring**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 3

    > 2. This Regulation lays down:
    > (a) harmonised rules for the placing on the market, the putting into service, and the use 
    > of AI systems in the Union;
    > (b) prohibitions of certain AI practices;
    > (c) specific requirements for high-risk AI systems and obligations for operators of such 
    > systems;
    > (d) harmonised transparency rules for certain AI systems;
    > (e) harmonised rules for the placing on the market of general-purpose AI models;
    > (f) rules on market monitoring, market surveillance governance and enforcement;
    > (g) measures to support innovation, with a particular focus on SMEs, including start-
    > ups.

- **Establishes ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Based on ← market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 4

    > 3. The post-market monitoring system shall be based on a post-market monitoring plan. The 
    > post-market monitoring plan shall be part of the technical documentation referred to in 
    > Annex IV. The Commission shall adopt an implementing act laying down detailed 
    > provisions establishing a template for the post-market monitoring plan and the list of 
    > elements to be included in the plan by … [six months before the entry into application of 
    > this Regulation]. That implementing act shall be adopted in accordance with the 
    > examination procedure referred to in Article 98(2).



---

## Node: data
<a name="node-data"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Relates to → data governance**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 1

    > Article 10
    > Data and data governance

### Incoming relationships

_(none)_



---

## Node: subject
<a name="node-subject"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **gives → informed consent**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 60

    > (59) ‘informed consent’ means a subject's freely given, specific, unambiguous and voluntary 
    > expression of his or her willingness to participate in a particular testing in real-world 
    > conditions, after having been informed of all aspects of the testing that are relevant to 
    > the subject's decision to participate;

- **participates in → testing in real-world conditions**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 60

    > (59) ‘informed consent’ means a subject's freely given, specific, unambiguous and voluntary 
    > expression of his or her willingness to participate in a particular testing in real-world 
    > conditions, after having been informed of all aspects of the testing that are relevant to 
    > the subject's decision to participate;

### Incoming relationships

- **participates in ← real-world testing plan**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 59

    > (58) ‘subject’, for the purpose of real-world testing, means a natural person who participates 
    > in testing in real-world conditions;

- **Testing in real world conditions ← ai systems**
  - Chapter 6: Measures in support of innovation
  - Article 5: Informed consent to participate in testing in real world conditions
  - Paragraph 2

    > 1. For the purpose of testing in real world conditions under Article 60, freely-given 
    > informed consent shall obtained from the subjects of testing prior to their participation 
    > in such testing and after their having been duly informed with concise, clear, relevant, 
    > and understandable information regarding:
    > (a) the nature and objectives of the testing in real world conditions and the possible 
    > inconvenience that may be linked to their participation;
    > (b) the conditions under which the testing in real world conditions is to be conducted, 
    > including the expected duration of the subject or subjects' participation;
    > (c) their rights, and the guarantees regarding their participation, in particular their 
    > right to refuse to participate in, and the right to withdraw from, testing in real 
    > world conditions at any time without any resulting detriment and without having to 
    > provide any justification;
    > (d) the arrangements for requesting the reversal or the disregard of the predictions, 
    > recommendations or decisions of the AI system;
    > (e) the Union-wide unique single identification number of the testing in real world 
    > conditions in accordance with Article 60(4) point (c), and the contact details of the 
    > provider or its legal representative from whom further information can be 
    > obtained.



---

## Node: intended purpose
<a name="node-intended-purpose"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **For ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 58

    > (57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its 
    > intended purpose in real-world conditions outside a laboratory or otherwise simulated 
    > environment, with a view to gathering reliable and robust data and to assessing and 
    > verifying the conformity of the AI system with the requirements of this Regulation and it 
    > is not considered to be placing the AI system on the market or putting it into service 
    > within the meaning of this Regulation, provided that all the conditions laid down in 
    > Article 57 or 60 are fulfilled;



---

## Node: amendments
<a name="node-amendments"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Involves ← annex iii**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 1

    > Article 7
    > Amendments to Annex III



---

## Node: simplified manner
<a name="node-simplified-manner"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Applicable to ← microenterprises**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 2

    > 1. Microenterprises within the meaning of Recommendation 2003/361/EC, may comply 
    > with certain elements of the quality management system required by Article 17 of this 
    > Regulation in a simplified manner, provided that they do not have partner enterprises or 
    > linked enterprises within the meaning of that Recommendation. For that purpose, the 
    > Commission shall develop guidelines on the elements of the quality management system 
    > which may be complied with in a simplified manner considering the needs of 
    > microenterprises, without affecting the level of protection or the need for compliance 
    > with the requirements in respect of high-risk AI systems.



---

## Node: biometric data
<a name="node-biometric-data"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **results from → personal data**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 35

    > (34) ‘biometric data’ means personal data resulting from specific technical processing relating 
    > to the physical, physiological or behavioural characteristics of a natural person,  such as 
    > facial images or dactyloscopic data;

- **performs identification on → natural person**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 37

    > (36) ‘biometric verification’ means the automated, one-to-one verification, including 
    > authentication, of the identity of natural persons by comparing their biometric data to 
    > previously provided biometric data;

### Incoming relationships

- **Provides or directly acquires ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 34

    > (33) ‘input data’ means data provided to or directly acquired by an AI system on the basis of 
    > which the system produces an output;

- **Ensures ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 5

    > 4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over 
    > the input data, that deployer shall ensure that input data is relevant and sufficiently 
    > representative in view of the intended purpose of the high-risk AI system.



---

## Node: narrow procedural task
<a name="node-narrow-procedural-task"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Performs ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 1: Classification rules for high-risk ai systems
  - Paragraph 4

    > 3. By derogation from paragraph 2, an AI system shall not be considered to be high-risk if 
    > it does not pose a significant risk of harm to the health, safety or fundamental rights of 
    > natural persons, including by not materially influencing the outcome of decision 
    > making. This shall be the case where one or more of the following conditions are 
    > fulfilled:
    > (a) the AI system is intended to perform a narrow procedural task;
    > (b) the AI system is intended to improve the result of a previously completed human 
    > activity;
    > (c) the AI system is intended to detect decision-making patterns or deviations from 
    > prior decision-making patterns and is not meant to replace or influence the 
    > previously completed human assessment, without proper human review; or
    > (d) the AI system is intended to perform a preparatory task to an assessment relevant 
    > for the purposes of the use cases listed in Annex III.
    > Notwithstanding the first subparagraph, an AI system referred to in Annex III shall 
    > always be considered to be high-risk where the AI system performs profiling of natural 
    > persons.



---

## Node: experts and other stakeholders
<a name="node-experts-and-other-stakeholders"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Invites to meetings ← advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 8

    > 7. The advisory forum shall hold meetings at least twice a year. The advisory forum may 
    > invite experts and other stakeholders to its meetings.



---

## Node: responsibilities
<a name="node-responsibilities"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Comprises ← ai value chain**
  - Chapter 3: High-risk ai systems
  - Article 20: Responsibilities along the ai value chain
  - Paragraph 1

    > Article 25
    > Responsibilities along the AI value chain

- **Applies to ← directive (eu) 2022/2557**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 13

    > 13 of Directive (EU) 2016/680 shall apply.



---

## Node: floating-point operation
<a name="node-floating-point-operation"></a>

*2 outgoing, 1 incoming*

### Outgoing relationships

- **subtype of → floating-point operation**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 68

    > (67) ‘floating-point operation’ or ‘FLOP’ means any mathematical operation or assignment 
    > involving floating-point numbers, which are a subset of the real numbers typically 
    > represented on computers by an integer of fixed precision scaled by an integer exponent 
    > of a fixed base;

- **subset of → real number**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 68

    > (67) ‘floating-point operation’ or ‘FLOP’ means any mathematical operation or assignment 
    > involving floating-point numbers, which are a subset of the real numbers typically 
    > represented on computers by an integer of fixed precision scaled by an integer exponent 
    > of a fixed base;

### Incoming relationships

- **subtype of ← floating-point operation**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 68

    > (67) ‘floating-point operation’ or ‘FLOP’ means any mathematical operation or assignment 
    > involving floating-point numbers, which are a subset of the real numbers typically 
    > represented on computers by an integer of fixed precision scaled by an integer exponent 
    > of a fixed base;



---

## Node: academia
<a name="node-academia"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **may support → ['process of drawing-up codes of practice']**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 4

    > 3. The AI Office may invite all providers of general-purpose AI models, as well as relevant 
    > national competent authorities, to participate in the drawing-up of codes of practice. 
    > Civil society organisations, industry, academia and other relevant stakeholders, such as 
    > downstream providers and independent experts, may support the process.

### Incoming relationships

_(none)_



---

## Node: internal market
<a name="node-internal-market"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **first making available to ← placing on the market**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 10

    > (9) ‘placing on the market’ means the first making available of an AI system or a general-
    > purpose AI model on the Union market;

- **improves ← regulation**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 2

    > 1. The purpose of this Regulation is to improve the functioning of the internal market and 
    > promote the uptake of human-centric and trustworthy artificial intelligence (AI), while 
    > ensuring a high level of protection of health, safety, fundamental rights enshrined in the 
    > Charter of Fundamental Rights, including democracy, the rule of law and 
    > environmental protection, against the harmful effects of artificial intelligence systems 
    > (AI systems) in the Union, and to support innovation.



---

## Node: ethical review
<a name="node-ethical-review"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Not mutually exclusive with ← innovative ai systems**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 4

    > 3. The testing of high-risk AI systems in real world conditions under this Article shall be 
    > without prejudice to any ethical review that is required by Union or national law.



---

## Node: up to date with relevant standards
<a name="node-up-to-date-with-relevant-standards"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Ensure awareness of ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 13

    > 12. Notified bodies shall participate in coordination activities as referred to in Article 38. They 
    > shall also take part directly, or be represented in, European standardisation organisations, 
    > or ensure that they are aware and up to date in respect of relevant standards.



---

## Node: representative
<a name="node-representative"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Designates ← union or member states**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 4

    > 3. Each representative shall be designated by their Member State for a period of three 
    > years, renewable once.



---

## Node: union institutions, bodies, offices and agencies
<a name="node-union-institutions-bodies-offices-and-agencies"></a>

*1 outgoing, 3 incoming*

### Outgoing relationships

- **Fall within scope → european data protection supervisor**
  - Chapter 7: Governance
  - Article 7: Designation of national competent authorities and single point of contact
  - Paragraph 10

    > 9. Where Union institutions, bodies, offices or agencies fall within the scope of this 
    > Regulation, the European Data Protection Supervisor shall act as the competent authority 
    > for their supervision.

### Incoming relationships

- **Subject to ← administrative fine**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 1

    > Article 100
    > Administrative fines on Union institutions, bodies, offices and agencies

- **Establishes an AI regulatory sandbox for ← european data protection supervisor**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 4

    > 3. The European Data Protection Supervisor may also establish an AI regulatory sandbox 
    > for Union institutions, bodies, offices and agencies, and may exercise the roles and the 
    > tasks of national competent authorities in accordance with this Chapter.

- **Market surveillance authority ← european data protection supervisor**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 3: Market surveillance and control of ai systems in the union market
  - Paragraph 10

    > 9. Where Union institutions, bodies, offices or agencies fall within the scope of this 
    > Regulation, the European Data Protection Supervisor shall act as their market surveillance 
    > authority, except in relation to the Court of Justice of the European Union acting in its 
    > judicial capacity.



---

## Node: provision of information
<a name="node-provision-of-information"></a>

*0 outgoing, 5 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Obtains ← commission**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Provides ← deployers**
  - Chapter 3: High-risk ai systems
  - Article 8: Transparency and provision of information to deployers
  - Paragraph 1

    > Article 13
    > Transparency and provision of information to deployers

- **Defines ← purpose of request**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

- **Establishes ← legal basis**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.

- **Specifies ← information technologies**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 20: Power to request documentation and information
  - Paragraph 5

    > 4. The request for information shall state the legal basis and the purpose of the request, 
    > specify what information is required, and set a period within which the information is to 
    > be provided, and indicate the fines provided for in Article 101 for supplying incorrect, 
    > incomplete or misleading information.



---

## Node: workers' rights
<a name="node-workers-rights"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Protecting → ai systems or ai models**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

### Incoming relationships

_(none)_



---

## Node: delegated act
<a name="node-delegated-act"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **Precondition for entering into force → european parliament or council**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 7

    > 6. Any delegated act adopted pursuant Article 6(6), Article 7(1) and (3), Article 11(3), Article 
    > 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall 
    > enter into force only if no objection has been expressed by either the European Parliament 
    > or the Council within a period of three months of notification of that act to the European 
    > Parliament and the Council or if, before the expiry of that period, the European Parliament 
    > and the Council have both informed the Commission that they will not object. That period 
    > shall be extended by three months at the initiative of the European Parliament or of the 
    > Council.

- **Precondition for entering into force → clear objectives**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 7

    > 6. Any delegated act adopted pursuant Article 6(6), Article 7(1) and (3), Article 11(3), Article 
    > 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall 
    > enter into force only if no objection has been expressed by either the European Parliament 
    > or the Council within a period of three months of notification of that act to the European 
    > Parliament and the Council or if, before the expiry of that period, the European Parliament 
    > and the Council have both informed the Commission that they will not object. That period 
    > shall be extended by three months at the initiative of the European Parliament or of the 
    > Council.

### Incoming relationships

- **Has power to adopt ← commission**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 1: Exercise of the delegation
  - Paragraph 2

    > 1. The power to adopt delegated acts is conferred on the Commission subject to the 
    > conditions laid down in this Article.

- **Performs ← commission**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.



---

## Node: law
<a name="node-law"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Expertise, where applicable, for their function ← competent personnel**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.



---

## Node: ce marking
<a name="node-ce-marking"></a>

*0 outgoing, 3 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Non-affixing requires non-compliance to end ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Affixes ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 11: Obligations of providers of high-risk ai systems
  - Paragraph 1

    > Article 16
    > Obligations of providers of high-risk AI systems 
    > Providers of high-risk AI systems shall:
    > (a) ensure that their high-risk AI systems are compliant with the requirements set out in 
    > Section 2;
    > (b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its 
    > accompanying documentation, as applicable their name, registered trade name or 
    > registered trade mark, the address at which they can be contacted;
    > (c) have a quality management system in place which complies with Article 17;
    > (d) keep the documentation referred to in Article 18;
    > (e) when under their control, keep the logs automatically generated by their high-risk AI 
    > systems as referred to in Article 19;
    > (f) ensure that the high-risk AI system undergoes the relevant conformity assessment 
    > procedure as referred to in Article 43, prior to its being placed on the market or put into 
    > service;
    > (g) draw up an EU declaration of conformity in accordance with Article 47;
    > (h) affix the CE marking to the high-risk AI system or, where that is not possible, on its 
    > packaging or its accompanying documentation, to indicate conformity with this 
    > Regulation, in accordance with Article 48;
    > (i) comply with the registration obligations referred to in Article 49(1);
    > (j) take the necessary corrective actions and provide information as required in Article 20;
    > (k) upon a reasoned request of a national competent authority, demonstrate the conformity of 
    > the high-risk AI system with the requirements set out in Section 2;
    > (l) ensure that the high-risk AI system complies with accessibility requirements in 
    > accordance with Directives (EU) 2016/2102 and (EU) 2019/882.

- **Requires ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 1

    > Article 30
    > Notification procedure



---

## Node: high-impact capabilities
<a name="node-high-impact-capabilities"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **equivalent to or exceed the capabilities of → general-purpose ai models**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 65

    > (64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities 
    > recorded in the most advanced general-purpose AI models;

### Incoming relationships

- **presumed to have ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 3

    > 2. A general-purpose AI model shall be presumed to have high impact capabilities 
    > pursuant to paragraph 1, point (a), when the cumulative amount of computation used 
    > for its training measured in FLOPs is greater than 10^25.

- **Provides ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 7: Record-keeping
  - Paragraph 4

    > 3. For high-risk AI systems referred to in point 1 (a) of Annex III, the logging capabilities 
    > shall provide, at a minimum:
    > (a) recording of the period of each use of the system (start date and time and end date 
    > and time of each use);
    > (b) the reference database against which input data has been checked by the system;
    > (c) the input data for which the search has led to a match;
    > (d) the identification of the natural persons involved in the verification of the results, as 
    > referred to in Article 14(5).



---

## Node: liability
<a name="node-liability"></a>

*0 outgoing, 4 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Affected by ← providers of intermediary services**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 7

    > 5. This Regulation shall not affect the application of the provisions on the liability of 
    > providers of intermediary services as set out in Chapter II of Regulation (EU) 2022/2065.

- **Maintains ← board**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 8

    > 7. The Board shall be organised and operated so as to safeguard the objectivity and 
    > impartiality of its activities.

- **Operates to safeguard ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.

- **Promotes and applies ← committee procedure**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.



---

## Node: digital ce marking
<a name="node-digital-ce-marking"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Requires ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 3

    > 2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it 
    > can easily be accessed via the interface from which that system is accessed or via an 
    > easily accessible machine-readable code or other electronic means.



---

## Node: bias detection and correction
<a name="node-bias-detection-and-correction"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Related to ← special categories of personal data**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 6

    > 5. To the extent that it is strictly necessary for the purpose of ensuring bias  detection and 
    > correction in relation to the high-risk AI systems in accordance with paragraph (2), points
    > (f) and (g) of this Article, the providers of such systems may exceptionally process special 
    > categories of personal data, subject to appropriate safeguards for the fundamental rights 
    > and freedoms of natural persons. In addition to the provisions set out in Regulation (EU) 
    > 2016/679, Directive (EU) 2016/680 and Regulation (EU) 2018/1725, all the following 
    > conditions shall apply in order for such processing to occur:
    > (a) the bias detection and correction cannot be effectively fulfilled by processing other 
    > data, including synthetic or anonymised data;
    > (b) the special categories of personal data are subject to technical limitations on the 
    > re-use of the personal data, and state of the art security and privacy-preserving 
    > measures, including pseudonymisation;
    > (c) the special categories of personal data are subject to measures to ensure that the 
    > personal data processed are secured, protected, subject to suitable safeguards, 
    > including strict controls and documentation of the access, to avoid misuse and 
    > ensure that only authorised persons with appropriate confidentiality obligations 
    > have access to those personal data;
    > (d) the personal data in the special categories of personal data are not to be 
    > transmitted, transferred or otherwise accessed by other parties;
    > (e) the personal data in the special categories of personal data are deleted once the 
    > bias has been corrected or the personal data has reached the end of its retention 
    > period, whichever comes first;
    > (f) the records of processing activities pursuant to Regulations (EU) 2016/679 and 
    > (EU) 2018/1725 and Directive (EU) 2016/680include the reasons why the 
    > processing of special categories of personal data was strictly necessary to detect 
    > and correct biases, and why that objective could not be achieved by processing 
    > other data.



---

## Node: ['providers', 'prospective providers', 'deployers']
<a name="node-providers-prospective-providers-deployers"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Makes available to ← commission**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 7

    > 6. The Commission shall be the controller of the EU database. It shall make available to 
    > providers, prospective providers and deployers adequate technical and administrative 
    > support. The EU database shall comply with the applicable accessibility requirements.



---

## Node: council
<a name="node-council"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Takes into account positions and findings ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.

- **Recipient of ← importers**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 15

    > 13. By … [seven years from the date of entry into force of this Regulation], the Commission 
    > shall carry out an assessment of the enforcement of this Regulation and shall report on 
    > it to the European Parliament, the Council and the European Economic and Social 
    > Committee, taking into account the first years of application of this Regulation. On the 
    > basis of the findings, that report shall, where appropriate, be accompanied by a proposal 
    > for amendment of this Regulation with regard to the structure of enforcement and the 
    > need for a Union agency to resolve any identified shortcomings.



---

## Node: innovative ai systems
<a name="node-innovative-ai-systems"></a>

*4 outgoing, 0 incoming*

### Outgoing relationships

- **Associated with the operation of → european data protection supervisor**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 11

    > 10. National competent authorities shall ensure that, to the extent the innovative AI systems 
    > involve the processing of personal data or otherwise fall under the supervisory remit of 
    > other national authorities or competent authorities providing or supporting access to data, 
    > the national data protection authorities and those other national or competent authorities 
    > are associated with the operation of the AI regulatory sandbox and involved in the 
    > supervision of those aspects to the extent of their respective tasks and powers.

- **Tested in → testing in real-world conditions**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 4

    > 3. The testing of high-risk AI systems in real world conditions under this Article shall be 
    > without prejudice to any ethical review that is required by Union or national law.

- **Not mutually exclusive with → ethical review**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 4

    > 3. The testing of high-risk AI systems in real world conditions under this Article shall be 
    > without prejudice to any ethical review that is required by Union or national law.

- **Presents → risk**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 1

    > Article 82
    > Compliant AI systems which present a risk

### Incoming relationships

_(none)_



---

## Node: member states and their relevant authorities
<a name="node-member-states-and-their-relevant-authorities"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Exchange of information → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.

### Incoming relationships

- **Dissemination of warnings ← commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 6

    > 4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member 
    > States and their relevant authorities, as well as those of notified bodies, with regard to the 
    > exchange of information and the dissemination of warnings, including in the context of 
    > cross-border cooperation, nor shall they affect the obligations of the parties concerned to 
    > provide information under criminal law of the Member States.



---

## Node: effects and possible interaction
<a name="node-effects-and-possible-interaction"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **give due consideration to ← enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 5

    > 4. The risk management measures referred to in paragraph 2, point (d), shall give due 
    > consideration to the effects and possible interaction resulting from the combined 
    > application of the requirements set out in this Section, with a view to minimising risks 
    > more effectively while achieving an appropriate balance in implementing the measures 
    > to fulfil those requirements.



---

## Node: data governance
<a name="node-data-governance"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Relates to ← data**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 1

    > Article 10
    > Data and data governance



---

## Node: board's rules of procedure
<a name="node-board-s-rules-of-procedure"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Designated representatives of ← union or member states**
  - Chapter 7: Governance
  - Article 2: Establishment and structure of the european artificial intelligence board
  - Paragraph 6

    > 5. The designated representatives of the Member States shall adopt the Board’s rules of 
    > procedure by a two-thirds majority. The rules of procedure shall, in particular, lay down 
    > procedures for the selection process, the duration of the mandate of, and specifications 
    > of the tasks of, the Chair, detailed arrangements for voting, and the organisation of the 
    > Board’s activities and those of its sub-groups.



---

## Node: right to explanation
<a name="node-right-to-explanation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Performs ← individual decision-making**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 15: Right to explanation of individual decision-making
  - Paragraph 1

    > Article 86
    > Right to explanation of individual decision-making



---

## Node: key performance indicators
<a name="node-key-performance-indicators"></a>

*2 outgoing, 2 incoming*

### Outgoing relationships

- **To measure → achievement of those objectives**
  - Chapter 10: Codes of conduct and guidelines
  - Article 1: Codes of conduct for voluntary application of specific requirements
  - Paragraph 3

    > 2. The AI Office and the Member States shall  facilitate the drawing up of codes of conduct 
    > concerning the voluntary application, including by deployers, of specific requirements to 
    > all AI systems, on the basis of clear objectives and key performance indicators to 
    > measure the achievement of those objectives, including elements such as, but not limited 
    > to:
    > (a) applicable elements provided for in Union ethical guidelines for trustworthy AI;
    > (b) assessing and minimising the impact of AI systems on environmental 
    > sustainability, including as regards energy-efficient programming and techniques 
    > for the efficient design, training and use of AI;
    > (c) promoting AI literacy, in particular that of persons dealing with the development, 
    > operation and use of AI;
    > (d) facilitating an inclusive and diverse design of AI systems, including through the 
    > establishment of inclusive and diverse development teams and the promotion of 
    > stakeholders’ participation in that process;
    > (e) assessing and preventing the negative impact of AI systems on vulnerable persons 
    > or groups of vulnerable persons, including as regards accessibility for persons with 
    > a disability, as well as on gender equality.

- **Correlates with → market and technological developments**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.

### Incoming relationships

- **Supports ← commission**
  - Chapter 5: General-purpose ai models
  - Article 1: Classification of general-purpose ai models as general-purpose ai models with systemic risk
  - Paragraph 4

    > 3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the 
    > thresholds listed in paragraphs 2 and 3 of this Article, as well as to supplement 
    > benchmarks and indicators in light of evolving technological developments, such as 
    > algorithmic improvements or increased hardware efficiency, when necessary, for these 
    > thresholds to reflect the state of the art.

- **Ensures reflection in reporting commitments ← ai office**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 6

    > 5. The AI Office shall aim to ensure that participants to the codes of practice report 
    > regularly to the AI Office on the implementation of the commitments and the measures 
    > taken and their outcomes, including as measured against the key performance indicators 
    > as appropriate. Key performance indicators and reporting commitments shall reflect 
    > differences in size and capacity between various participants.



---

## Node: implementation of regulation
<a name="node-implementation-of-regulation"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Oversees ← commission**
  - Chapter 10: Codes of conduct and guidelines
  - Article 2: Guidelines from the commission on the implementation of this regulation
  - Paragraph 1

    > Article 96
    > Guidelines from the Commission on the implementation of this Regulation



---

## Node: data management
<a name="node-data-management"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Includes → data management**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.

### Incoming relationships

- **Includes ← data management**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 2

    > 1. Providers of high-risk AI systems shall put a quality management system in place that 
    > ensures compliance with this Regulation. That system shall be documented in a systematic 
    > and orderly manner in the form of written policies, procedures and instructions, and shall 
    > include at least the following aspects:
    > (a) a strategy for regulatory compliance, including compliance with conformity 
    > assessment procedures and procedures for the management of modifications to the 
    > high-risk AI system;
    > (b) techniques, procedures and systematic actions to be used for the design, design 
    > control and design verification of the high-risk AI system;
    > (c) techniques, procedures and systematic actions to be used for the development, 
    > quality control and quality assurance of the high-risk AI system;
    > (d) examination, test and validation procedures to be carried out before, during and after 
    > the development of the high-risk AI system, and the frequency with which they have 
    > to be carried out;
    > (e) technical specifications, including standards, to be applied and, where the relevant 
    > harmonised standards are not applied in full or do not cover all of the relevant 
    > requirements set out in Section 2, the means to be used to ensure that the high-risk 
    > AI system complies with those requirements ;
    > (f) systems and procedures for data management, including data acquisition, data 
    > collection, data analysis, data labelling, data storage, data filtration, data mining, data 
    > aggregation, data retention and any other operation regarding the data that is 
    > performed before and for the purpose of the placing on the market or the putting into 
    > service of high-risk AI systems;
    > (g) the risk management system referred to in Article 9;
    > (h) the setting-up, implementation and maintenance of a post-market monitoring system, 
    > in accordance with Article 72;
    > (i) procedures related to the reporting of a serious incident in accordance with 
    > Article 73;
    > (j) the handling of communication with national competent authorities, other relevant 
    > authorities, including those providing or supporting the access to data, notified 
    > bodies, other operators, customers or other interested parties;
    > (k) systems and procedures for record-keeping of all relevant documentation and 
    > information;
    > (l) resource management, including security-of-supply related measures;
    > (m) an accountability framework setting out the responsibilities of the management and 
    > other staff with regard to all the aspects listed in this paragraph.



---

## Node: union institution
<a name="node-union-institution"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Contributes to → fines imposed under this article**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 7

    > 6. Funds collected by imposition of fines in this Article shall contribute to the general 
    > budget of the Union. The fines shall not affect the effective operation of the Union 
    > institution, body, office or agency fined.

### Incoming relationships

_(none)_



---

## Node: certificate
<a name="node-certificate"></a>

*6 outgoing, 6 incoming*

### Outgoing relationships

- **contributes to the development of tools and templates → general-purpose ai models**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 4

    > 3. The scientific panel shall advise and support the AI Office, in particular with regard to 
    > the following tasks:
    > (a) supporting the implementation and enforcement of this Regulation as regards 
    > general-purpose AI models and systems, in particular by:
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > 
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (i) alerting the AI Office of possible systemic risks at Union level of general-
    > purpose AI models, in accordance with Article 90;
    > (ii) contributing to the development of tools and methodologies for evaluating 
    > capabilities of general-purpose AI models and systems, including through 
    > benchmarks;
    > (iii) providing advice on the classification of general-purpose AI models with 
    > systemic risk;
    > (iv) providing advice on the classification of various general-purpose AI models 
    > and systems;
    > (v) contributing to the development of tools and templates;
    > (b) supporting the work of market surveillance authorities, at their request;
    > (c) supporting cross-border market surveillance activities as referred to in 
    > Article 74(11), without prejudice to the powers of market surveillance authorities;
    > (d) supporting the AI Office in carrying out its duties in the context of the safeguard 
    > clause pursuant to Article 81.

- **Supports → requirements set out in this section**
  - Chapter 7: Governance
  - Article 6: Access to the pool of experts by the member states
  - Paragraph 2

    > 1. Member States may call upon experts of the scientific panel to support their enforcement 
    > activities under this Regulation.

- **upholds → impartiality and objectivity**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.

- **ensures → confidentiality of information and data obtained**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.

- **does not seek or take → instructions for use accompanying the systems**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 5

    > 4. The experts on the scientific panel shall perform their tasks with impartiality and 
    > objectivity, and shall ensure the confidentiality of information and data obtained in 
    > carrying out their tasks and activities. They shall neither seek nor take instructions from 
    > anyone when exercising their tasks under paragraph 3. Each expert shall draw up a 
    > declaration of interests, which shall be made publicly available. The AI Office shall 
    > establish systems and procedures to actively manage and prevent potential conflicts of 
    > interest.

- **Reason by → expert**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.

### Incoming relationships

- **Establishes ← commission**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.

- **qualified alert ← ai office**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 2

    > 1. The scientific panel may provide a qualified alert to the AI Office where it has reason to 
    > suspect that:
    > (a) a general-purpose AI model poses concrete identifiable risk at Union level; or,
    > (b) a general-purpose AI model meets the requirements referred to in Article 51 .

- **suspends or withdraws ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **Valid for period indicated ← ai systems or ai models**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 3

    > 2. Certificates shall be valid for the period they indicate, which shall not exceed five years for 
    > AI systems covered by Annex I, and four years for AI systems covered by Annex III. On 
    > the application of the provider, the validity of a certificate may be extended for further 
    > periods, each not exceeding five years for AI systems covered by Annex I, and four years 
    > for AI systems covered by Annex III, based on a re-assessment in accordance with the 
    > applicable conformity assessment procedures. Any supplement to a certificate shall 
    > remain valid, provided that the certificate which it supplements is valid.

- **Applied for ← providers**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 3

    > 2. Certificates shall be valid for the period they indicate, which shall not exceed five years for 
    > AI systems covered by Annex I, and four years for AI systems covered by Annex III. On 
    > the application of the provider, the validity of a certificate may be extended for further 
    > periods, each not exceeding five years for AI systems covered by Annex I, and four years 
    > for AI systems covered by Annex III, based on a re-assessment in accordance with the 
    > applicable conformity assessment procedures. Any supplement to a certificate shall 
    > remain valid, provided that the certificate which it supplements is valid.

- **poses concrete identifiable risk at Union level ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 2

    > 1. The scientific panel may provide a qualified alert to the AI Office where it has reason to 
    > suspect that:
    > (a) a general-purpose AI model poses concrete identifiable risk at Union level; or,
    > (b) a general-purpose AI model meets the requirements referred to in Article 51 .



---

## Node: scientific research and development
<a name="node-scientific-research-and-development"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Sole purpose ← ai systems or ai models**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 8

    > 6. This Regulation does not apply to AI systems or AI models, including their output, 
    > specifically developed and put into service for the sole purpose of scientific research and 
    > development.

- **Related to ← article 6(1)**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 1

    > Article 68
    > Scientific panel of independent experts



---

## Node: residual risk associated with each hazard
<a name="node-residual-risk-associated-with-each-hazard"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **affects ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 6

    > 5. The risk management measures referred to in paragraph 2, point (d), shall be such that the 
    > relevant residual risk associated with each hazard, as well as the overall residual risk of the 
    > high-risk AI systems is judged to be acceptable.
    > In identifying the most appropriate risk management measures, the following shall be 
    > ensured:
    > (a) elimination or reduction of identified and evaluated risks pursuant to paragraph 2 
    > as far as technically feasible through adequate design and development of the high-
    > risk AI system;
    > (b) where appropriate, implementation of adequate mitigation and control measures 
    > addressing risks that cannot be eliminated;
    > (c) provision of information required pursuant to Article 13 and, where appropriate, 
    > training to deployers. 
    > With a view to eliminating or reducing risks related to the use of the high-risk AI system, 
    > due consideration shall be given to the technical knowledge, experience, education, the 
    > training to be expected by the deployer, and the presumable context in which the system is 
    > intended to be used.



---

## Node: human-centric and trustworthy artificial intelligence
<a name="node-human-centric-and-trustworthy-artificial-intelligence"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **promotes ← regulation**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 2

    > 1. The purpose of this Regulation is to improve the functioning of the internal market and 
    > promote the uptake of human-centric and trustworthy artificial intelligence (AI), while 
    > ensuring a high level of protection of health, safety, fundamental rights enshrined in the 
    > Charter of Fundamental Rights, including democracy, the rule of law and 
    > environmental protection, against the harmful effects of artificial intelligence systems 
    > (AI systems) in the Union, and to support innovation.



---

## Node: other relevant bodies or sources
<a name="node-other-relevant-bodies-or-sources"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Takes into account positions and findings ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 11

    > 9. In carrying out the evaluations and reviews referred to in paragraphs 1 to 7, the 
    > Commission shall take into account the positions and findings of the Board, of the 
    > European Parliament, of the Council, and of other relevant bodies or sources.



---

## Node: effective
<a name="node-effective"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Causes ← fines**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 4

    > 3. Fines imposed in accordance with this Article shall be effective, proportionate and 
    > dissuasive.



---

## Node: arguments submitted pursuant to paragraph 2
<a name="node-arguments-submitted-pursuant-to-paragraph-2"></a>

*1 outgoing, 0 incoming*

### Outgoing relationships

- **Does not sufficiently substantiate → general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 4

    > 3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 
    > are not sufficiently substantiated and the relevant provider was not able to demonstrate 
    > that the general-purpose AI model does not present, due to its specific characteristics, 
    > systemic risks, it shall reject those arguments, and the general-purpose AI model shall be 
    > considered to be a general-purpose AI model with systemic risk.

### Incoming relationships

_(none)_



---

## Node: anomalies, dysfunctions and unexpected performance
<a name="node-anomalies-dysfunctions-and-unexpected-performance"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **facilitates detection of ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.



---

## Node: public
<a name="node-public"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Makes annual reports or abstracts thereof available ← national competent authorities**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 17

    > 16. National competent authorities shall submit to the AI Office and to the Board, annual 
    > reports, starting one year after the establishment of the AI regulatory sandbox and every 
    > year thereafter until its termination and a final report. Those reports shall provide 
    > information on the progress and results of the implementation of those sandboxes, 
    > including best practices, incidents, lessons learnt and recommendations on their setup 
    > and, where relevant, on the application and possible revision of this Regulation, 
    > including its delegated and implementing acts, and on the application of other Union 
    > law supervised by the competent authorities within the sandbox. The national competent 
    > authorities shall make those annual reports or abstracts thereof available to the public, 
    > online. The Commission shall, where appropriate, take the annual reports into account 
    > when exercising its tasks under this Regulation.

- **Should be accessible and publicly available ← eu database**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 5

    > 4. With the exception of the section referred to in Article 49(4) and Article 60(5), the 
    > information contained in the EU database registered in accordance with Article 49 shall 
    > be accessible and publicly available in a user-friendly manner. The information should 
    > be easily navigable and machine-readable. The information registered in accordance 
    > with Article 60 shall be accessible only to market surveillance authorities and the 
    > Commission, unless the prospective provider or provider has given consent for also 
    > making the information accessible the public.



---

## Node: fines imposed under this article
<a name="node-fines-imposed-under-this-article"></a>

*1 outgoing, 1 incoming*

### Outgoing relationships

- **Communicated to → board**
  - Chapter 12: Penalties 
  - Article 3: Fines for providers of general-purpose ai models
  - Paragraph 5

    > 4. Information on fines imposed under this Article shall also be communicated to the 
    > Board as appropriate.

### Incoming relationships

- **Contributes to ← union institution**
  - Chapter 12: Penalties 
  - Article 2: Administrative fines on union institutions, bodies, offices and agencies
  - Paragraph 7

    > 6. Funds collected by imposition of fines in this Article shall contribute to the general 
    > budget of the Union. The fines shall not affect the effective operation of the Union 
    > institution, body, office or agency fined.



---

## Node: level of damage suffered by them
<a name="node-level-of-damage-suffered-by-them"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **considers ← administrative fine**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 9

    > 7. When deciding whether to impose an administrative fine and when deciding on the 
    > amount of the administrative fine in each individual case, all relevant circumstances of the 
    > specific situation shall be taken into account and, as appropriate, regard shall be given to 
    > the following:
    > (a) the nature, gravity and duration of the infringement and of its consequences, taking 
    > into account the purpose of the AI system, as well as, where appropriate, the 
    > number of affected persons and the level of damage suffered by them;
    > (b) whether administrative fines have already been applied by other market surveillance 
    > authorities of one or more Member States to the same operator for the same 
    > infringement;
    > (c) whether administrative fines have already been applied by other authorities to the 
    > same operator for infringements of other Union or national law, when such 
    > infringements result from the same activity or omission constituting a relevant 
    > infringement of this Regulation;
    > (d) the size, the annual turnover and market share of the operator committing the 
    > infringement;
    > (e) any other aggravating or mitigating factor applicable to the circumstances of the 
    > case, such as financial benefits gained, or losses avoided, directly or indirectly, 
    > from the infringement;
    > (f) the degree of cooperation with the national competent authorities, in order to 
    > remedy the infringement and mitigate the possible adverse effects of the 
    > infringement;
    > (g) the degree of responsibility of the operator taking into account the technical and 
    > organisational measures implemented by it;
    > (h) the manner in which the infringement became known to the national competent 
    > authorities, in particular whether, and if so to what extent, the operator notified 
    > the infringement;
    > (i) the intentional or negligent character of the infringement;
    > (j) any action taken by the operator to mitigate the harm suffered by the affected 
    > persons.



---

## Node: third country
<a name="node-third-country"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Use ← ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 6

    > 4. This Regulation applies neither to public authorities in a third country nor to international 
    > organisations falling within the scope of this Regulation pursuant to paragraph 1, where 
    > those authorities or organisations use AI systems in the framework of international 
    > cooperation or agreements for law enforcement and judicial cooperation with the Union or 
    > with one or more Member States, provided that such a third country or international 
    > organisation provides adequate safeguards with respect to the protection of fundamental 
    > rights and freedoms of individuals.



---

## Node: market surveillance authorities, commission
<a name="node-market-surveillance-authorities-commission"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Accessible only to ← eu database**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 5

    > 4. With the exception of the section referred to in Article 49(4) and Article 60(5), the 
    > information contained in the EU database registered in accordance with Article 49 shall 
    > be accessible and publicly available in a user-friendly manner. The information should 
    > be easily navigable and machine-readable. The information registered in accordance 
    > with Article 60 shall be accessible only to market surveillance authorities and the 
    > Commission, unless the prospective provider or provider has given consent for also 
    > making the information accessible the public.



---

## Node: annex iii
<a name="node-annex-iii"></a>

*1 outgoing, 2 incoming*

### Outgoing relationships

- **Involves → amendments**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 1

    > Article 7
    > Amendments to Annex III

### Incoming relationships

- **Updates through delegated acts ← commission**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 6

    > 5. The Commission shall adopt delegated acts in accordance with Article 97 to update 
    > Annexes VI and VII in  light of technical progress.

- **Add or modify use-cases ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 2: Amendments to annex iii
  - Paragraph 2

    > 1. The Commission shall adopt delegated acts in accordance with Article 97 to amend Annex 
    > III by adding or modifying use-cases of high-risk AI systems where both of the following 
    > conditions are fulfilled:
    > (a) the AI systems are intended to be used in any of the areas listed in Annex III;
    > (b) the AI systems pose a risk of harm to  health and safety, or an adverse impact on 
    > fundamental rights, and that risk is equivalent to, or greater than, the risk of harm or 
    > of adverse impact posed by the high-risk AI systems already referred to in Annex III.



---

## Node: directive (eu) 2022/2557
<a name="node-directive-eu-2022-2557"></a>

*5 outgoing, 1 incoming*

### Outgoing relationships

- **Applies to → responsibilities**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 13

    > 13 of Directive (EU) 2016/680 shall apply.

- **Facilitates → obligations laid down in this regulation**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.

- **Applies to → inspections, investigations or audits**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.

- **Does not contravene → public and national security interests**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.

- **Does not prevent → laws, regulations or administrative provisions**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 7: Confidentiality
  - Paragraph 3

    > 60 Directive (EU) 2016/943 of the European Parliament and of the Council of 8 June 2016 on 
    > the protection of undisclosed know-how and business information (trade secrets) against 
    > their unlawful acquisition, use and disclosure (OJ L 157, 15.6.2016, p. 1).
    > (b) the effective implementation of this Regulation, in particular for the purposes of 
    > inspections, investigations or audits;
    > (c) public and national security interests;
    > (d) the conduct of criminal or administrative proceedings;
    > (e) information classified pursuant to Union or national law.

### Incoming relationships

- **defines ← critical infrastructure**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 63

    > (62) ‘critical infrastructure’ means critical infrastructure as defined in Article 2, point (4), of 
    > Directive (EU) 2022/2557;



---

## Node: article 6(1)
<a name="node-article-6-1"></a>

*13 outgoing, 4 incoming*

### Outgoing relationships

- **Involves → committee procedure**
  - Chapter 11: Delegation of power and committee procedure 
  - Article 2: Committee procedure
  - Paragraph 1

    > Article 98
    > Committee procedure

- **Related to → evaluation and review**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 1

    > Article 112
    > Evaluation and review

- **Applicable from → partial application**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Applicable to → ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 1: Ai regulatory sandboxes
  - Paragraph 1

    > Article 57
    > AI regulatory sandboxes

- **Applies to → enforcement measures**
  - Chapter 3: High-risk ai systems
  - Article 4: Risk management system
  - Paragraph 1

    > Article 9
    > Risk management system

- **Requires → partial application**
  - Chapter 3: High-risk ai systems
  - Article 6: Technical documentation
  - Paragraph 1

    > Article 11
    > Technical documentation

- **Applies to → identification number of the notified body**
  - Chapter 3: High-risk ai systems
  - Article 30: Identification numbers and lists of notified bodies
  - Paragraph 1

    > Article 35
    > Identification numbers and lists of notified bodies

- **Applicable to → partial application**
  - Chapter 3: High-risk ai systems
  - Article 36: Common specifications
  - Paragraph 1

    > Article 41
    > Common specifications

- **Satisfies → specific requirements**
  - Chapter 3: High-risk ai systems
  - Article 37: Presumption of conformity with certain requirements
  - Paragraph 1

    > Article 42
    > Presumption of conformity with certain requirements

- **Applicable to → providers**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 1

    > Article 52
    > Procedure

- **Applicable to → codes of conduct**
  - Chapter 5: General-purpose ai models
  - Article 6: Codes of practice
  - Paragraph 1

    > Article 56
    > Codes of practice

- **Applicable to → advisory forum**
  - Chapter 7: Governance
  - Article 4: Advisory forum
  - Paragraph 1

    > Article 67
    > Advisory forum

- **Related to → scientific research and development**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 1

    > Article 68
    > Scientific panel of independent experts

### Incoming relationships

- **Classified as ← operators of high-risk ai systems**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 3

    > 2. For  AI systems classified as high-risk AI systems in accordance with Article 6(1) and

- **Complies with ← commission**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 4

    > 3. The Commission shall ensure that all sensitive information obtained in the course of its 
    > investigations pursuant to this Article is treated confidentially in accordance with 
    > Article 78.

- **Applies to ← ai regulatory sandbox**
  - Chapter 6: Measures in support of innovation
  - Article 2: Detailed arrangements for and functioning of ai regulatory sandboxes
  - Paragraph 1

    > Article 58
    > Detailed arrangements for and functioning of AI regulatory sandboxes

- **Applies to ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 1

    > Article 28
    > Notifying authorities



---

## Node: conformity assessment body
<a name="node-conformity-assessment-body"></a>

*7 outgoing, 5 incoming*

### Outgoing relationships

- **performs → testing**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 22

    > (21) ‘conformity assessment body’ means a body that performs third-party conformity 
    > assessment activities, including testing, certification and inspection;

- **listed in Section B of Annex I; → notifying authority**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 23

    > (22) ‘notified body’ means a conformity assessment body notified in accordance with this 
    > Regulation and other relevant Union harmonisation legislation as listed in Section B of 
    > Annex I;

- **Performs → proportional**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 1

    > Article 29
    > Application of a conformity assessment body for notification

- **Can perform activities of → notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 5

    > 4. The conformity assessment body concerned may perform the activities of a notified body 
    > only where no objections are raised by the Commission or the other Member States within 
    > two weeks of a notification by a notifying authority where it includes an accreditation 
    > certificate referred to in Article 29(2), or within two months of a notification by the 
    > notifying authority where it includes documentary evidence referred to in Article 29(3).

- **Accredited by → national accreditation body**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 3

    > 2. The application for notification shall be accompanied by a description of the conformity 
    > assessment activities, the conformity assessment module or modules and the types of AI 
    > systems for which the conformity assessment body claims to be competent, as well as by 
    > an accreditation certificate, where one exists, issued by a national accreditation body 
    > attesting that the conformity assessment body fulfils the requirements laid down in 
    > Article 31. 
    > Any valid document related to existing designations of the applicant notified body under 
    > any other Union harmonisation legislation shall be added.

- **Claims competence for → union and national law**
  - Chapter 3: High-risk ai systems
  - Article 24: Application of a conformity assessment body for notification
  - Paragraph 3

    > 2. The application for notification shall be accompanied by a description of the conformity 
    > assessment activities, the conformity assessment module or modules and the types of AI 
    > systems for which the conformity assessment body claims to be competent, as well as by 
    > an accreditation certificate, where one exists, issued by a national accreditation body 
    > attesting that the conformity assessment body fulfils the requirements laid down in 
    > Article 31. 
    > Any valid document related to existing designations of the applicant notified body under 
    > any other Union harmonisation legislation shall be added.

- **Allows for exemption from → regulation**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 1

    > Article 46
    > Derogation from conformity assessment procedure

### Incoming relationships

- **Consultations initiated by ← commission**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 6

    > 5. Where objections are raised, the Commission shall, without delay, enter into 
    > consultations with the relevant Member States and the conformity assessment body. 
    > Having regard thereto, the Commission shall decide whether the authorisation is 
    > justified. The Commission shall address its decision to the Member State concerned and 
    > the relevant conformity assessment body.

- **Compliance monitoring ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 1

    > Article 43
    > Conformity assessment

- **Undergoes in event of substantial modification ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 38: Conformity assessment
  - Paragraph 5

    > 4. High-risk AI systems that have already been subject to a conformity assessment 
    > procedure shall undergo a new conformity assessment procedure in the event of a 
    > substantial modification, regardless of whether the modified system is intended to be 
    > further distributed or continues to be used by the current deployer.
    > For high-risk AI systems that continue to learn after being placed on the market or put into 
    > service, changes to the high-risk AI system and its performance that have been pre-
    > determined by the provider at the moment of the initial conformity assessment and are part 
    > of the information contained in the technical documentation referred to in point 2(f) of 
    > Annex IV, shall not constitute a substantial modification.

- **Provides ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 25: Notification procedure
  - Paragraph 4

    > 3. The notification referred to in paragraph 2 of this Article shall include full details of the 
    > conformity assessment activities, the conformity assessment module or modules, the types 
    > of AI systems concerned, and the relevant attestation of competence. Where a 
    > notification is not based on an accreditation certificate as referred to in Article 29(2), the 
    > notifying authority shall provide the Commission and the other Member States with 
    > documentary evidence which attests to the competence of the conformity assessment 
    > body and to the arrangements in place to ensure that that body will be monitored 
    > regularly and will continue to satisfy the requirements laid down in Article 31.

- **Provides Union AI conformity assessment for testing purposes ← union ai testing support structures**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 13: Union ai testing support structures
  - Paragraph 1

    > Article 84
    > Union AI testing support structures



---

## Node: competent personnel
<a name="node-competent-personnel"></a>

*3 outgoing, 1 incoming*

### Outgoing relationships

- **Expertise in fields such as → information technologies**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.

- **Expertise in fields such as → ai**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.

- **Expertise, where applicable, for their function → law**
  - Chapter 3: High-risk ai systems
  - Article 23: Notifying authorities
  - Paragraph 8

    > 7. Notifying authorities shall have an adequate number of competent personnel at their 
    > disposal for the proper performance of their tasks. Competent personnel shall have the 
    > necessary expertise, where applicable, for their function, in fields such as information 
    > technologies, AI and law, including the supervision of fundamental rights.

### Incoming relationships

- **Challenges notified bodies' competence ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 32: Challenge to the competence of notified bodies
  - Paragraph 1

    > Article 37
    > Challenge to the competence of notified bodies



---

## Node: eu database
<a name="node-eu-database"></a>

*2 outgoing, 1 incoming*

### Outgoing relationships

- **Accessible only to → market surveillance authorities, commission**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 5

    > 4. With the exception of the section referred to in Article 49(4) and Article 60(5), the 
    > information contained in the EU database registered in accordance with Article 49 shall 
    > be accessible and publicly available in a user-friendly manner. The information should 
    > be easily navigable and machine-readable. The information registered in accordance 
    > with Article 60 shall be accessible only to market surveillance authorities and the 
    > Commission, unless the prospective provider or provider has given consent for also 
    > making the information accessible the public.

- **Should be accessible and publicly available → public**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 5

    > 4. With the exception of the section referred to in Article 49(4) and Article 60(5), the 
    > information contained in the EU database registered in accordance with Article 49 shall 
    > be accessible and publicly available in a user-friendly manner. The information should 
    > be easily navigable and machine-readable. The information registered in accordance 
    > with Article 60 shall be accessible only to market surveillance authorities and the 
    > Commission, unless the prospective provider or provider has given consent for also 
    > making the information accessible the public.

### Incoming relationships

- **Enters data into ← providers**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 3

    > 2. The data listed in Section A of Annex VIII shall be entered into the EU database by the 
    > provider or, where applicable, by the authorised representative.



---

## Node: failure to provide access
<a name="node-failure-to-provide-access"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Penalty for not providing ← logs referred to in article 12(1)**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 5

    > 4. The request for access shall state the legal basis, the purpose and reasons of the request 
    > and set the period within which the access is to be provided, and the fines provided for in 
    > Article 101 for failure to provide access.



---

## Node: placing on the market
<a name="node-placing-on-the-market"></a>

*2 outgoing, 1 incoming*

### Outgoing relationships

- **first making available to → internal market**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 10

    > (9) ‘placing on the market’ means the first making available of an AI system or a general-
    > purpose AI model on the Union market;

- **supply of → general-purpose ai models**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 11

    > (10) ‘making available on the market’ means the supply of an AI system or a general-purpose 
    > AI model for distribution or use on the Union market in the course of a commercial 
    > activity, whether in return for payment or free of charge;

### Incoming relationships

- **Places ← distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 1

    > Article 24
    > Obligations of distributors



---

## Node: ai literacy
<a name="node-ai-literacy"></a>

*0 outgoing, 2 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Involved in ← deployers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 57

    > (56) ‘AI literacy’ means skills, knowledge and understanding that allows providers, deployers 
    > and affected persons, taking into account their respective rights and obligations in the 
    > context of this Regulation, to make an informed deployment of AI systems, as well as to 
    > gain awareness about the opportunities and risks of AI and possible harm it can cause;

- **Involved in ← providers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 57

    > (56) ‘AI literacy’ means skills, knowledge and understanding that allows providers, deployers 
    > and affected persons, taking into account their respective rights and obligations in the 
    > context of this Regulation, to make an informed deployment of AI systems, as well as to 
    > gain awareness about the opportunities and risks of AI and possible harm it can cause;



---

## Node: types of ai systems concerned
<a name="node-types-of-ai-systems-concerned"></a>

*7 outgoing, 10 incoming*

### Outgoing relationships

- **Related to → union or member states**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Involved in → data necessary for the identification**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Related to → providers of intermediary services**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Associated with → risk involved**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 4

    > 3. The Member States shall immediately inform the Commission and the other Member 
    > States of a finding under paragraph 1. That information shall include all available details, 
    > in particular the data necessary for the identification of the AI system concerned, the origin 
    > and the supply chain of the AI system, the nature of the risk involved and the nature and 
    > duration of the national measures taken.

- **Notifies → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 1

    > Article 79
    > Procedure at national level for dealing with AI systems presenting a risk

- **Accesses → ai regulatory sandbox**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 1

    > Article 79
    > Procedure at national level for dealing with AI systems presenting a risk

- **similar function → product presenting a risk**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 2

    > 1. AI systems presenting a risk shall be understood as a “product presenting a risk” as defined 
    > in Article 3, point 19 of Regulation (EU) 2019/1020, in so far as they present risks to the 
    > health or safety, or to  fundamental rights, of persons.

### Incoming relationships

- **Consults and provides opportunity for presentation of views ← commission**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 7

    > 5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, 
    > objections are raised by a Member State against an authorisation issued by a market 
    > surveillance authority of another Member State, or where the Commission considers the 
    > authorisation to be contrary to Union law, or the conclusion of the Member States 
    > regarding the compliance of the system as referred to in paragraph 3 to be unfounded, the 
    > Commission shall, without delay, enter into consultations with the relevant Member State. 
    > The operators concerned shall be consulted and have the possibility to present their views. 
    > Having regard thereto, the Commission shall decide whether the authorisation is justified. 
    > The Commission shall address its decision to the Member State concerned and to the 
    > relevant operators.

- **belongs to ← providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.

- **Takes appropriate restrictive measures against ← market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 8: Procedure at national level for dealing with ai systems presenting a risk
  - Paragraph 10

    > 9. The market surveillance authorities of the Member States shall ensure that appropriate 
    > restrictive measures are taken in respect of the product or the AI system concerned, such as 
    > withdrawal of the product or the AI system from their market, without undue delay.

- **Confirmed that there is no ← national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 10

    > 9. With the exception of certificates unduly issued, and where a designation has been 
    > withdrawn, the certificates shall remain valid for a period of nine months under the 
    > following circumstances:
    > (a) the national competent authority of the Member State in which the provider of the 
    > AI system covered by the certificate has its registered place of business has 
    > confirmed that there is no risk to health, safety or fundamental rights associated 
    > with the high-risk AI systems concerned; and
    > (b) another notified body has confirmed in writing that it will assume immediate 
    > responsibilities for assessing those AI systems and completes its assessment within

- **informs ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 31: Changes to notifications
  - Paragraph 6

    > 5. Where its designation has been suspended, restricted, or fully or partially withdrawn, the 
    > notified body shall inform the providers concerned at the latest within 10 days.

- **Concerned ← eu declaration of conformity**
  - Chapter 3: High-risk ai systems
  - Article 42: Eu declaration of conformity
  - Paragraph 3

    > 2. The EU declaration of conformity shall state that the high-risk AI system concerned meets 
    > the requirements set out in Section 2. The EU declaration of conformity shall contain the 
    > information set out in Annex V, and shall be translated into a language that can be easily 
    > understood by the national competent authorities of the Member States in which the 
    > high-risk AI system is placed on the market or made available.

- **Transparency obligations ← provider of an ai system**
  - Chapter 4: Transparency obligations for providers and deployers of certain ai systems 
  - Article 1: Transparency obligations for providers and users of certain ai systems
  - Paragraph 1

    > Article 50
    > Transparency obligations for providers and users of certain AI systems

- **Makes available on the Union market ← providers of intermediary services**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 11: Compliant ai systems which present a risk
  - Paragraph 3

    > 2. The provider or other relevant operator shall ensure that corrective action is taken in 
    > respect of all the AI systems concerned that it has made available on the Union market 
    > within the timeline prescribed by the market surveillance authority of the Member State 
    > referred to in paragraph 1.

- **Point of contact ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 19: Alerts of systemic risks by the scientific panel
  - Paragraph 4

    > 3. A qualified alert shall be duly reasoned and indicate at least:
    > (a) the point of contact of the provider of the general-purpose AI model with systemic 
    > risk concerned;
    > (b) a description of the relevant facts and the reasons for the alert by the scientific 
    > panel;
    > (c) any other information that the scientific panel considers to be relevant, including, 
    > where appropriate, information gathered on its own initiative.

- **during investigations ← notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.



---

## Node: up-to-date scientific or technical expertise
<a name="node-up-to-date-scientific-or-technical-expertise"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Requires ← commission**
  - Chapter 7: Governance
  - Article 5: Scientific panel of independent experts
  - Paragraph 3

    > 2. The scientific panel shall consist of experts selected by the Commission on the basis of 
    > up-to-date scientific or technical expertise in the field of AI necessary for the tasks set 
    > out in paragraph 3, and shall be able to demonstrate meeting all of the following 
    > conditions:
    > (a) having particular expertise and competence and scientific or technical expertise in 
    > the field of AI;
    > (b) independence from any provider of AI systems or general-purpose AI models or 
    > systems;
    > (c) an ability to carry out activities diligently, accurately and objectively. The 
    > Commission, in consultation with the Board, shall determine the number of 
    > experts on the panel in accordance with the required needs and shall ensure fair 
    > gender and geographical representation.



---

## Node: regulation
<a name="node-regulation"></a>

*6 outgoing, 11 incoming*

### Outgoing relationships

- **improves → internal market**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 2

    > 1. The purpose of this Regulation is to improve the functioning of the internal market and 
    > promote the uptake of human-centric and trustworthy artificial intelligence (AI), while 
    > ensuring a high level of protection of health, safety, fundamental rights enshrined in the 
    > Charter of Fundamental Rights, including democracy, the rule of law and 
    > environmental protection, against the harmful effects of artificial intelligence systems 
    > (AI systems) in the Union, and to support innovation.

- **promotes → human-centric and trustworthy artificial intelligence**
  - Chapter 1: General provisions
  - Article 1: Subject matter
  - Paragraph 2

    > 1. The purpose of this Regulation is to improve the functioning of the internal market and 
    > promote the uptake of human-centric and trustworthy artificial intelligence (AI), while 
    > ensuring a high level of protection of health, safety, fundamental rights enshrined in the 
    > Charter of Fundamental Rights, including democracy, the rule of law and 
    > environmental protection, against the harmful effects of artificial intelligence systems 
    > (AI systems) in the Union, and to support innovation.

- **Does not preclude → union or member states**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 13

    > 11. This Regulation does not preclude the Union or Member States from maintaining or 
    > introducing laws, regulations or administrative provisions which are more favourable to 
    > workers in terms of protecting their rights in respect of the use of AI systems by 
    > employers, or from encouraging or allowing the application of collective agreements 
    > which are more favourable to workers.

- **Applicable → entry into force**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Entails → direct applicability**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

- **Provides for → partial application**
  - Chapter 13: Final provisions 
  - Article 12: Entry into force and application
  - Paragraph 1

    > Article 113
    > Entry into force and application
    > This Regulation shall enter into force on the twentieth day following that of its publication in the 
    > Official Journal of the European Union.
    > It shall apply from … [24 months from the date of entry into force of this Regulation]. 
    > However:
    > (a) Chapters I and II shall apply from … [six months from the date of entry into force 
    > of this Regulation];
    > (b) Chapter III  Section 4, Chapter V, Chapter VII and Chapter XII shall apply from 
    > … [12 months from the date of entry into force of this Regulation], with the 
    > exception of Article 101;
    > (c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 
    > … [36 months from the date of entry into force of this Regulation].
    >  
    > This Regulation shall be binding in its entirety and directly applicable in all Member States.

### Incoming relationships

- **Adheres to ← general-purpose ai models**
  - Chapter 5: General-purpose ai models
  - Article 3: Obligations for providers of general-purpose ai models
  - Paragraph 1

    > Article 53
    > Obligations for providers of general-purpose AI models

- **If necessary ← commission**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 12

    > 10. The Commission shall, if necessary, submit appropriate proposals to amend this 
    > Regulation, in particular taking into account developments in technology, the effect of AI 
    > systems on health and safety, and on fundamental rights, and in the light of the state of 
    > progress in the information society.

- **Allows for exemption from ← conformity assessment body**
  - Chapter 3: High-risk ai systems
  - Article 41: Derogation from conformity assessment procedure
  - Paragraph 1

    > Article 46
    > Derogation from conformity assessment procedure

- **adopted ← harmonised standards and common specifications**
  - Chapter 13: Final provisions 
  - Article 11: Evaluation and review
  - Paragraph 5

    > 4. The reports referred to in paragraph 2 shall devote specific attention to the following:
    > (a) the status of the financial, technical and human resources of the national competent 
    > authorities in order to effectively perform the tasks assigned to them under this 
    > Regulation;
    > (b) the state of penalties, in particular administrative fines as referred to in Article 99(1), 
    > applied by Member States for infringements of this Regulation;
    > (c) adopted harmonised standards and common specifications developed to support 
    > this Regulation;
    > (d) the number of undertakings that enter the market after the entry into application 
    > of this Regulation, and how many of them are SMEs.

- **allows for decision whether to use or disregard output ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 9: Human oversight
  - Paragraph 5

    > 4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be 
    > provided to the user in such a way that natural persons to whom human oversight is 
    > assigned are enabled, as appropriate and proportionate to the following circumstances:
    > (a) to properly understand the relevant capacities and limitations of the high-risk AI 
    > system and be able to duly monitor its operation, including in view of detecting and 
    > addressing anomalies, dysfunctions and unexpected performance ;
    > (b) to remain aware of the possible tendency of automatically relying or over-relying on 
    > the output produced by a high-risk AI system (‘automation bias’), in particular for 
    > high-risk AI systems used to provide information or recommendations for decisions 
    > to be taken by natural persons;
    > (c)  to correctly interpret the high-risk AI system’s output, taking into account, for 
    > example, the interpretation tools and methods available;
    > (d)  to decide, in any particular situation, not to use the high-risk AI system or to 
    > otherwise disregard, override or reverse the output of the high-risk AI system;
    > (e)  to intervene in the operation of the high-risk AI system or interrupt the system 
    > through a ‘stop’ button or a similar procedure that allows the system to come to a 
    > halt in a safe state.

- **Formulation of ← training, validation and testing data sets**
  - Chapter 3: High-risk ai systems
  - Article 5: Data and data governance
  - Paragraph 3

    > 2. Training, validation and testing data sets shall be subject to data governance and 
    > management practices appropriate for the intended purpose of the high-risk AI system. 
    > Those practices shall concern in particular:
    > (a) the relevant design choices;
    > (b) data collection processes and the origin of data, and in the case of personal data, 
    > the original purpose of the data collection;
    > (c) relevant data-preparation processing operations, such as annotation, labelling, 
    > cleaning, updating, enrichment and aggregation;
    > (d) the formulation of  assumptions, in particular with respect to the information that 
    > the data are supposed to measure and represent;
    > (e) an assessment of the availability, quantity and suitability of the data sets that are 
    > needed;
    > (f) examination in view of possible biases that are likely to affect the health and safety 
    > of persons, have a negative impact on fundamental rights or lead to discrimination 
    > prohibited under Union law, especially where data outputs influence inputs for 
    > future operations;
    > (g) appropriate measures to detect, prevent and mitigate possible biases identified 
    > according to point (f);
    > (h) the identification of relevant data gaps or shortcomings that prevent compliance 
    > with this Regulation, and how those gaps and shortcomings can be addressed.

- **imposes ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 4

    > 3. Where a notified body finds that an AI system no longer meets the requirements set out in 
    > Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw 
    > the certificate issued or impose restrictions on it, unless compliance with those 
    > requirements is ensured by appropriate corrective action taken by the provider of the 
    > system within an appropriate deadline set by the notified body. The notified body shall 
    > give reasons for its decision.
    > An appeal procedure against decisions of the notified bodies, including against 
    > conformity certificates issued, shall be available.

- **or otherwise terminate it ← authorised representatives of providers**
  - Chapter 6: Measures in support of innovation
  - Article 4: Testing of high-risk ai systems in real world conditions outside ai regulatory sandboxes
  - Paragraph 8

    > 7. Any serious incident identified in the course of the testing in real world conditions shall 
    > be reported to the national market surveillance authority in accordance with Article 73. 
    > The provider or prospective provider shall adopt immediate mitigation measures or, 
    > failing that, shall suspend the testing in real world conditions until such mitigation takes 
    > place, or otherwise terminate it. The provider or prospective provider shall establish a 
    > procedure for the prompt recall of the AI system upon such termination of the testing in 
    > real world conditions.

- **Permitted under certain conditions ← specific requirements**
  - Chapter 6: Measures in support of innovation
  - Article 7: Derogations for specific operators
  - Paragraph 1

    > Article 63
    > Derogations for specific operators

- **Enforce ← general-purpose ai models**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 17: Enforcement of the obligations of providers of general-purpose ai models
  - Paragraph 1

    > Article 88
    > Enforcement of the obligations of providers of general-purpose AI models

- **Exercises powers for evaluation ← notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 21: Power to conduct evaluations
  - Paragraph 1

    > Article 92
    > Power to conduct evaluations



---

## Node: assessment activities
<a name="node-assessment-activities"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Relates to ← notifying authority**
  - Chapter 3: High-risk ai systems
  - Article 26: Requirements relating to notified bodies
  - Paragraph 7

    > 6. Notified bodies shall be organised and operated so as to safeguard the independence, 
    > objectivity and impartiality of their activities. Notified bodies shall document and 
    > implement a structure and procedures to safeguard impartiality and to promote and apply 
    > the principles of impartiality throughout their organisation, personnel and assessment 
    > activities.



---

## Node: promotional material
<a name="node-promotional-material"></a>

*0 outgoing, 1 incoming*

### Outgoing relationships

_(none)_

### Incoming relationships

- **Indicated in ← identification number of the notified body**
  - Chapter 3: High-risk ai systems
  - Article 43: Ce marking
  - Paragraph 5

    > 4. Where applicable, the CE marking shall be followed by the identification number of the 
    > notified body responsible for the conformity assessment procedures set out in Article 43. 
    > The identification number of the notified body shall be affixed by the body itself or, under 
    > its instructions, by the provider or by the provider’s authorised representative. The 
    > identification number shall also be indicated in any promotional material which mentions 
    > that the high-risk AI system fulfils the requirements for CE marking.



---

## Node: providers
<a name="node-providers"></a>

*28 outgoing, 8 incoming*

### Outgoing relationships

- **which are not established in the Union → authorised representatives of providers**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **Involved in → ai literacy**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 57

    > (56) ‘AI literacy’ means skills, knowledge and understanding that allows providers, deployers 
    > and affected persons, taking into account their respective rights and obligations in the 
    > context of this Regulation, to make an informed deployment of AI systems, as well as to 
    > gain awareness about the opportunities and risks of AI and possible harm it can cause;

- **is a type of → operator**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 9

    > (8) ‘operator’ means a provider, product manufacturer, deployer, authorised representative, 
    > importer or distributor;

- **undergoes → obligations pursuant to article 16**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **transparency obligations pursuant to Article 50 → users**
  - Chapter 12: Penalties 
  - Article 1: Penalties
  - Paragraph 6

    > 3 % of its total worldwide annual turnover for the preceding financial year, whichever is 
    > higher:
    > (a) obligations of providers pursuant to Article 16;
    > (b) obligations of authorised representatives pursuant to Article 22;
    > (c) obligations of importers pursuant to Article 23;
    > (d) obligations of distributors pursuant to Article 24;
    > (e) obligations of deployers pursuant to Article 26;
    > (f) requirements and obligations of notified bodies pursuant to Articles 31, 33(1), 
    > 33(3), 33(4) or 34;
    > (g) transparency obligations for providers and users pursuant to Article 50.

- **Informed of interaction with AI system → natural person**
  - Chapter 4: Transparency obligations for providers and deployers of certain ai systems 
  - Article 1: Transparency obligations for providers and users of certain ai systems
  - Paragraph 2

    > 1. Providers shall ensure that AI systems intended to interact directly with natural persons are 
    > designed and developed in such a way that the natural persons concerned are informed that 
    > they are interacting with an AI system, unless this is obvious from the point of view of a 
    > natural person who is reasonably well-informed, observant and circumspect, taking into 
    > account the circumstances and the context of use. This obligation shall not apply to AI 
    > systems authorised by law to detect, prevent, investigate or prosecute criminal offences, 
    > subject to appropriate safeguards for the rights and freedoms of third parties, unless 
    > those systems are available for the public to report a criminal offence.

- **Related to → operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Establishes → market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 2

    > 1. Providers shall establish and document a post-market monitoring system in a manner that 
    > is proportionate to the nature of the AI technologies and the risks of the high-risk AI 
    > system.

- **Requires non-compliance to end → market surveillance governance and enforcement**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Request → commission**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 22: Power to request measures
  - Paragraph 2

    > 1. Where necessary and appropriate, the Commission may request providers to:
    > (a) take appropriate measures to comply with the obligations set out in Article 53;
    > (b) require a provider to implement mitigation measures, where the evaluation carried 
    > out in accordance with Article 92 has given rise to serious and substantiated 
    > concern of a systemic risk at Union level;
    > (c) restrict the making available on the market, withdraw or recall the model.

- **cooperates with → notifying authority**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.

- **belongs to → types of ai systems concerned**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 2: Reporting of serious incidents
  - Paragraph 7

    > 7. Following the reporting of a serious incident pursuant to paragraph 1, the provider 
    > shall, without delay, perform the necessary investigations in relation to the serious 
    > incident and the AI system concerned. This shall include a risk assessment of the 
    > incident, and corrective action. 
    > The provider shall cooperate with the competent authorities, and where relevant with the 
    > notified body concerned, during the investigations referred to in the first subparagraph, 
    > and shall not perform any investigation which involves altering the AI system concerned 
    > in a way which may affect any subsequent evaluation of the causes of the incident, prior 
    > to informing the competent authorities of such action.

- **Subject to → fines**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 5

    > 4. The provider shall ensure that all necessary action is taken to bring the AI system into 
    > compliance with the requirements and obligations laid down in this Regulation. Where 
    > the provider of an AI system concerned does not bring the AI system into compliance 
    > with those requirements and obligations within the period referred to in paragraph 2 of 
    > this Article, the provider shall be subject to fines in accordance with Article 99.

- **Makes available on the market → ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 9: Procedure for dealing with ai systems classified by the provider as
  - Paragraph 6

    > 5. The provider shall ensure that all appropriate corrective action is taken in respect of all 
    > the AI systems concerned that it has made available on the Union market.

- **Non-affixing requires non-compliance to end → ce marking**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Incorrect drawing up requires non-compliance to end → eu declaration of conformity**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Non-carrying out requires non-compliance to end → registration in the eu database**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Non-appointing requires non-compliance to end → authorised representatives of providers**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 12: Formal non-compliance
  - Paragraph 2

    > 1. Where the market surveillance authority of a Member State makes one of the following 
    > findings, it shall require the relevant provider to put an end to the non-compliance 
    > concerned, within a period it may prescribe:
    > (a) a CE marking has been affixed in violation of Article 48;
    > (b) a CE marking has not been affixed;
    > (c) a EU declaration of conformity has not been drawn up;
    > (d) a EU declaration of conformity has not been drawn up correctly;
    > (e) registration in the EU database has not been carried out;
    > (f) where applicable, an authorised representative has not been appointed;
    > (g) technical documentation is not available.

- **Terminates mandate due to non-compliance → authorised representatives of providers**
  - Chapter 5: General-purpose ai models
  - Article 4: Authorised representatives of providers of general-purpose ai models
  - Paragraph 6

    > 4. The authorised representative shall terminate the mandate if it considers or has reason 
    > to consider the provider to be acting contrary to its obligations pursuant to this 
    > Regulation. In such a case, it shall also immediately inform the AI Office about the 
    > termination of the mandate and the reasons therefor.

- **Applicable to → distributors**
  - Chapter 3: High-risk ai systems
  - Article 19: Obligations of distributors
  - Paragraph 2

    > 1. Before making a high-risk AI system available on the market, distributors shall verify that 
    > it bears the required CE marking, that it is accompanied by a copy of EU declaration of 
    > conformity and instructions for use, and that the provider and the importer of the system, 
    > as applicable, have complied with their respective obligations as laid down in Article 16, 
    > points (b) and (c) and Article 23(3).

- **subcontracting or activity delegation → subsidiaries**
  - Chapter 3: High-risk ai systems
  - Article 28: Subsidiaries of notified bodies and subcontracting
  - Paragraph 4

    > 3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of 
    > the provider. Notified bodies shall make a list of their subsidiaries publicly available.

- **Applied for → certificate**
  - Chapter 3: High-risk ai systems
  - Article 39: Certificates
  - Paragraph 3

    > 2. Certificates shall be valid for the period they indicate, which shall not exceed five years for 
    > AI systems covered by Annex I, and four years for AI systems covered by Annex III. On 
    > the application of the provider, the validity of a certificate may be extended for further 
    > periods, each not exceeding five years for AI systems covered by Annex I, and four years 
    > for AI systems covered by Annex III, based on a re-assessment in accordance with the 
    > applicable conformity assessment procedures. Any supplement to a certificate shall 
    > remain valid, provided that the certificate which it supplements is valid.

- **Compliance with this Regulation → level of protection required**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.

- **Compliance with this Regulation → degree of rigour**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.

- **Proportionality → size of the provider’s organisation**
  - Chapter 3: High-risk ai systems
  - Article 12: Quality management system
  - Paragraph 3

    > 2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the 
    > size of the provider’s organisation. Providers shall in any event comply with the degree of 
    > rigour and the level of protection required to ensure the compliance of their high-risk AI 
    > systems with this Regulation.

- **Gives access to automatically generated logs → national competent authorities**
  - Chapter 3: High-risk ai systems
  - Article 16: Cooperation with competent authorities
  - Paragraph 3

    > 2. Upon a reasoned request by a national competent authority, providers shall also give the 
    > requesting national competent authority, as applicable, access to the automatically 
    > generated logs of the high-risk AI system referred to in Article 12(1), to the extent such 
    > logs are under their control.

- **Monitor Operation → deployers**
  - Chapter 3: High-risk ai systems
  - Article 21: Obligations of deployers of high-risk ai systems
  - Paragraph 6

    > 5. Deployers shall monitor the operation of the high-risk AI system on the basis of the 
    > instructions for use and, where relevant, inform providers in accordance with Article 72. 
    > Where deployers have reason to consider that the use of the high-risk AI system in 
    > accordance with the instructions may present a risk within the meaning of Article 79(1), 
    > they shall, without undue delay, inform the provider or distributor and the relevant market 
    > surveillance authority, and shall suspend the use of that system. Where deployers have 
    > identified a serious incident, they shall also immediately inform first the provider, and 
    > then the importer or distributor and the relevant market surveillance authorities of that 
    > incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis 
    > mutandis. This obligation shall not cover sensitive operational data of deployers of AI 
    > systems which are law enforcement authorities.
    > For deployers that are financial institutions subject to requirements regarding their 
    > internal governance, arrangements or processes under Union financial services law, the 
    > monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by 
    > complying with the rules on internal governance arrangements, processes and mechanisms 
    > pursuant to the relevant financial service law.

- **Enters data into → eu database**
  - Chapter 8: Eu database for  high-risk ai systems 
  - Article 1: Eu database for high-risk ai systems listed in annex iii
  - Paragraph 3

    > 2. The data listed in Section A of Annex VIII shall be entered into the EU database by the 
    > provider or, where applicable, by the authorised representative.

### Incoming relationships

- **place on the market ← general-purpose ai models**
  - Chapter 1: General provisions
  - Article 2: Scope
  - Paragraph 2

    > 1. This Regulation applies to:
    > (a) providers placing on the market or putting into service AI systems or placing on the 
    > market general-purpose AI models in the Union, irrespective of whether those 
    > providers are established or located within the Union or in a third country;
    > (b) deployers of AI systems that have their place of establishment or are located within 
    > the Union;
    > (c) providers and deployers of AI systems that have their place of establishment or are 
    > located in a third country, where the output produced by the AI system is used in the 
    > Union;
    > (d) importers and distributors of AI systems;
    > (e) product manufacturers placing on the market or putting into service an AI system 
    > together with their product and under their own name or trademark;
    > (f) authorised representatives of providers, which are not established in the Union;
    > (g) affected persons that are located in the Union.

- **Recall initiated by ← ai systems**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 17

    > (16) ‘recall of an AI system’ means any measure aiming to achieve the return to the provider or 
    > taking out of service or disabling the use of an AI system made available to deployers;

- **Upon a reasoned request ← commission**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 6

    > 5. Upon a reasoned request of a provider whose model has been designated as a general-
    > purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take 
    > the request into account and may decide to reassess whether the general-purpose AI 
    > model can still be considered to present systemic risks on the basis of the criteria set out 
    > in Annex XIII. Such request shall contain objective, detailed and new reasons that have 
    > arisen since the designation decision. Providers may request reassessment at the earliest 
    > six months after the designation decision. Where the Commission, following its 
    > reassessment, decides to maintain the designation as a general-purpose AI model with 
    > systemic risk, providers may request reassessment at the earliest six months after that 
    > decision.

- **provides instructions for use to ← deployers**
  - Chapter 1: General provisions
  - Article 3: Definitions
  - Paragraph 16

    > (15) ‘instructions for use’ means the information provided by the provider to inform the 
    > deployer of in particular an AI system’s intended purpose and proper use ;

- **Becomes aware of risk ← operators of high-risk ai systems**
  - Chapter 3: High-risk ai systems
  - Article 15: Corrective actions and duty of information
  - Paragraph 3

    > 2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and 
    > the provider becomes aware of that risk, it shall immediately investigate the causes, in 
    > collaboration with the reporting deployer, where applicable, and inform the market 
    > surveillance authorities of the Member State or Member States in which they made the 
    > high-risk AI system available on the market and, where applicable, the notified body that 
    > issued a certificate for that high-risk AI system in accordance with Article 44, in 
    > particular, of the nature of the non-compliance and of any relevant corrective action 
    > taken.

- **Requires post-market monitoring to ensure compliance with requirements ← operators of high-risk ai systems**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Evaluates continuous compliance with requirements in Chapter III, Section 2 ← market monitoring**
  - Chapter 9: Post-market monitoring, information sharing, market surveillance
  - Article 1: Post-market monitoring by providers and post-market monitoring plan for high-risk ai systems
  - Paragraph 3

    > 2. The post-market monitoring system shall actively and systematically collect, document and 
    > analyse relevant data which may be provided by deployers or which may be collected 
    > through other sources on the performance of high-risk AI systems throughout their 
    > lifetime, and which allow the provider to evaluate the continuous compliance of AI 
    > systems with the requirements set out in Chapter III, Section 2. Where relevant, post-
    > market monitoring shall include an analysis of the interaction with other AI systems. 
    > This obligation shall not cover sensitive operational data of deployers which are law-
    > enforcement authorities.

- **Applicable to ← article 6(1)**
  - Chapter 5: General-purpose ai models
  - Article 2: Procedure
  - Paragraph 1

    > Article 52
    > Procedure



---

